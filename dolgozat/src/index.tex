\documentclass[
    parspace, % Add vertical space between paragraphs
    noindent, % No indentation of first lines in each paragraph
    %nohyp, % No hypenation of words
    %twoside, % Double sided format
    %draft, % Quicker draft compilation without rendering images
    %final, % Set final to hide todos
]{elteiktdk}[2023/04/10]

\usepackage{xcolor}
\newcommand{\guessvalue}{}
%\newcommand{\guessvalue}{\color{red} \textbf{$_{?}$}}

\usepackage{tabularx}
\usepackage{booktabs}
\newcolumntype{Y}{>{\centering\arraybackslash}X}

\usepackage{graphicx}
\usepackage{svg}
\usepackage{todonotes}
\usepackage{float}

\newcommand{\rhpad}{\vspace{0.6\baselineskip}}

\newcommand{\longcomment}[2]{\todo[inline]{Long comment: #1}}

\usepackage[newfloat]{minted}
\AtBeginEnvironment{minted}{\singlespacing}


\title{TDK-dolgozat címe}
\date{2023}
\author{Horváth Dávid}
\degree{Programtervező Informatikus BSc}
%\period{XXX. évfolyam}

\supervisor{Vincellér Zoltán}
\affiliation{PhD, Mesteroktató}

\university{Eötvös Loránd Tudományegyetem}
\faculty{Informatikai Kar}
\department{Információs Rendszerek Tanszék}
\city{Budapest}
\logo{elte_cimer_szines}


\addbibresource{references.bib}

\begin{document}

\documentlang{hungarian}

\listoftodos
\cleardoublepage

\makecover
\cleardoublepage

\maketitle

\tableofcontents
\cleardoublepage


\begin{abstract}
A szofisztikált tesztelés és a gyors prototípusgyártás
a modern szoftvertechnológia két elengedhetetlen alapköve.
Mégis, ezek egyik fő függőségének, a teszt- illetve demóadatoknak
a rendelkezésre állása manapság is komoly kihívást jelent.
Általában két megközelítési móddal találkozunk:
vagy az adatok on-the-fly generálával, mely ugyan kisköltségű,
de teljesen inkonzisztens az egyes üzenetváltások között;
vagy valamilyen tesztadatbázis odahelyezésével,
mely nem csak hogy hordozza a tényleges adatbázis minden költségigényét,
de jellemzően számos további nehézséggel bővíti, például adatgenerálással vagy anonimizálással.
Dolgozatomban egy olyan megoldás alapjait mutatom be,
mely további előnyök mellett ötvözi az előbbi kettő erősségeit,
míg kikerüli azok fő hátrányait.
Az előterjesztett megoldás egy akár közel zéró erőforrásigényű virtuális relációs adattár,
melynek szerkezete és adattartalma konfigurációs fájlon keresztül rugalmasan paraméterezhető,
és a lekérdezéseket on-the-fly hajtja végre konzisztensen
(legyen az SQL, NoSQL, GraphQL vagy egyéb),
és performanciája legalábbis összemérhető a valódi adatbázisokéval.
Ez a performancia akkor érhető el, ha a virtuális adatokhoz virtuális indexek is tartoznak,
aminek implementációbeli magját többek között a rendezett értékkészletek,
a diszkrét monoton függvények
illetve a kriptográfiában is használatos megfordítható permutációk adják.
\end{abstract}

\todo[inline]{összetett adatok, valós címek, földrajzi koordináták etc.}

\chapter{Körvonalazás}

\section{XXX}

\todo[inline,color=red]{chapter: Körvonalazás}

Előző változat bevezető fejezetei:

\begin{itemize}
    \item Végtelenített virtuális világok
    \item A fakerek limitációi
    \item Néhány visszatérő probléma közös nevezője
    \item A fakerek limitációi
    \item Virtuális mockolás: hézag jelei új termék számára?
    \item \dots
    \item Felépítés
    \item Konfiguráció
\end{itemize}

Egyéb:

\begin{itemize}
    \item Relational data as a serverless function
    \item SQL, NoSQL, GraphQL, REST etc.
    \item miniconnect, minibase, REPL
\end{itemize}

\begin{figure}[H]
\centering
\includesvg{diagram/simplearch}
\caption{A virtuális adatbázis vázlatos architektúrája}
\label{A virtuális adatbázis vázlatos architektúrája}
\end{figure}














%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%% BEGIN CONTENT %%%%%%

\chapter{A virtuális adattár}

\section{A TreeRandom és kapcsolódó API-k}

\todo[inline]{section: A TreeRandom és kapcsolódó API-k}

\section{A storage API}

\todo[inline]{section: A storage API}

\longcomment{Storage API ötletelés}{
schemák, táblák és oszlopok
indexek
   normál
   rendezetlen (optimalizálás-egyszerűsítés, kezelhető normál indexek halmazaként is)
   full-text
   spatial???
}

\section{A csak-olvasható alapréteg}

Bármilyen imlementáció legyen is mögötte, a storage API egy relációs adatbázist ír le.
Tehát ahhoz, hogy a virtuális adatok funkcionálisan egy valódi (csak-olvasható) relációs adathalmaz képét adják,
nem kell más, mint hogy konzisztens módon elérhetők legyenek a storage API-n keresztül.
A konzisztencia ez esetben két dolgot jelent:

\begin{enumerate}
  \item Felépíthető egy olyan tényleges immutábilis relációs adatbázis ($M$),
        hogy minden lehetséges relációs lekérdezés esetében, amely a közös schemára ($S$) értelmes,
        a virtuális és a tényleges adatbázis esetében visszaadott eredménytábla megegyezik.
  \item Az $S$ schema teljesíti a virtuális adatbázis felhasználói konfigurációját ($C$),
        valamint az $M$-ben szereplő adatok tulajdonságai illeszkednek a $C$-ben leírt megkötésekre.
\end{enumerate}

A virtuális adatokhoz a legalsó szinten a storage API megfelelő hívásaival férünk hozzá.
Ezek a speciális hívások szűk keresztmetszetet képeznek,
hiszen ezek szimulálják például a közvetlen adatelérést is.
Ha ezen hívások performanciája nagyságrendileg (de legalább aszimptotikusan) összemérhető a tényleges adatbázisokéval,
akkor az erre épülő lekérdezésfuttató és egyéb rétegek már
a tényleges adatbázisoknál megszokott módon és nagyságrendi teljesítménnyel tudnak működni.

A lekérdező műveletek esetében két különösen fontos hozzáférési módot kell kiemelni:

\begin{enumerate}
  \item rekordok véletlen elérése (random access)
  \item adott érték előfordulásainak keresése egy oszlopban (reverse index)
\end{enumerate}

Ha e két hozzáférési mód hatékony, akkor már a lekérdezések jelentős részénél
elérhető a tényleges adatbázisokéval összemérhető performancia.

A storage API-ban úgy definiáltuk a megfelelő interfészt,
hogy a keresésen kívül még néhány további funkciót is támogatnia kelljen,
például a rendezést és a $NULL$ értékek kezelését.
A következőkben leírt értékkiosztási módszerek többéségénél természetes módon következik,
hogy ezeket a további elvárásokat is teljesítik.
Az e szempontból problémás esetekre külön ki fogok térni.

\todo[inline]{További megjegyzések a csak-olvasható alapréteghez}

\section{Megoldások a hatékony értékkiosztásra}

Virtuális adatok alatt elsősorban az egy-egy oszlop alá besorakozó,
közös típussal rendelkező mezőértékeket értem.
Úgy is fogalmazhatunk, hogy az adatokat alapvetően oszlop-orientáltan fogjuk előállítani.
Minden oszlophoz tartozik majd egy virtuális adatlista,
melynek hossza egyenlő az adott oszlopot tartalmazó tábla hosszával
(opcionálisan szerepelhetnek benne $NULL$ értékek),
a többi érték típusának pedig kompatibilisnak kell lennie az oszlophoz megadott típussal.

Bár ez a megközelítás oszloponként független adatlistákra van szabva,
valójában más jellegű értékkiosztási módszer is lehet mögötte,
amennyiben az egy-egy oszlophoz tartozó listanézetek biztosítottak.
Majd a több oszlopot érintő megkötések esetében ez lesz a helyzet.
De lássuk először az egyoszlopos értékkiosztások lehetséges módszereit.

\subsection{Indexelt egyoszlopos értékkiosztások}

\subsubsection{Előzetes megfontolások}

E következőkben olyan értékkiosztási módszereket veszek végig,
amelyek lehetőleg biztosítják az alábbiakat:

\begin{enumerate}
  \item hatékony elérés (random access)
  \item pozíciótól függő érték (seed + sorindex)
  \item hatékony kereshetőség valamilyen formában
  \item hatékony rendezés
\end{enumerate}

Az egyes esetekben megvizsgálom majd, hogyan teljesíthetők ezek az elvárások.

\subsubsection{$NULL$ értékek kezelése}

Általánosan, bármely értékkiosztási módszerhez könnyen hozzáilleszthető a $NULL$ értékek támogatása.
Mindössze az szükséges, hogy az értékkiosztást a tábla méreténél kisebb intervallumra végezzük,
a maradék helyeket pedig $NULL$-nak tekintjük.
Ezen felül opcionálisan egy permutáció beiktatásával az értékeket elkeverhetjük,
ami által a $NULL$ értékek is szétszórtan szerepelnek majd.

Nem szükséges ez a plusz kompozíció,
ha a fentiek magába az eredeti értékkiosztásba is könnyen beépíthetők.
Például a kétfázisú értékkiosztásokba egyszerűen felvehető a $NULL$ mint érték.

Akár beépítetten, akár plusz kompozícióval van megvalósítva a $NULL$ értékek kezelése,
természetesen figyelni kell a speciális kezelésre a keresés-rendezés során.

A fentiek fényében az egyes értékkiosztások tárgyalásánál a $NULL$ értékek kezelésével nem foglalkozom.

\subsubsection{Egyszerű értékkiosztások}

Tényleges adatbázisokban némely esetben meglehetősen következetes módon
szerepelnek az értékek az oszlop értéklistájában.
Természetesen az ilyen esetek szimulálása a legegyszerűbb.

Triviális eset, ha egy oszlop mezői egy közös konstans értéket tartalmaznak.

Ha olyan adathalmazt szimulálunk, melyre megengedhető a feltevés,
hogy sem törlés, sem releváns értelemben problémás felülírás nem történt még,
úgy egyes oszlopok szekvenciális alapon generálhatók.
Az ilyen esetekben az $n$-edik érték lekérése
egy egyszerű lineáris függvénnyel számolható.
Az érték keresése hasonló, lényegileg az inverz függvényt kell alkalmazni.

A szekvenciális oszlopoknál az érték megegyezik a rekord 1-től indított sorszámával.

Időbélyegeket is generálhatunk hasonló módon,
amennyiben megengedhető, hogy a szimulált időadatok között egyenletes időközök legyenek.

\subsubsection{Általános kétfázisú értékkiosztás}

\todo[inline, color=red]{subsubsection: Kétfázisú értékkiosztás (bevezetés, leírás)}

Talán a leggyakoribb eset, hogy előre ismert véges értékkészlet elemeeit szeretnénk viszontlátni az oszlopban,
mégpedig összevissza

\begin{figure}[H]
\centering
\includesvg{diagram/distribution}
\caption{A kétfázisú értékkiosztás alapelve: egymás után végrehajtott visszafejthető disztribúció és permutáció}
\label{A kétfázisú értékkiosztás alapelve}
\end{figure}

\begin{figure}[H]
\centering
\includesvg{diagram/getvalue}
\caption{Adatlekérés a kétfázisú értékkiosztásból}
\label{Adatlekérés a kétfázisú értékkiosztásból}
\end{figure}

\begin{figure}[H]
\centering
\includesvg{diagram/findvalue}
\caption{Keresés a kétfázisú értékkiosztásban: a rendezett értékkészlet és a vetítések megfordíthatósága biztosítja a gyors kereshetőséget az virtuális listában}
\label{Keresés a kétfázisú értékkiosztásban}
\end{figure}

Egy adott érték lekérése egyszerűen így történik:

\begin{minted}{python}
def getValueOfMonotonic(i, start, end):
    flip = split(start, end)
    if i < flip:
        return getValueOfMonotonic(i, start, flip)
    else:
        return getValueOfMonotonic(i, flip, end)
\end{minted}

\todo[inline, color=red]{Nyújtás: jó egyáltalán így a lekérés-kód? index-pszeudokód + leírás}

\subsubsection{Kétfázisú értékkiosztás gyakoriságtáblázattal}

\todo[inline, color=red]{subsubsection: Kétfázisú értékkiosztás gyakoriságtáblázattal}

\subsubsection{Zajosan monoton értékkiosztások}

A rányújtásnak az előzőleg monoton függvény előállításához használt elve
másféle értékkiosztáshoz is felhasználható,
nevezetesen olyan (szigorúan vagy nem szigorúan) monoton adatsor előállításához,
melynek egyes értékei összességében egy adott sűrűség szerint növekednek
(vagy csökkennek; az egyszerűség kedvéért most csak a növekedő esetről lesz szó),
de lokálisan nagy az egyenetlenség.
Ezt is két lépésben fogjuk megvalósítani.\footnote{
  A két lépés általánosítható egy általános sűrűségfüggvény és egy zajfüggvény komponációjává,
  ahol a sűrűségfüggvény meredeksége és a zajfüggvény kilengése közötti viszonyt kell biztosítani
  (hogy az értékek ne ugorják át egymást).
  Itt most nem foglalkozunk ezzel az általánosabb kerettel.
}

Az első lépés alapelve tehát hasonló az előbbi megoldás első fázisához.
Ám itt nem lehetséges értékeket vetítünk ki a tábla hosszára,
hanem a táblahossznyi alaphalmazt vetítjük majd ki egy diszkrét lehetséges értékkészletre.
Minden sorindexhez hozzárendelődik egy (a szigorú monotonitás elvárása esetén nemüres)
dedikált sáv az értékkészletből.

A második lépés választ egy értéket a sávból.
Nem szigoróan monoton értéksor esetén megengedjük az üres sávot is,
és ilyen esetekben mindig a rákövetkező elemet választjuk.\footnote{
  Ebben az esetben explicite ki kell zárni, hogy az utolsó sáv üres legyen.
  Ez legtermészetesebben egy logikai paraméter beiktatásával érhető el,
  amelyet a rekurzió során mindig a csak felsőbb sávra küldünk tovább \texttt{true} értékkel
  (a többire \texttt{false} értékkel),
  alapértelmezett értéke \texttt{true}.
  Ha \texttt{true} értéket kaptunk, biztosítani kell, hogy az aktuális felsőbb sáv ne legyen üres.
}

Az érték elérése ekkor úgy történik, hogy először lekérjük az értéksávot a rányújtó függvénytől,
majd meghívjuk az értékválasztó függvényt,
melynek paraméterei az értéksáv, a sorindex és a \texttt{TreeRandom}-ból vett seed lesznek.
Egy kézenfekvő megvalósítás,
hogy a sorindex és a seed alapján inicializált randomgenerátorral
generáltatunk egy véletlen értéket, ami a sávba esik.

Az értéksávra való keresés úgy történik, hogy a vesszük a minimális és a maximális keresett értéket,
és az inverz rányújtást használva megkeressük a megfelelő sorindexeket, amelyek sávjához az érté tartozik.
Ezen sorindexek feszítik ki majd a találati sorindexsávot.
Hogy az alsó érték beletartozik-e, annak eldöntéséhez le kell generálni
az alső sorindexhez a konkrét értéket, és ellenőrizni, hogy nagyobbegyenlő-e,
mint a keresett alsó érték.
A felső érték esetében hasonlóan kell eljárni.

A rendezés triviális, mivel az értékek eleve rendezettek.

Ezzel az eljárással nem csak zajossá tudtuk tenni az eloszlást,
de a monotonitás garantálása mellett megengedtük,
hogy az értékek esetlegesen csomósodhassanak,
illetve elméletben tetszőlegesen eltávolodhassanak attól a helytől,
amit egy egyszerű, szigorúan egyenletes kiosztás esetén vettek volna föl.

Ez az értékkiosztás különösen alkalmas időbélyegek szimulálására,
amikor az események általános sűrűsége adott,
de véletlenszerű, zajos kiemenetet szeretnénk látni.

\subsubsection{Összetett értékek kezelése}

\subsubsection{Értékkiosztás reguláris kifejezés alapján}

Ha elengedjük a gyors keresés kritériumát,
akkor a reguláris kifejezés alapján történő értékkiosztást nagyon könnyen megvalósíthatjuk
egy véges automatával történő véletlenszerű inverz mintaillesztéssel\footnote{
  A jelenlegi implementáció a \textit{generex} könyvtárat használja erre.
},
(a sorindex és a \texttt{TreeRandom}-ból vett seed figyelembevételével).

Ha azonban fenn szeretnénk tartani a gyors keresés lehetőségét,
szükséges lesz, hogy képezni tudjuk az adott reguláris kifejezésre illeszkedő összes string virtuális listáját.
Azaz bármely $n$ sorindexre elő kell tudnunk állítani az $n$-edik illeszkedő stringet
(méghozzá abc-rendben, nem pedig a reguláris kifejezés szerkezete alapján).
És fordítva, tetszőleges stringre meg kell tudni mondani,
hányadik illeszkedő stringgel azonos vagy melyikhez van közel, ha nem illeszkedik.
Ha már van egy ilyen virtuális listánk, azt a kétfázisú értékkiosztással könnyen a kívánt oszloppá alakíthatjuk.

A megoldás a reguláris kifejezések lehetőségeinek csak valamilyen (erősen limitált) részhalmazát fogja támogatna.
De még így is bőven lefedi az olyan egyszerű eseteket, mint például a telefonszámok, email-címek stb.

Az ilyen típusú szöveggenerálás részletes bemutatása kimutat a jelen dolgozat keretei közül,
így ennek ismertetését itt mellőzöm\footnote{
  Egy egyszerű erre készült prototípus a \texttt{strex} könyvtár.
}.

\subsubsection{Full-text indexelt értékkiosztás}

A full-text indexelés legegyszerűbb formáját fogjuk támogatni:
tudunk majd keresni a szövegben előforduló szavakra,
azaz vissza tudjuk adni azon sorindexeket,
amelyekhez tartozó szövegekben szerepel a keresett szó.

Most egy kicsit egyszerűsített modellt mutatok be,
melyben a szavak betűkarakterek sorozatai,
és ezeket szóközök választják el.
Az egyéb szövegjellemzők (például központozás) hozzáadása nehézség nélkül kivitelezhető,
az áttekinthetőség kedvéért ezekkel most nem foglalkozom.

A módszert két generálási szakaszra bontjuk, a prefix és a fennamaradó rész generálására.

Kezdjük a prefix kiosztásával.
Legyen a szavak minimális száma $L$ ($L > 0$).
Vegyük $L$ darab szólistát, melyek rendre az $n$-edik szó lehetséges értékeit tartalmazzák.\footnote{
  Legegyszerűbb esetben ezek azonosak, és egy szótár szavai vagy generált értékek.
  De egy szofisztikáltabb változatban figyelembe vehetjük a valós szövegek jellemzőit.
}
Vegyük most azt a virtuális listát, melyen keresztül ezek Descartes-szorzatát érjük el.
Nem kell mást tennünk, mint ezt a listát a kétfázisú értékkiosztással alkalmazni.
Ha jó nekünk, hogy minden érték $L$ szóból áll, készen is vagyunk.

Ha viszont változó szószámot szeretnénk,
akkor még gondoskodni kell a szövegek fennmaradó részének előállításáról.
Ehhez állítsunk elő egy-egy függvényt a szótár minden egyes szavára\footnote{
  Feltételezzük, hogy a szótár nem túl nagy (például lorem ipsum szavak).
  De nagy szótár esetén is van egy egyszerű kerülőmegoldás:
  a függvényt több rekurzív szinten valósítjuk meg,
  először a szótár nagy szeleteit osztjuk ki,
  majd azon belül kisebb szeleteket, és így tovább, végül az egyes szavakat.
  Így a szó ellenőrzése az adott sorindexre még mindig logaritmikus számítási igényű.
}, mely lényegileg a két logikai érték rányújtása:
bármely sorindexre megadja, hogy az adott szó szerepel-e a hozzá tartozó szövegben,
illetve bármely szóra megadja a sorindexeket, ahol a szó szerepel.

Már csak annyi feladatunk van, hogy bármely sorindexre következetesen visszaadjunk egy olyan szöveget,
mely megfelel ennek a két kritériumnak:

\begin{enumerate}
  \item Az első $L$ szó egyezik az első szakaszban meghatározott prefixszel.
  \item A fennmaradó rész pontosan azokat a szavakat tartalmazza,
        amely a fennmaradó rész kiosztásánál leírt szóválasztásnak megfelel.
        Ugyanaz a szó többször is szerepelhet.
\end{enumerate}

A második kritérium természetesen legegyszerűbben úgy teljesíthető,
hogy a kiválasztott szavakat abc-rendben felsoroljuk.
De mivel teljes szabadságunk van (az index már biztosított),
tetszőlegesen szofisztikált módszereket alkalmazhatunk a minél életszerűbb szöveg kialakítására
(ez egy skálázható opció).

A rendezésnél kihasználjuk, hogy az első $L$ szóra már van egy hagyományos indexünk,
így a prefixek egymáshoz képesti rendezése már megoldott.
Mivel a Descartes-szorzat miatt a lehetséges prefixek listája igen nagy,
feltehetjük, hogy jellemzően nem nagyon lesznek ismétlődő prefixek,
de ha igen, csak azokon belül kell tényleges összehasonlításos rendezést végezni.

\subsection{Nem-indexelt egyoszlopos értékkiosztások}

A nem-indexelt oszlopok (melyek jellemzően terjedelmesebb adattartalmak)
szimulálásakor a keresési-rendezési szempontokat egyáltalán nem kell figyelembe venni.
Az indexelt oszlopokra vonatkozó elvárások közül így csak az első kettő releváns:

\begin{enumerate}
  \item hatékony elérés (random access)
  \item pozíciótól függő érték (seed + sorindex)
\end{enumerate}

Vagyis elég, ha a \texttt{TreeRandom} által a sorhoz generált seed alapján
előállítunk egy \texttt{Random} példányt.
Ennek felhasználásával a tartalmat tetszőleges determinisztikus módszerrel állíthatjuk elő.
Erre az előállításra kizárólag a konfigurációban megadott beállítások jelentenek megszorítást.

A következő listában csak fölvillantanám az ilyen adattartalmak néhány jellegzetes típusát:\footnote{
  Amelyeket érdemes beépítetten is támogatni, nyilván a lehetőségek korlátlanok.
}

\begin{itemize}
  \item \textbf{Egyszerű szöveg:}
    a legegyszerűbb megoldásban egy szótár szavait véletlenszerűen helyezzük egymás után,
    közben feljavítva a szövegképet nagybetűs szavakkal és központozással
    (az eredmény tovább javítható kifejezésminták használatával,
    amikor időnként egy többszavas részletre előre adott sablont használunk)
  \item \textbf{Strukturált szöveg:}
    először előállítunk egy általános struktúrát (címek, bekezdések stb.),
    majd ennek elemeit feltöltjük szöveggel,
    végül a struktúrát valamilyen jelölőnyelven szolgáltatjuk (HTML, MarkDown, plain text stb.)
  \item \textbf{Strukturált adat:}
    generált vagy konfigurációban megadott adat-schema alapján rekurzívan generáljuk le az adatstruktúrát,
    a schema-nyelv (pl. JSON Schema) és a kimenet formátuma is konfigurálható (JSON, YAML, TOML stb.)
  \item \textbf{Egyszerűbb képek:}
    véletlen színű háttérre véletlen paraméterekkel
    rárajzolunk néhány egyszerű alakzatot,
    majd a megadott vektoros/raszteres formátumban szolgáltatjuk
  \item \textbf{Jelszó-hash:}
    tulajdonképpen elég a hash értéket magára a seedre generálni
  \item \textbf{Általános BLOB/CLOB:}
    lekéréskor a konfigurációban megadott mérethatárok közötti bájtsort/karaktersort kell visszaadni,
    ami könnyen ellátható random bájtok/karakterek generálásával
\end{itemize}

\subsection{Értékkiosztás oszlopok közötti megkötésekkel}

\todo[inline]{subsubsection: Értékkiosztás oszlopok közötti megkötésekkel}

\subsection{Táblák közötti kapcsolatok}

\todo[inline]{subsubsection: Táblák közötti kapcsolatok (idegen értékkészlet; kiszervezett megkötések)}

\section{Az írhatósági réteg}

\todo[inline]{section: Az írhatósági réteg}


%%%%%% END CONTENT %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%














\section{Ömlesztett megjegyzések}

\longcomment{Ömlesztett megjegyzések}{
Vincellér:
-----------------
Néhány visszatérő szituáció:
- prototyping
    - nem feltétlenül végleges
- mocking
    - csak legyen ott
- tesztelés
    - megbízhatóan kerüljön oda, akár zsinórban többször
- gyors anonimizálás
    - körülbelül tudjuk, minek kell látszódnia
- 

--> ez mind egyetlen probléma

A problémák valódi adatbázis használatával
- erőforrásigény
    - el kell induljon a szerver
    - le kell generálni és fizikailag letárolni az összes adatot
    - ez rengeteg helyet is foglal, pedig általában kevés adatot használunk
    - 
- egyéb (színnel is lehet jelezni)
    - tudni kell, mi a schema
    - tudni kell mifélék az adatok
--> az egyéb kategóriába tartozók:
- dokumentáció jellegűek (nem kell minden indításkor végrehajtani őket)
    (az adatbázis alternatívái: full dependency mock (unit test))

docker: bemutatni, hogy mit tud, elindítani egy szervert
ebből az adatrengetegből egy szemernyi nem létezik valójában

Vincellér: 
nagyon a gyakorlat felől lezdtünk neki
de nekem csak annyit kell bizonyítanom, hogy **érdemes** ezzel foglalkozni, a hangsúly az ötlet/tézis megvalósíthatóságán van
maga a szoftver egy PoC, ami már nagy előrelépés a teljesen elméleti dolgozatok tömegéhez képest
a fő fókusz a storage on-the-fly implementálhatóságán van
különösen az adatgenerálásra fogok koncentrálniáll
grátisz, hogy ez egy eladható szoftver, de azért ebbe is belemehetûnk
fő altézis: milyen módokon hatékony read-only kereshető (reverse-indexelt), on-the-fly adatlistákat vagy ezek csoportját implementálni (ez a kutatandó elméleti szűk keresztmetszet, az alkalmazás kontextusára rámutatva)
nem innovatíve szedtem-hordtam össze megoldásokat, hogy valamit pusztán mindenáron létrehozzak
érdekel a dolog anatómiája, a főprobléma, és hogy miért nincs még ilyen
mindenre ráfókuszáltunk, csak pont az adalisták generálására nem (a többi megoldott probléma)

Vincellérnek:
------------------------
előzmény, motiváció, nagyon régóta érik
inspirációk
  - faker
  - seed alapú nyílt világ
  - relációs schema
a gyakorlati probléma
  - prototype, mock
  - teszt
  - szimuláció (akkurátusság, performancia)
  - oktatás
a keresztmetszeti storage interfész
már van egy viszonylag tűrhetően működő prototípus
az elméleti problémák
  - read-only + diff layer
  - adatgenerálás
     - a monolithic+permutation megoldás, ezek külön-külön
     - egyéb módok
         - egyszerűek (counter, egysz. regex)
         - kereshető regex
         - speciális constrained
         - struct schema
         - binary gen (pl. kép)
  - később: profilozás
interfészek, erőteljes bővíthetőség
tesztek
  - unit
  - performance (junit)
  - demonstrációs
  - egyéb
saját és külső sql planner (saját, calcite), runtime döntés?
a kutatni- és mérnivalók
kiegészítő témák (miniconnect api, jdbc bridge, repl, stb.)
egyéb (a másik i rány):
   - generálás meglévő adatbázisból
      - és vica versa
      - column szemantikájának detektálása
   - AI által generált schema, szöveg vagy constraintek alapján
     "please provide a medium sized database for a book library"
        (az egész read-only adatbázis maga a schema, azáltal meghatározott)

TDK-hoz:
---------------
a prototípus teljes docker+holodb-s változata
mysql-változat éles adatbázissal
mysql-változat anonimizálással
mysql-változat adatgenerálással
measurement
eredmények táblázata
módszertani leírás (a tdk-s latex keretben)
holodb szerkezeti diagramja

A1: mysql létező adatbázissal, file copy
A2: mysql létező adatbázissal, SQL dump
B1: mysql anonimizálással, file copy
B2: mysql anonimizálással, SQL dump
C: mysql generálással
D: h2 generálással
E: holodb

projekt előfeltétele
- A: feltöltött adatbázis
- B: feltöltött adatbázis
- C: kész schema
- D: kész schema
- E: schema-vázlat
mock előfeltétele
- A: ---
- B: anonimizáló szkript
- C: generáló szkript
- D: generáló szkript
- E: deklaratív konfiguráció
data dump
- A: copy / dump
- B: copy / dump
- C: ---
- D: ---
- E: ---
adatbázis indítása
- A-E: start
adatbázis inicializációja
- A: ---
- B: másolás + anonimizáció
- C: adatgenerálás
- D: adatgenerálás
- E: ---
tesztfuttatás
- A-E: teszt(1..n)
adatbázis lebontása és zárása
- A-D: drop, (...?), stop
- E: stop

join-hibához:
pontosan akkor kell meghagyni a távolis sorokkal nem rendelkező rekordot left joinnál, ha a távoli táblára nincs szűrés (ha nem a szűrés miatt üres); de ezzel empirikusan is kísérletezni kellene mysql-en

fejlesztendő:
- generális where (konjunktív normálforma? oszlopokra csoportosított? joinok? nagyon nem triviális)
- egyszerű group by
- allekérdezés a SELECT részben
- 

isCommittable
a diff layer a tranzakcióbeli módosításokhoz hasonló

holodb vs. materializált nézettáblák
(read-only tesztnél azt is ki lehetne próbálni)

killer feature: middleware
sql extensions (e.g. jdbc methods, possibly like sqlline)
holodb/minibase api: fulltext index?

funkcionálisan nincs különbség fizikai elérés és on-the-fly generálás között, ha a generálás:
1.) gyors
2.) konzisztens/szisztematikus
3.) valószerű eredményt ad

vertical composition: 
- compound data structures (vector, complex, geospatial etc.)
- compound short text (full names, addresses, email etc.)
horizontal composition:
- use union of data sources
- possibly track their source for multi-column constraints
}

\end{document}
