\documentclass[
    parspace,
    noindent,
    nohyp,
]{elteiktdk}[2023/04/10]

\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{svg}
\usepackage{float}
\usepackage{pgfplots}
\usepackage{arydshln}
\usepackage{fontawesome}
\usepackage{picture,xcolor}
\usepackage{pdflscape}
\usepackage{fancyvrb}
\usepackage[backend=bibtex,style=numeric]{biblatex}

\usepackage{todonotes}

\newcommand{\rhpad}{\vspace{0.6\baselineskip}}

\newcommand{\thesispar}[1]{
\vspace{1em}
\hspace{0.7cm}\parbox[left][][c]{15.8cm}{\linespread{1.2}\selectfont #1}
\vspace{1em}
}

\usepackage[newfloat]{minted}

\title{HoloDB: szintetikus adatok on-the-fly szolgáltatása relációs adatbázisként}
\date{2024}
\author{Horváth Dávid \\ {\small\href{mailto:horvathdown@student.elte.hu}{horvathdown@student.elte.hu}} }
\degree{Programtervező Informatikus BSc}

\supervisor{Dr. Vincellér Zoltán}
\affiliation{Mesteroktató}

\university{Eötvös Loránd Tudományegyetem}
\faculty{Informatikai Kar}
\department{Információs Rendszerek Tanszék}
\city{Budapest}
\logo{elte_cimer_szines}


\addbibresource{references.bib}

\begin{document}

\documentlang{hungarian}

\listoftodos
\cleardoublepage

\makecover
\cleardoublepage

\maketitle

\tableofcontents
\cleardoublepage


\begin{abstract}
ABSTRACT
\end{abstract}

% TODO: a bevezetőben utalni mindenféle korábbi generáló-módszerre
% PI-jegyek, fraktálok stb.

% TODO: https://www.cs.cmu.edu/afs/cs/academic/class/15451-s06/www/lectures/scrabble.pdf

\chapter{INTRO}

\section{Bevezetés}

\section{A holografikus adatbázis jelentése}

\todo[inline]{nem lehet pontos definíciót adni, legfeljebb bizonyos követelményeket, inkább körülírjuk}

\section{Koncepció}

\subsection{Konfiguráció}

% TODO: rendesen szövegezni, ide tenni a konfigurációs részt az eredetiből

A virtuális adatbázis felépítése a felhasználó számára legegyszerűbben
egy konfigurációs fájl segítségével történik,
melyben megadja az adatbázis-séma egyes elemeit, megkötéseit.
Az oszlopok tartalmának keverési módja és értékkészlete kényelmesen állítható,
például előre adott értéklistákkal, reguláris kifejezéssel,
vagy akár a felhasználó által implementált Java-osztállyal.
A virtuális adatbázis Dockerben való indításához vagy beágyazott használatához
lényegileg elég egy ilyen konfigurációs fájl.

% TODO: ide a konfigfájl-példát

\subsection{ALKALMAZHATÓSÁG}

% ha mindezt meg tudjuk valósítani...

\todo[inline]{előnyöket itt tárgyalni: azonnali indulás, minimális memóriafogyasztás}

% TODO: azonnali indulás: lazy init-en még dolgozni!

\subsection{ÚJONNAN MEGNYÍLÓ LEHETŐSÉGEK}

\todo[inline]{serverless}

\todo[inline]{dev mode}

\todo[inline]{JPA annotációk}

\todo[inline]{letapogatás és renderelés, AI általi generálás}

\todo[inline]{zero mode}






\chapter{ALAPOK}

\section{A nagy illúzió: adatok a semmiből}

Olyan adattár felépítésére vállalkozunk,
mely egy \textbf{tetszőlegesen nagy méretű, koherens és kereshető} adattárat imitál,
miközben valójában mindebből semmi sem létezik, az egész mintegy illúzió.
Hogy az illúziót elérjük, egyfelől meg kell találni azt az univerzális modellt,
mely a lehető legrugalmasabb felhasználásra alkalmassá teszi;
másrészt a cél eléréséhez, mint majd látjuk, apró eszközök egész tárát kell összeverbuválni,
akár olyan távol eső területekről mint az automataelmélet vagy a kriptográfia.

\todo[inline]{itt leírni az implementációs alapötletet a lehető legvilágosabban, ez lesz a legtömörebb, fő bekezdés}

Alapvetésként egy \textbf{relációs adatbázist leíró API}-t fogunk definiálni.
Aligha érdemes más megközelítést választani,
hiszen a relációs adatmodell nem csak tradicionálisan megkerülhetetlen
és máig a legnépszerűbb adattárolási módszer,
de egyúttal közös nevezőként szolgál bármely más modellhez.
Egy relációs struktúrával gyakorlatilag bármilyen interpretált adathalmazt le tudunk írni,
mivel a relációs séma általános normalizációs segédeszköz.
A célunk lényegében a relációs API implementálása lesz, konfigurálható módon.

A legtöbb alkalmazásban van valamilyen fokális entitástípus,
mely alapvető szerepet tölt be az architektúrában
(például egy könyvtári katalógusban a \textit{könyv}).
Célszerű lehet egy ilyet esetünkben is meghatározni,
hogy súlypontként, viszonyulási alapként, segítse az architektúrában való eligazodást.
A tervezés elején hamar arra a megállapításra jutottam,
hogy a virtuális adattár legalapvetőbb objektuma az \textbf{\textit{értéklista}}.
Az értéklista egyszerű esete valamely egyszerű \textit{értékkészlet},
az API-ban pedig egy-egy \textit{oszlop} értéklistájával fogunk találkozni.

% FIXME: mi ez a szöveg?
Az oszlop alatt azonos típusú adatokat találunk,
az alapfeladat mindig ezen adatok értékkészletének megadása lehet.
Általában egy index is egy (esetleg néhány) konkrét oszlophoz köthető.

Természetesen hamar ki kell lépnünk a szigorú oszlop-orientáltságból.
Lehetnek például összefüggések egy táblán belüli oszlopok között.
Ezt azonban még mindig egyszerűbb az oszlopos megközelítés megfelelő kiterjesztésével elérni,
mintsem áttérni valamiféle rekordonkénti generálásra.
Úgy is mondhatjuk, hogy az oszlopok alatti mezők jó jelöltek arra, hogy ``világszeletek'' legyenek,
míg egész rekordok nem.

Lehetnek továbbá megkötések táblák között is.
Ezek kezelése is könnyen elvégezhető az oszlopos alapvetésre építve.
Az egyoszlopos idegen kulcsoknál például csak annyit kell biztosítani,
hogy a hivatkozó oszlop értékkészletét a hivatkozott oszlop tartalma adja.

A mindenkori módszert, amivel egy oszlop értéklistáját biztosítjuk,
\textit{értékkiosztás}nak fogom nevezni.
A virtuális értékkiosztások olyan függvényt valósítanak meg,
mellyel egy adott mező értéke determinisztikusan előállítható.
De az igazán lényeges kihívást az jelenti, hogy szeretnénk,
hogy az értéklistában keresni is tudnánk, méghozzá hatékonyan.
Lentebb elsősorban olyan megoldásokat fogok bemutatni,
amelyek teljesítik ezt az erősebb feltételt is,
azaz \textbf{kereshető virtuális értékkiosztások}.

A legtöbb oszlop értékkészlete jól behatárolható.
A tényleges értékek egy efölötti multihalmazt alkotnak,
néha ehhez valamilyen statisztikai megkötés is tartozik ($NULL$-ok száma, értékgyakoriság stb.).
Továbbá a multihalmaz valamilyen konkrét sorrendben helyezkedik el az oszlop alatt.
Itt jegyzem meg, hogy a tábla rekordjait sorrendezettnek fogom tekinteni:
azonos táblabeli valamely két oszlop értéklistájából kiválasztott egy-egy mező
azonos rekordhoz tartozik pontosan akkor, ha a sorrendbeli pozíciójuk (sorindexük) megegyezik.

Vegyük észre, hogy két független problémával szembesülünk:

\begin{enumerate}
    \item az oszlopértékek multihalmazának előállítása
    \item az értékek összekeverése
\end{enumerate}




% TODO: az első is két részre bontható
% ekkor a cél: eljutni egy értékkészlettől egy oszlop-értéklistáig

Ezeket úgy kell megvalósítani, hogy a végső értéklista adott pozíciójú eleme is gyorsan lekérhető legyen,
valamint egy adott érték vagy értéksáv előfordulási pozíciói is
(utóbbi esetben lehetőleg rendezetten).

Ha feltesszük, hogy a multihalmaz úgy van ábrázolva,
hogy a hatékony értéklekérés és keresés-rendezés elvárása teljesül,
akkor már csak egy (a tábla hosszára értelmezett) invertálható permutációra van szükség,
hogy az adatokat visszakereshetően szét is szórjuk.
(Lentebb látni fogjuk, milyen módszerrel tudjuk a multihalmaz egy eleve rendezett reprezentációját képezni,
amivel az elvárások teljesíthetők.)
A gyorsan számítható invertálható permutációk tipikus képviselője a modulo szorzásra épül.
Plusz költség vállalásával pedig sokkal jobb szórást biztosító permutációkat kapunk,
ha kriptográfiai módszereket vetünk be.

A \textbf{kétfázisú értékkiosztásnak} ezzel az ötletével
egy egyszerűbb adatbázis szimulációjának kívánalmait már tulajdonképpen le is fedtük,
de néhány további hasznos értékkiosztási módszert is be fogok majd mutatni.

Ha oszlopokat elő tudunk állítani,
akkor a táblák és egyéb objektumok köréépítése nem jelent kihívást.
Ha pedig a relációs adatmodell ezen objektumait sikeresen implementáltuk,
a relációs műveletek és \textbf{az SQL lekérdezések megvalósítása már a hagyományos módon történhet}.
Éppen ezért a továbbiakban a lekérdezések problémakörével csak érintőlegesen foglalkozom.
Ma már elérhető néhány nyílt forrású jól felépített SQL query planner framework is,
melyek megkönnyítik egy teljes körű SQL-szerver kiépítését.
Ilyen például az általam használt Apache Calcite.
Ugyanakkor egy saját SQL-futtatót is implementáltam, mely sok esetben gyorsabban fut,
cserébe csak egy limitált SQL-t támogat.

\section{Hatékony számítások}

Mivel a szoftver létező prototípusa, a \textit{HoloDB}
(és a hozzá kapcsolódó segédeszközök, könyvtárak nagy része) Java programnyelven íródott,
a továbbiakban felteszem, hogy Java környezetben vagyunk.

A hagyományos, tényleges adatbázisokban az egyik szűk keresztmetszetet
az adatok fizikai elérése jelenti.
Esetünkben ezt a virtuális értékkiosztások fenti függvényei helyettesítik.
A blokkbetöltés, keresési index-struktúrák, materializált nézetek frissítése
és hasonló problémák helyett a virtuális adattárban
a visszaadandó adatok hatékony kiszámítását kell megoldani,
ami általában az oszlopok értékeinek on-the-fly produkálását és keresését jelenti.

Az on-the-fly értékszámítás során gyakran fordulnak elő átmeneti óriás számok,
ám nehezen jósolható előre, hogy pontosan mikor.
Például eleve egy értékkészlet is lehet óriási
(gondoljunk például az \texttt{{\textbackslash}w{30}}
reguláris kifejezés alapján kitöltött oszlop lehetséges értékeinek számára),
miközben szükségünk van bármely pozíció közvetlen hivatkozására.
Az adatbázis méretére vonatkozóan nem akartam megkötéseket tenni.
Nem lett volna célszerű explicit ellenőrzések tömegét beépíteni
a problémás számítások, potenciális túlcsordulások földerítésére.
Továbbá az egész számokat egységesen szerettem volna kezelni, függetlenül azok méretétől.
Így a beépített \texttt{BigInteger} típust egy idő után elvetettem,
mivel az kisebb számokra a \texttt{long} típushoz képest rendkívül rossz performanciájú.

\todo[inline]{ide a hashert, és bármit, ami még jöhet}

\todo[inline]{skálázhatóságról (FastHasher, LinearMonotonic, noop permutation)}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Az értékkiosztási trükkökkel egy statikus adatbázist írunk le,
ami értelemszerűen önmagában csak-olvashatóan érhető el.
Látni fogjuk, hogyan lehet egy csak-olvasható tábla fölé olyan dekorátort helyezni,
mely a változások megfelelő adatstruktúrában történő számontartásával
az írhatóság illúzióját kelti.























\chapter{STORAGE API}

\section{ARCHITEKTÚRA}

\section{REFERENCIA-IMPLEMENTÁCIÓK (így???)}

\subsection{KÉTLÉPÉSES ÉRTÉKKIOSZTÁS}

\subsection{A monoton méretigazítás megvalósítása}

A \texttt{Monotonic} interfész példánya egy függvényt reprezentál,
amely az első $n$ nemnegatív egész számot értékkészletként nem monoton növekvő módon hozzárendeli az első $N$ nemnegatív egész számhoz.
Ez arra használható, hogy egy $n$ elemű, rendezetten tárolt értékkészetet egy $N$ méretű adatbázistábla oszlopához igazítsunk méretben,
Tehát a példány $M(i) = m$ függvényt valósítja meg, ahol $0 \leq i < N$ illetve $0 \leq m < n$, továbbá $i_1 < i_2 \implies M(i_1) < M(i_2)$.
Ugyanakkor indexelt elérést is biztosít, azaz lekérdezhető, hogy egy adott érték mely indexsávhoz rendelődik,
azaz megvalósítja az $M^{-1}(m) = [i, j)$ függvényt is, ahol $0 \leq m < n$ illetve $0 \leq i \leq j \leq N$, és teljesülnek az alábbiak:

\begin{itemize}
\item $x < i \implies M(x) < M(i)$
\item $i \leq x < j \implies M(i) \leq M(x) < M(j)$.
\item $i < x \implies M(i) < M(x)$
\end{itemize}

Az indexelt elérés az \texttt{indicesOf(value)} metóduson keresztül biztosított, amely egy \texttt{Range} objektumot ad vissza.
A metódus egy variánsa maga is egy \texttt{Range} példányt vár, és értelemszerűen általánosítja az előbbit.

\subsubsection{Triviális implementációk}

\todo[inline]{Monotonic: triviális implementációk}

\todo[inline]{Monotonic: egyszerű diszkrét lineáris interpoláció: általában ez is elég, a permutáció majd úgyis elkeveri}

\subsubsection{Szimulált mintavétel}

\todo[inline]{Monotonic: szimulált mintavétel (jó ez a cím?)}

\todo[inline]{Monotonic: szimulált mintavétel: a lényeg, hogy bizonyos mértékű véletlenszerűséget ad}

\subsubsection{Gyakoriságtáblázat használata}

A fenti implementációk mindegyike általánosítható úgy, hogy figyelembe vegyen egy gyakoriságtáblázatot,
mely az értékkészlet elemeihez azok relatív gyakoriságát rendeli.

\todo[inline]{Monotonic: gyakoriságtáblázat}

\subsection{Permutációk megvalósítása}

A \texttt{Permutation} interfész megvalósításához olyan eljárást keresünk, amellyel változó (és akár óriás) tartományon értelmezett,
seed szerint variálható permutációfüggvény és annak inverze előáll, ahol mindkét függvény hatékony.

Az inverz függvényhívás az \texttt{indexOf(value)} metódus biztosítja.

\subsubsection{Triviális implementációk}

Triviális implementációként használhatjuk egyszerűen az azonosságfüggvényt:

$$
P_{id}(i) = i (0 \leq i < n)
$$

Gyorsan számolható, de ránézésre már jó keveredést adó permutációt nyerünk, ha kihasználjuk, hogy az $ai \equiv p \pmod{n}$ lineáris kongruencia
$i$-re és $p$-re nézve egy permutációt ad, ha $a$ és $n$ relatív prímek:

$$
P_n(a,b)(i) = ai + b \mod n
$$

Az $a$ és $b$ számot előre, a seed alapján határozzuk meg.
Az $a$ szám relatív prím kell legyen $n$-re nézve, amit úgy érünk el, hogy ciklusban az euklideszi algoritmussal ellenőrizzük, hogy a legnagyobb közös osztójuk 1-e,
és amíg nem, addig meghívjuk a számot tároló \texttt{LargeInteger} példány \texttt{nextProbablePrime()} metódusát az új jelölt lekéréséhez.
Ez a háttérben a \texttt{BigInteger} osztály ugyanilyen nevű metódusát hívja, amely a Java beépített heurisztikus megoldása a következő valószínű prím megkeresésére.
Nekünk csak egy relatív prím kell, tehát nincs szükség valódi prímtesztre, elég a gyors heurisztika és aztán a legnagyobb közös osztó ellenőrzése.

\subsubsection{Feistel-hálózatok alkalmazása}

Az invertálható, változtatható méretű, seed alapján újrakeverhető permutációk könnyen analógiába állíthatók kriptográfiai titkosítófüggvényekkel.
Ezek általában dekódolhatóak, változtatható blokkméretűek és kulcs alapján újrakeverhetőek.
Felmerül a gondolat, hogy közvetlenül egy ilyen kriptográfiai módszert alkalmazzunk a permutáció megvalósításához.
Mindenképpen olyan megoldásra van szükség, amelynél a blokkméret tetszőlegesen megadható, és kellően flexibilis, skálázható.

A Feistel-hálózatok pontosan ilyenek, és emiatt gyakran használják is őket összetett titkosító eljárások keretrendszereként.
A Fesitel-hálózat egy többkörös eljárás, ahol minden körben az adat egyik felének bitjeit módosítjuk.
Ez alapesetben megköveteli a páros blokkméretet, de mindjárt kitérek arra, hogyan lehet ezen enyhíteni.

Az egyes körökben felváltva a bitsorozat bal illetve jobb oldali felét változtatjuk; nevezzük ezt célhelynek, a másik fél tartalmát pedig forrásbiteknek.
Egy-egy kör abból áll, hogy a forrásbitekre meghívunk egy az adott körhöz dedikált hossztartó hash-függvényt,
és az így kapott eredményt XOR-ral ráfésüljük a célhelyre.
Vegyük észre, hogy az XOR művelet tulajdonságai miatt ez reverzibilis művelet (bármi legyen is a hash-függvény),
ha újra alkalmazzuk, az eredeti bitsorozatot kapjuk vissza,
és a forrásbitek még rendelkezésre állnak, mivel azokat ezen a ponton még nem írtuk felül.
Tehát a Feistel-hálózattal kapott kód visszafejtése úgy történik, hogy a lépéseket visszafelé újra végrehajtjuk.
Általában nem szükséges, hogy az egyes körökhöz különböző hash-függvényt használjunk,
számunkra bőven elég, ha a páros és páratlan körökhöz eltérő függvényt alkalmazunk (szigorúan még ez sem lenne szükséges).

A Feistel-hálózatokkal való kísérletezés során észre vettem,
hogy a páros blokkméret követelménye enyhíthető a következőképpen.
Páratlan blokkméret esetén vegyünk még egy zéró bitet az adat végéhez,
amivel biztosítjuk, hogy a jobb oldali bitmező is a megfelelő méretű legyen.
Minden olyan kör végén, ahol a jobb oldali biteket írtuk felül, az utolsó bitet ezután nullázzuk.
A legvégén pedig csak az utolsó bit nélküli részt adjuk vissza.
Az utolsó bit ekkor minden művelet előtt és után is fixen zéró,
ami természetesen visszafelé haladva ugyanúgy teljesül, tehát a dekódolás is determinisztikus marad.
Ha egyúttal páros számú kört követelünk meg,
akkor az utolsó kör végére garantáltan ugyanabba a fázisba kerülünk, mint az első kör kezdete előtt;
vagyis a dekódolás folyamata teljesen egyezni fog az elkódolással, ami egyszerűsíti az eljárást.

\todo[inline]{Feistel: algoritmust csatolni}

Persze, még ha a páros blokkméret követelményét így sikerült is eliminálni,
továbbra is adott a probléma, hogy az $n$ blokkmérethez mindig $2^n$ méretű permutáció tartozik.
Vagyis, ha nem csak 2-hatvány méretű permutációkat akarunk támogatni, méretigazítás szükséges.
Ezt a rejection sampling technika alkalmazásával érjük el.
Az adott $n$ mérethez vesszük azt a legkisebb $N$ 2-hatványt, amely $n$-nél nem kisebb.
Amikor egy adott $i$ értékre a Feistel-hálótól az intervallumon kívül eső $p$-t kapunk,
azaz amelyre $p \geq n$ ($p < N$), akkor újra meghívjuk a permutáló függvényt $p$-re.
Addig ismételjük a hívást, míg végre $n$-nél kisebb szám nem születik.
A páratlan blokkméret megengedettsége a hatékonyság szempontjából is fontos,
mivel jelentősen csökkentheti az intervallumon kívülre esés esélyét,
és így a Feistel-hálózatra történő hívások számát.

\subsubsection{Permutációk méretigazítása}

A rejection sampling nem minden implementációnál hatékony,
de pontosan ezért vezettük be a $resized(newSize)$ metódust magán a $Permutation$ interfészen.
Így minden implementáció maga döntheti el, hogyan kell átméretezni.
Egy extrém eset például, amikor a $P_{+1}(i) = (i + 1) \mod N$ permutációt $n$-re kell átméreteznünk.
Rejection sampling módszerrel $i = n$ esetén $N - n$ számú extra iterációt kellene végrehajtani,
ami nagy méret esetén rendkívül költséges.
Helyette viszont egyszerűen áttérhetünk modulo $n$-re: $P_{+k}(k)(i) = i + k \mod n$.

\subsubsection{XXXX}

\todo[inline]{egy trükkös saját permutáció, ami elég jó szórást ad (ha sikerül megoldani)}

\todo[inline]{bitkeverés, kétfelől átfedő 2-hatványos permutálás stb.}

\subsection{HASH-FÜGGVÉNYEK}

\todo[inline]{Utalni a Fesitel-hálózatban (és egyebütt) történt használatra}

\subsection{VIRTUÁLIS ÉRTÉKKÉSZLETEK}

\subsubsection{EGYSZERŰEK...}

\todo[inline]{Számsávok, felsorolások stb.}

\subsubsection{Értékkészlet reguláris kifejezésből}

Karakterláncokból álló értékkészletnek egy reguláris kifejezés alapján történő generálása
talán a legáltalánosabb és legrugalmasabb módja az értékek definiálásának.
Emiatt ezzel a résztémával fogok a legrészletesebben foglalkozni,
és közlöm a fontosabb algoritmusok pszeudokódját is.

Véletlenszerű karakterláncok reguláris kifejezés alapján történő generálására
számos könyvtár elérhető a Java környezethez
(például a \textit{xeger} és a \textit{generex}).
Ezek olyan automaták, melyek az állapotgráfot véletlenszerű választásokkal járják be.
Ez a megközelítés önmagában nem alkalmas arra,
hogy az összes illeszkedő karakterláncból álló virtuális értékkészletet
a számunkra megfelelő módon implementálni lehessen vele.
Nem tudjuk ugyanis lekérni a rendezett értékkészlet $n$-edik elemét,
és nem tudjuk kikeresni, hogy adott érték mely pozíción található.

Lássuk, hogyan tudnánk valami módon mégis egy rendezett értékkészletet implementálni.
Első megközelítésben a reguláris kifejezéssel kapcsolatban az alábbi megszorításokat tesszük:

\begin{itemize}
    \item a reguláris kifejezés csak konstans karaktereket, karakterosztályokat, csoportokat és alternációt tartalmaz
    \item a rendezés karakterenkénti, azaz nincs másodlagos, harmadlagos stb. összehasonlítási szempont
\end{itemize}

Ilyen megkötések mellett a reguláris kifejezéshez egyértelműen rendelhető egy
irányított körmentes szógráf (DAWG),
melyben egy érték nélküli kiinduló elemből érjük el a karaktereket reprezentáló csomópontokat,
és kizárólag a szóvég csomópontokon (EOW) terminálhatunk,
melyekből már nem érhető el további csomópont.

A gráf lényegileg egy prefix fa, ahol egyes azonos részfákat újrahasznosítunk.
Az újrahasznosítás miatt a fa tárhelyigénye a reguláris kifejezéshez képest lineáris.
A szóvég csomópontok lesznek a fa levelei.
A prefix fa könnyen felépíthető a reguláris kifejezés szintaxisfájából (AST).

A gráf tömöríthető úgy, hogy konstans karakterek helyett karakterosztályokat engedünk meg.
Ekkor egy adott csomópontból elérhető csomópontok listájára az alábbiak érvényesek:

\begin{itemize}
    \item a lista elemei diszjunkt karakterosztályok
    \item bármely listaelem minden tartalmazott karaktere későbbi, mint bármely megelőző elem bármely tartalmazott karaktere
\end{itemize}

Világos, hogy ezek biztosításához átalakításokat kell nyerni a reguláris kifejezésekből
kapott nyers karakterosztályok listáján.
Ahol egymásnak alternatíváját jelentő két karakterosztály között átfedés van,
ott mindkét karakterosztályt szét kell bontani úgy,
hogy a kapott karakterosztályokból képzett halmazra a fentiek érvényesek legyenek.
Például ha az \texttt{[a-c]}, \texttt{[cf]} és \texttt{[c-e]} jelennek meg alternatívaként,
akkor ezek szétbomlanak az \texttt{[ab]}, \texttt{[c]}, \texttt{[de]} és \texttt{[f]}
karaketerosztályokra, és átveszik a korábbiakból elérhető csomópontokat.
A \texttt{[c]} karakterosztály egy alternációt vesz át a három eredeti csomópontból.
A fát érdemes egyúttal úgy normalizálni, hogy megszüntetjük az alternációk miatti üres értékű,
pusztán az elágazás kedvéért felvett csomópontokat (gyermekelemeket),
ez természetesen az egyenlő csomópontok részfáinak összefésülését vonja maga után.

Egy egyszerű reguláris kifejezés teljes szófája így néz ki:

\begin{figure}[H]
\centering
\includesvg[width=0.9\textwidth]{diagram/trie-simple}
\caption[Egyszerű reguláris kifejezés szófája]{
    A \texttt{([a-c]z(tt|uu)|a[s-z])} reguláris kifejezés szófája
}
\end{figure}

Látható, hogy minden csomóponthoz hozzárendeltük a hozzá tartozó részfa leveleinek számát.
Ez rekurzióval könnyen számítható.
Ez az információ már elegendő ahhoz, hogy kikeressük az $n$-edik elemet,
illetve hogy megkeressük egy adott karakterlánc (megtalált vagy beillesztési) pozícióját,
valamint hogy tetszőleges pozíciótól kezdve iteráljunk az illeszkedő karakterláncokon.

\todo[inline]{iteráció stb. algoritmusa}

Most pedig nézzük hogyan tudunk engedni a fenti megszorításokon.

A repetíciós operátorok explicite kifejtés által kerülnek visszavezetésre a fentiekre.
A kifejtés természetesen egyes esetekben lényegesen növeli a fa méretét a bemenethez képest.
Ennek kioptimalizálásával most nem foglalkozom.

Először a korlátlan repetíciót (\texttt{*}, \texttt{+} stb.) elmináljuk rendre az alábbi szabályokkal:

\begin{enumerate}
    \item \texttt{$A$+} ~ $\longrightarrow$ ~ \texttt{$A$\{1,\}}
    \item \texttt{$A$\{$n$,\}} ~ $\longrightarrow$ ~ \textt{$A$\{$n$\}$A$*}
    \item \texttt{$A$*} ~ $\longrightarrow$ ~ \texttt{$A$\{$c$\}} ~~~~~ (ahol $c$ előre definiált konstans)
\end{enumerate}

A fix repetíció egyszerű konkatenációvá alakul,
az opcionális repetíció kifejtése pedig a következő rekurzív módon történik:

\begin{enumerate}
    \item $R_1~=$ \texttt{$A$\{,1\}} ~ $\longrightarrow$ ~ \texttt{($A$|)}
    \item $R_n~=$ \texttt{$A$\{,$n$\}} ~ $\longrightarrow$ ~ \texttt{($AR_{n-1}$|)} ~~~~~ (ahol $n>1$)
\end{enumerate}

Az opcionális előfordulás (\texttt{?}) az $R_1$ esettel egyenlő.

Horgonyokat is implementálhatunk,
mint például szövegvég (\texttt{\$}) vagy szóhatár (\texttt{{\textbackslash}b}).
Ezeket a kezdeti feldolgozáskor úgy tekintjük,
mint az átementi üres csomópontokat, tehát elimináljuk őket,
de közben megjegyezzük megjegyezzük és sorban kikényszerítjük a teljesülésüket.
A horgonynak ellentmondó gyermekelemeket eltávolítjuk,
ha ezután nem marad gyermekelem, a teljesíthetetlen ágat jelző kivételt dobunk,
a kivételt elkapó szint el fogja dobni az ágat mint lehetetlent,
ami újabb kivételdobást eredményezhet, ha az ottani gyermekelemek emiatt elfogytak.

\todo[inline]{prefix-fa példa}

Az igazi nehézségek akkor kezdődnek,
amikor elengedjük a karakterenkénti rendezés megszorítását.
Erre például ékezetes karakterek vagy kis- és nagybetűk keveredése esetén lehet szükségünk.
Ekkor ugyanis esetleg csak egy későbbi eltérés dönti el,
hogy egy korábbi ékezet vagy nagybetűsség számít-e.
Vagyis nem elegendő a karakterenkénti prefix-fa a rendezettség biztosításához.

A Unicode Collation Algorithm (UCA) szabvány egy nyelvfüggő többszintű összehasonlítási sémát definiál,
de ezek között van olyan általános séma is, mely tekinthető egy nyelvfüggetlen összehasonlításnak.
Ennek egy egyszerűsített háromszintű változatát fogjuk alapul venni.

Minden karaktert három komponens,
nevezetesen az alapkarakter, a diakritikus jel és a kis-/nagybetűsség együttesének tekintünk.
Alapkarakternek a karakter diakritikus jelektől megfosztott kisbetűs alakját nevezzük.
Tehát például az '\texttt{Á}' karakter az '\texttt{a}' alapkarakterre,
a '\texttt{´}' ékezetre és a nagybetűsségre bontható.
Komponensenként egyértelmű és független lesz a rendezés,
ennek konkrét szabályait most nem részletezem.

Maguk a karakterláncok is három rétegre esnek szét, és komponensenként hasonlítandók össze.
Összehasonlításkor a későbbi komponens csak akkor vizsgálandó,
ha az azt megelőző komponensek a két karakterláncban egyeznek.
A pozícionált diakritikus jeleket és nagybetűsség-jelzéseket speciális karaktereknek is tekinthetjük,
ekkor lényegileg karakterenkénti összehasonlításra is visszavezethetjük az eljárást.
Például:

\begin{verbatim}
AL      =   al   __   __     =   al<U:0><U:1>
alom    =   alom ____ ____   =   alom
Alom    =   alom ____ U___   =   alom<U:0>
álom    =   alom ´___ ____   =   alom<D:´:0>
BAL     =   bal  ___  UUU    =   bal<U:0><U:1><U:2>
báL     =   bal  _´_  __U    =   bal<D:´:1><U:2>
\end{verbatim}

A jobb oldalon a karakterláncoknak egy normálalakja látható,
mely csak az alapkarakterek után csak a módosításokat tartalmazza,
és a kisbetűsséget alapértelmezettnek tekinti.
Természetesen másfajta normálalakot is választhatunk (például ahol a nagybetűsség az alapértelmezett),
akár a reguláris kifejezés tulajdonságaitól függően,
de mindenképpen rendelkeznünk kell bizonyos, egymással konzisztens algoritmusokkal,
amelyek együtt implementálják az értékkészlethez tartozó rendezési kollációt:

\begin{enumerate}
    \item karakterláncot és normálalakot egymásba alakító függvényekkel
    \item normálalakban lévő karakterláncokat összehasonlító komparátorral
    \item reguláris kifejezést a normálalak szerinti prefix-gráfba alakító algoritmussal
    \item a prefix-gráfot bejáró, kereső stb. algoritmusokkal
\end{enumerate}

\todo[inline]{a háromrétegű prefix-fa leírása}
% Az ötlet az, hogy egyetlen prefix-fa helyett hármat hozunk létre.
% A harmadik a normál prefix-fa, mely az eredeti karaktereket tartalmazza, karakterenként rendezve.
% A második ennek topológiai egyszerűsítése úgy, hogy a kis-nagybetű különbségeket összevonjuk.
% Ahol az összevonásnál azonos gyermekelemeket találunk, azok közül csak az egyiket tartjuk meg,
% de beiktatunk egy szorzó csomópontot (TODO: megmagyarázni).
% Az első fa szintén a normál prefix-fa egyszerűsítése,
% de itt minden azonos alapkarakterű karaktert összevonunk.
% A csomópontok számításánál figyelembe kell venni a szorzókat,
% amelyek felszorozzák a részfák levél-számát.

% az algoritmus:
    % - az első fát normál módon érjük el, azonban a leveleknél 1-nél nagyobb számosságok fognak maradni a szorzók miatt
    % - a második fát le kell szűkíteni az alapkarakterek szerint, ez alternatív számosságokat fog adni, ez alapján tudjuk számolni az alapkarater-fán belüli pozíciókat, itt is 1-nél nagyobb számosságokra lyukadhatunk ki
    % - a harmadik fát szintén szűkítjük a kisbetűs karakterek szerint
% a számosságok csak az érintett karakterekre számítandók, és cache-elhetők
% így elég hatékony algoritmust kapunk

\pagebreak

\appendix

\phantomsection
\addcontentsline{toc}{chapter}{\biblabel}
\printbibliography[title=\biblabel]
\cleardoublepage

\phantomsection
\addcontentsline{toc}{chapter}{\lstfigurelabel}
\listoffigures
\cleardoublepage

\phantomsection
\addcontentsline{toc}{chapter}{\lsttablelabel}
\listoftables
\cleardoublepage

\chapter{Projektlinkek}

\begin{itemize}
    \item GitHub organization: \par \url{https://github.com/miniconnect/}
    \item Használati példák: \par \url{https://github.com/miniconnect/general-docs/tree/main/examples}
    \item HoloDB projekt: \par \url{https://github.com/miniconnect/holodb}
    \item Docker Hub repository: \par \url{https://hub.docker.com/r/miniconnect/holodb/tags}
    \item \texttt{LargeInteger} benchmark: \par \url{https://github.com/miniconnect/miniconnect-api/tree/master/projects/lang/src/jmh/java/hu/webarticum/miniconnect/lang}
    \item HoloDB benchmark: \par \url{https://github.com/davidsusu/holodb-tdk/tree/main/benchmark}
\end{itemize}

\end{document}
