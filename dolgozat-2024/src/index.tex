\documentclass[
    parspace,
    noindent,
    nohyp,
]{elteiktdk}[2023/04/10]

\usepackage[dvipsnames]{xcolor}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{hyperref}
\usepackage{appendix}
\usepackage{float}
\usepackage{pgfplots}
\usepackage{arydshln}
\usepackage{fontawesome}
\usepackage{picture,xcolor}
\usepackage{pdflscape}
\usepackage{fancyvrb}
\usepackage{algorithm2e}
\usepackage[backend=bibtex,style=numeric]{biblatex}

\usepackage{todonotes}

\newcommand{\rhpad}{\vspace{0.6\baselineskip}}

\newcommand{\thesispar}[1]{
\vspace{1em}
\hspace{0.7cm}\parbox[left][][c]{15.8cm}{\linespread{1.2}\selectfont #1}
\vspace{1em}
}

\usepackage[newfloat]{minted}

\title{HoloDB: Relációs demóadatok on-the-fly generálása deklaratív konfigurációból}
\date{2024}
\author{Horváth Dávid \\ {\small\href{mailto:horvathdown@student.elte.hu}{horvathdown@student.elte.hu}} }
\degree{Programtervező Informatikus BSc}

\supervisor{Dr. Vincellér Zoltán}
\affiliation{Mesteroktató}

\university{Eötvös Loránd Tudományegyetem}
\faculty{Informatikai Kar}
\department{Információs Rendszerek Tanszék}
\city{Budapest}
\logo{elte_cimer_szines}

\addbibresource{references.bib}

\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}



\begin{document}

\documentlang{hungarian}

\listoftodos
\cleardoublepage

\makecover
\cleardoublepage

\maketitle

\tableofcontents
\cleardoublepage


\begin{abstract}
A modern szoftverfejlesztési módszertanok fő fókuszai közé tartozik
a gyors prototípusgyártás és a (kézi illetve automatizált) integrált tesztelés.
Mindkét esetben tesztadatokat kell biztosítanunk az alkalmazás működtetéséhez.
Ugyanakkor általában demóadatokra számos egyéb helyzetben is szükségünk lehet
(például mockolás, füst-teszt, szoftverbemutató, edukáció).
Nem csak a demóadatok előállítása (jellemzően generálás vagy anonimizálás)
jelent folyamatos terhelést a fejlesztési infrastruktúrára,
de azok indítási, működtetési és memóriaigénye is jelentős, hiszen hús-vér adatbázisokról van szó.
A pályamunkában egy jelentősen újszerű megközelítést mutatok be,
mely szükségtelenné teszi az adatok előzetes előállítását.
A HoloDB egy (felhasználói szemszögből szokványos) relációs adatbázis,
mely minimális indulási idővel és memóriaigénnyel futtatható
egy deklaratív konfigurációs fájlból, mely tömören leírja az adatbázis szerkezetét
és az adataival szembeni követelményeket.
Az on-the-fly generálás miatt maguk az adatok nem foglalnak memóriát,
ugyanakkor a szerver egy lekérések között is koherens adathalmazt szolgáltat.
Számos célmegoldás (köztük kriptográfiai módszerek) együttes alkalmazásával
koherens és gyors működést biztosító adatelérési technikák váltak lehetővé (például a virtuális indexek),
így a HoloDB (alkalmas skálázás mellett) runtime performanciában
sem marad alul a hagyományos adatbázisokkal szemben.
A relációs modell a megfelelő adapterekkel könnyen kiterjeszthető más adatmodellekre is
(például NoSQL, GraphQL).
Az új megoldás reményeim szerint további előnyök mellett
ötvözi a korábbi megoldások erősségeit, míg kikerüli azok fő hátrányait.
Valamint olyan új lehetőségek nyílnak meg, mint a „dev mode”
(az adatbázis azonnali frissülése a konfigurációs fájl módosításakor)
vagy a serverless adatbázis-szolgáltatás.
\end{abstract}

\chapter{(+++) Bevezetés}

\iffalse
% TODO: a bevezetőben utalni mindenféle korábbi generáló-módszerre
% PI-jegyek, fraktálok stb.

% TODO: az elején tisztázni, hogy ez nem implementáció-tanulmány,
%       hanem egy "proposal" (és egyúttal prototípus)
%       -- ezt az absztrakban is hangsúlyozni;
%       mindennapi probléma, de nem mindennapi megoldás;
%       sokféle helyzet, ugyanaz az "absztrakt" probléma
% TODO: alapkövetelményeket felsorolni, majd tételesen kifejteni
% TODO: az "Implementációs kérdések" kiemelt részletek csokra,
%       amelyeket a legfontosabbnak tűnik részletesen tárgyalni,
%       illetve megvalósíthatóságukat igazolni

--------------------------------------------

- Bevezetés (kezdés: egy mindennapi probléma arcai/inkarnációi/??? [???])
- A rendszer architektúrája
- Implementációs kérdések
- Proof of Concept / Gyakorlati eredmények / ??? (tényleg működik ez az egész, legalább nagyjából?)
- Összegzés
\fi

\section{(+++) BEVEZETÉS}

\todo[inline]{nem lehet pontos definíciót adni, legfeljebb bizonyos követelményeket, inkább körülírjuk}

Tisztázni:

- Javában van
- rengeteg kis alkatrészt kell összehozni, a vállalkozás lényege éppen ez az integráció

\section{(+++) ALKALMAZHATÓSÁG}

% ha mindezt meg tudjuk valósítani...

\todo[inline]{előnyöket itt tárgyalni: azonnali indulás, minimális memóriafogyasztás}

% TODO: azonnali indulás: lazy init-en még dolgozni!

\section{(+++) ÚJONNAN MEGNYÍLÓ LEHETŐSÉGEK}

\todo[inline]{serverless}

\todo[inline]{dev mode}

\todo[inline]{JPA annotációk}

\todo[inline]{letapogatás és renderelés, AI általi generálás}

\todo[inline]{zero mode}

\section{Tézisek}

\todo[inline]{Téziseket összeszedni}




\chapter{(???) RÉGI BEVEZETÉS}


\section{(???) Egy mindennapi probléma}

\todo[inline]{Hangsúlyozni és kifejteni, hogy a relációs algebrával itt nem foglalkozunk, és az integrált teszt PoC célú}

A szoftverfejlesztési és -release-elési folyamatok lényeges előfeltevése,
hogy az alkalmazás mint absztrakt entitás snapshot jellegű,
azaz a mindenkori forráskód \textit{aktuális állapotának} determinisztikus derivátuma,
nem függ annak történetiségétől.
Ezt az átlátható képet bolygatja meg a szoftverhez tartozó produkciós adatbázis,
mely önálló életet él, a felhasználói világgal való interakcióban folyamatosan változik;
szerkezete, szemantikája azonban szorosan az alkalmazás mindenkori állapotához kötődik.%
\cite{Ploski2007DatabaseCentricVersioning}%
\cite{Fluri2023DatabaseCiCd}

Ez a kettős természet legalább két nagy problémakört eredményez.
Az egyik a verziókezelést érinti:
a produkciós adatbázist csak differencia-szkriptek végrehajtásával tudjuk módosítani,
ami jellemzően egy másodlagos verziókezelést eredményez a forráskód verziókezelésén belül
(ilyen megoldás például a Flyway vagy a Liquibase),
és sorozatos összefésülési problémákhoz vezet, ha az adatok szerkezete vagy szemantikája gyakran változik.
A jelen dolgozatban nem foglalkozom ennek a kérdésnek a részleteivel,
de az utolsó fejezetben vázolom majd egy lehetséges megoldás körvonalait,
ami szervesen következik majd az azt megelőzőekből.

A másik problémakör azzal kapcsolatos, hogy ugyan az életnek kitett produkciós adatbázist
kénytelenek vagyunk minden erőforrásigényével életben tartani, valamint minden release-zel migrálni,
ám az \textit{absztrakt értelemben vett} adatbázisra számos egyéb helyen is szükségünk van,
ahol ezek a folyamatos költségek igencsak nemkívánatosak.
Ilyenek esetek jellemzően:

\begin{itemize}
    \item adatfüggő mockok és stubok biztosítása
    \item integrációs és egyéb tesztelés
    \item gyors prototípusgyártás, kísérletezés draft adatbázisokkal
    \item rövid demonstrációk, prezentációk
\end{itemize}

Ezeken kívül, ha egy csak-olvasható adattartalmat akarunk nagy adatforgalomnak kitenni,
fölmerülhet az adattartalom replikálása, és load-balanceren keresztüli elérhetővé tétele,
ekkor azonban minden újabb példány jelentősen növeli a tárhelyfoglalást.

Más területeken is találkozhatunk a fentiekhez hasonló kívánalmakkal.
Például az adatbázisok oktatásában is jellemző, hogy minden egyes hallgató
számára saját izolált adatbázist bocsátanak rendelkezésre,
bár mindegyik ilyen adatbázis szükségszerűen ugyanazzal a szerkezettel és adatokkal bír.

Fogalmazzuk meg tehát a problémát, mintegy tézisként:

\thesispar{
    \textbf{Alapmegállapítás:} Igen gyakran ütközünk abba a helyzetbe,
    hogy adott struktúrájú és szemantikájú, viszonylag rövid életű adatbázist kell biztosítani,
    a produkciós adatokra vonatkozó speciális követelmények nélkül.
}

A következő szekcióban röviden végigveszem, hogyan szokás általában a hasonló problémákat kezelni.


\section{(???) Körkép}

\subsection{(???) Kérések mockolása on-the-fly}

Főként webes API-k által szolgáltatott adathalmazok mockolásakor találkozunk olyan megoldással,
mely a lekérés (és persze az előre beállított szabályok)
alapján generálja a válaszként visszaadandó adatstruktúrát
(például egy rekordot vagy dokumentumot JSON formátumban).
Az adatok véletlenszerűen kerülnek kitöltésre a lekérés szerkezete alapján megállapított séma keretébe.

A módszer SQL-lekérdezések közvetlen mockolásához is adaptálható,
a lekérdezés (és ha ismert, akkor a séma) alapján meg kell állapítani
a visszaadandó eredménytábla szerkezetét, adattípusait,
az egy-egy oszlopba kerülő adatok jellegét,
majd random jelleggel elő kell állítani bizonyos számú, ezeknek a követelményeknek megfelelő eredményrekordot.

Ez a megoldás természetesen nem fog konzisztens eredményt adni több lekérés esetén,
így csak olyan esetben használható, ahol a lekérések nem épülnek egymásra.
Cserébe erőforrásigénye csekély, nincs szükség az adatok tényleges tárolására.

\subsection{(???) Seed-alapú virtuális világok}

Játékokban és egyszerűbb szimulációkban már igen régóta generálnak kisebb világokat,
pályákat procedurálisan valamilyen pszeudovéletlen algoritmus segítségével.
A módszer továbbfejleszthető óriás világokra is.
Ekkor a világteret (sorosan vagy hierarchikusan) szeletekre osztjuk, és biztosítunk egy függvényt,
mely minden szelethez rendel egy saját seed értéket.
Legegyszerűbb esetben a seed az adott szelet sorszámából generált hash.
Amikor szükségünk van valamely szelet tartalmára, a szelet seedjéből kiindulva,
az adott szabályrendszer szerint on-demand legeneráljuk az adott szelet tartalmát.
A sűrűn benépesített világok szomszédos szeleteinek összeillesztése néha nem triviális.
Felületszerű, folytonos struktúrák generálása esetén gyakran használnak
valamilyen eleve interpolált eljárást, például Perlin-zajt,
így biztosítva a közeli helyeken a közeli értékeket.

Egy közismert korai példa seed-alapú módszert alkalmazó szoftverre
az \textit{Elite} nevű 1984-ben kiadott űrhajós videójáték.
A játékban meglátogatható bolygók kapnak egy-egy seedet, mely már teljesen meghatározza a felépítésüket,
így elég a bolygó meglátogatásakor legenerálni a tartalmát.
A módszert utána számtalan óriástérképes játék vette át.

Ha elvonatkoztatunk a játékoktól és szimulációktól,
ez a generálási módszer közel tetszőleges jellegű virtuális adathalmaz böngészését képes biztosítani.
De azonnal szembeötlik a legfőbb hátrány is: a virtuális világ-adathalmaz nem kereshető.
Nem tudjuk például hatékonyan lekérdezni, mely világszeletekben lelhető föl egy adott típusú objektum.

\subsection{(???) Relációs adatok generálása}

Amikor adatokat generálunk egy relációs modellbe, azt legegyszerűbb megközelítésben fölfoghatjuk úgy is,
mint az előbb bemutatott virtuális világ materializációját egy relációs adatbázisszerveren.
Milyen előnyöket nyerünk ezzel? Lássunk néhányat:

\begin{itemize}
    \item utófeldolgozás: utólag holisztikus módosításokat eszközölhetünk az adathalmazon
    \item egynemű módosíthatóság: minden eleve tárolva van, nem kell külön kezelni a változásokat
    \item kereshetőség: az adatbázis-kezelő biztosítja az indexeket
\end{itemize}

A fő hátrány egyértelmű: a nagy tárhelyigény.
Vegyük észre, hogy még a kereshetőség is az indexek által foglalt további tárhely által valósul meg
(az adatok eleve nagy tárhelyfoglalása mellett tűnhet esetleg elhanyagolhatónak).
Ha pedig nem materializált adatok mellé gyártunk indexeket,
akkor ki kell dolgoznunk valamilyen módszert a módosulások detektálására, ami általában nem triviális.

Egyes generátorok a séma és az adatok jellegének deklaratív leírását is támogatják. Ez a leírás tárolható egy konfigurációs fájlban, ami verziókezelhető, dokumentálható, könnyen szerkeszthető.

\subsection{(???) Relációs adatok anonimizálása}

Ha már rendelkezésre áll egy éles adatbázis, kézenfekvőnek látszik,
hogy ennek replikáit használjuk a teszt- vagy mockadatbázis kiindulópontjaként.
Így rögtön egy valószerű adathalmazzal indulunk,
az érzékeny adatok miatt azonban anonimizálásra van szükség.
Tehát a generáláshoz szükséges számítási igényt összességében
felcseréljük a másolás és anonimizáció költségeivel.

Úgy tűnik, gyakran anonimizálással együtt is egyszerűbb dolog meglévő adatokból kiindulni,
mint egy komplett generálási folyamatot kiépíteni.
Egy köztes megoldás lehet a séma és az adatok nagyjábóli jellegének letapogatása,
és az ez alapján történő újragenerálás.

Azonban a bevezetőben felsorolt szituációk egy részében
nem is áll rendelkezésre egy létező produkciós adatbázis.
És ha rendelkezésre is áll, további probléma,
hogy az adatok megszerzéséhez az éles adatbázist kell felkeresni,
ami egyfelől az éles környezet nemkívánatos terhelését jelenti,
másrészt az oda való beauthentikálás (a konkrét megoldástól függően) biztonsági kockázatot rejthet.

Egy általános megoldás tehát aligha épülhet anonimizálásos módszerre.

\subsection{(???) Nagy teljesítményű megoldások}

Számos nagyteljesítményű szolgáltatás érhető el a piacon szintetikus adatok generálásához.
A MOSTLY AI megoldása az akkurátus adatokra helyezi a hangsúlyt.
Az adatok statisztikai jellemzői és mintázatának lényeges elemei
jól illeszkednek a valós adatoknál látható mintázatokra.

Egyes platformok általános megoldást nyújtanak az adatgenerálásra, anonimizációra és CI/CD-integrációra.
Ilyen például a Tonic, a GenRocket és a Delphix.

Ezek a rendszerek erősen optimalizálják és párhuzamosítják a tesztadatok előállításának egyes lépéseit,
így a fapados megoldásokhoz képest költséghatékonyak.
Csodát viszont nem lehet várni: a számításigény aszimptotikusan,
a tárhelyigény pedig teljes egészében ugyanaz marad.


\section{(???) Egy újfajta megoldás körvonalai}

Az előzőekben tárgyalt megközelítések mindegyikének megvoltak a maga sajátos előnyei ás hátrányai.
Fölmerül a kérdés: létezhet-e olyan megoldás, amely egyesíti az előnyöket és kikerüli a fő hátrányokat;
vagy pedig ez eleve lehetetlen?
Nincs-e itt egy beöltésre váró hézag?

Először szedjük össze tételesen, milyen követelményeket várnánk el
egy (egyelőre hipotetikus) ideális készterméktől:

\begin{itemize}
    \item támogatja a relációs adatmodellt, mint legáltalánosabbat
    \item determinisztikus, konzisztens, és egy root seed módosításával újrakeverhető
    \item azonnal elindul, nincs számottevő preparálási folyamat
    \item az adatokat csak elérésükkor, on-the-fly számítja
    \item indexeket szolgáltat, ahol az egyébként is elvárható lenne
    \item könnyen és rugalmasan konfigurálható, fájlon keresztül is
    \item óriás adatmennyiséget is képes szimulálni
    \item skálázható, finomhangolható
    \item egyedi működéssel könnyen bővíthető
    \item opcionálisan írható
\end{itemize}

Az utolsó, írhatósági követelménynél föltehetjük, hogy az ilyen adatbázis viszonylag rövid életű,
tehát jellemzően nincs szükség sem nagy mennyiségű módosítás megjegyzésére,
sem azok hosszú távú perzisztálására.
Így az egynemű módosíthatóság fentebb említett előnyét nyugodtan elvethetjük:
ha egyszer a módosítási réteg megfelelően és univerzálisan implementálva van,
az egyes felhasználásoknál már nincs ezzel kapcsolatos teendő.

Belátható, hogy a fenti elvárások egyszerre teljesíthetők.
Az alábbi szekcióban röviden vázolom, milyen előnyöket nyerünk így,
az implementáció részleteit pedig a következő fejezetben fogom bemutatni.


\section{(???) Az új megközelítés előnyei}

Egyszerű esetben a felhasználói élményben három lényeges fázist különíthetünk el.
A figyelem először a konfigurációs fájlra irányul:
egyetlen átlátható, deklaratív, verziókezelhető adatfájlban minden megadható, amire szükség van.
Majd az adatbázis létrejöttekor és a szerver elindulásakor látható,
hogy teljesen hiányzik a szokásos másolási, generálási, anonimizálási, inicializálási vagy egyéb folyamat,
a szerver töredékmásodperc alatt válaszképesen elindul.
Végül a szerver rendkívül kis memóriaigénnyel fut\footnote{
    Ami persze a betöltött plusz adatforrások és az adatmódosítások arányában megnövekedhet.
},
ugyanakkor a lekérdezések futási ideje sem marad el nagyságrendekkel a tényleges adatbázisétól.

Az integrált konfigurációs fájl előnye még,
hogy létező adatbázispéldányokon tanított mesterséges intelligencia segítségével is generálható.\footnote{
    Ígéretes, hogy mindössze egy példafájl és pár instrukció után
    a ChatGPT-4 is meglehetősen jó eredményt generál.
}
A gépi tanulással segített konfigurációmenedzsment egyre szélesebb körben alkalmazott módszer,
megtaláljuk a SAT-solverektől\cite{Hoos2021AutomatedCA} a felhőarchitektúrákig\cite{Osypanka2022ResourceUC},
és nem látszik elvi akadálya az adatbázisra vonatkozó konfigurációban való alkalmazásnak.

De ilyen vaskos módszerek nélkül, egyszerűbb heurisztikákkal is lehetséges
egy létező adatbázis jó közelítéssel történő letapogatása.
Hasonlóan, a virtuális adatbázis könnyen materializálható egy valós adatbázisba.

A könnyűsúlyú jelleg olyan új távlatokat nyit, amelyek korábban föl sem merülhettek.
Például most már nincs akadálya, hogy bármely füstteszt adatbázis-eléréssel fusson.
A CI/CD pipeline-okban adathalmazok kívülről való betöltése nélkül is lehet rövid feladatokat futtatni.
A megoldás adatbázistervező eszközökbe való integrálással lehetővé tudnánk tenni,
hogy az épp aktuális tervállapot gombnyomásra teljes értékű adatbázisként elinduljon.

Azt hiszem, jogosan merül föl a kérdés, hogy miért nem született és terjedt el még hasonló termék.
Ennek fő okait az alábbiakban látom:

\begin{itemize}
    \item \textbf{kontraintuitivitás}: előzetes feltevésemmel szemben úgy tűnik,
          nem olyan egyszerű átlátni a koncepciót, és megérteni hogy „hol is vannak az adatok”
    \item \textbf{megfelelő SQL plannerek eddigi hiánya}: az Apache Calcite például
          csak az utóbbi néhány évben vált népszerűvé, és inkább elsősorban adatintegrátorként
    \item \textbf{bejáratott alternatívák}: az on-the-fly mockolás lehetősége,
          a NoSQL adatbázisok és felhőmegoldások fejlődése stb.
          sztenderdképző kerülőmegoldásokként elfedték a problémát
    \item \textbf{érdekhiány}: a releváns területen működő nagyobb vendorok elsősorban
          erőforrásokat adnak bérbe, és az általuk fejlesztett architektúrák (történetileg érthetően)
          elsősorban az ezeken futó tényleges adatbázisok kezelését kell megoldják;
          nincs tehát nyomás jelentősen más típusú termékkel való kísérletezésre
\end{itemize}



\chapter{Architektúra}

\section{A nagy illúzió: adatok a semmiből}

Olyan adatszolgáltatás felépítésére vállalkoztunk tehát,
mely egy \textbf{tetszőlegesen nagy méretű, koherens és kereshető} adattárat imitál,
miközben mindebből semmi sem létezik fizikálisan.
Hogy egy koherens adattároló látszatát elérjük,
apró eszközök egész tárát kell összeverbuválni,
akár olyan távol eső területekről mint az automataelmélet és a kriptográfia.
Mivel a cél egy felhasználói szemszögből is áttekinthető és rugalmas eszköz biztosítása,
választanunk kell egy kellően általános adathozzáférési modellt a virtuális adathalmaz prezentálásához.
Ez az adatmodell a relációs adatmodell lesz,
melyhez egy konkrét \textbf{relációs storage API}-t fogunk definiálni.
Aligha érdemes más megközelítést választani,
hiszen a relációs adatmodell nem csak tradicionálisan megkerülhetetlen
és máig a legnépszerűbb adattárolási módszer,
de egyúttal egy olyan általános adatelérési normalizációs szabvány,
mely közös nevezőként tud szolgálni további modellhez (pl. no-sql, gráfadatbázisok).

A legtöbb alkalmazásban elkülöníthető valamilyen fokális entitástípus,
mely alapvető szerepet tölt be az architektúrában
(például egy könyvtári katalógusban a \textit{könyv}).
Célszerű lehet ilyen entitást esetünkben is meghatározni,
hogy segítse az architektúrában való eligazodást.
Úgy tűnik, a virtuális adattár legalapvetőbb objektuma az \textbf{\textit{értéklista}}.
Az értéklista egyszerű esete valamely egyszerű \textit{értékkészlet},
az API-ban pedig egy-egy \textit{oszlop} értéklistájával fogunk találkozni.

Egy oszlopban azonos típusú adatok gyűjteménye szerepel.
Ezeket az adatokat általában egy adott \textit{értékkészlet}ből válogatjuk össze.
Jó, ha az értékkészlet rendezett, kereshető;
mint majd látjuk, ez az egyszerű megkötés már általában elégséges ahhoz,
hogy maga az oszlop is kereshető legyen.

Néha esetleg ki kell lépnünk a szigorú oszlop-orientáltságból.
Lehetnek például összefüggések egy táblán belüli oszlopok között.
Ezt azonban még mindig egyszerűbb az oszlopos megközelítés megfelelő kiterjesztésével elérni,
mintsem áttérni valamiféle rekordonkénti generálásra.
Úgy is mondhatjuk, hogy az oszlopok alatti mezők jó jelöltek arra, hogy „világszeletek” legyenek,
míg egész rekordok nem.

Lehetnek továbbá megkötések táblák között is.
Ezek kezelése is könnyen elvégezhető az oszlopos alapvetésre építve.
Az egyoszlopos idegen kulcsoknál például csak annyit kell biztosítani,
hogy a hivatkozó oszlop értékkészletét a hivatkozott oszlop tartalma adja.

A mindenkori módszert, amivel egy oszlop értéklistáját biztosítjuk,
\textit{értékkiosztás}nak fogom nevezni.
A virtuális értékkiosztások olyan függvényt valósítanak meg,
mellyel egy adott mező értéke determinisztikusan előállítható.
De az igazán lényeges kihívást az jelenti, hogy szeretnénk,
hogy az értéklistában keresni is tudnánk, méghozzá hatékonyan,
hasonlóan ahhoz, mint amikor tényleges adatbázisok esetén
az oszlophoz generálva van egy index.
A virtuális adattár esetén is lehetséges lenne indexeket generálni,
de ez az adatok materializálásával járna,
a virtuális adattár mégse lenne többé virtuális;
ez annyit jelent, hogy úgymond az indexek is virtuálisak
(ennek oda-vissza koherensnek kell lennie; itt jön majd képbe a kriptográfia).
Elsősorban olyan megoldásokat fogok bemutatni,
amelyek teljesítik a kereshetőség erősebb feltételét is,
azaz \textbf{kereshető virtuális értékkiosztások}.

A legtöbb oszlop értékkészlete jól behatárolható.
Az oszlopban tételesen szereplő értekek egy efölötti
tetszőlegesen sorrendezett multihalmazt (tömböt) alkotnak,
néha ehhez valamilyen statisztikai megkötés is tartozik ($NULL$-ok száma, értékgyakoriság stb.).
A tábla rekordjait mindig sorrendezettnek fogom tekinteni:
azonos táblabeli valamely két oszlop értéklistájából kiválasztott egy-egy mező
azonos rekordhoz tartozik pontosan akkor, ha a sorrendbeli pozíciójuk (sorindexük) megegyezik.

Vegyük észre, hogy két független problémával szembesülünk:

\begin{enumerate}
    \item az oszlopértékek monoton rendezett multihalmazának előállítása
    \item az értékek összekeverése
\end{enumerate}

Mivel az oszlopot általában értékkészlettel definiáljuk, ebből három független probléma lesz:

\begin{enumerate}
    \item az értékkészlet biztosítása
    \item az értékkészlet monoton rendezett multihalmazának képzése
    \item az értékek összekeverése
\end{enumerate}

A multihalmaz itt egy monoton növekvő tömböt jelent,
ezt fogjuk még egy további lépésben összekeverni egy permutáló függvény alkalmazásával.
Ennek a többlépéses módszernek az átláthatóságon kívül az is nagy előnye,
hogy a lépések egymástól függetlenül skálázhatók.

Ezeket úgy kell megvalósítani, hogy a végső értéklista adott pozíciójú eleme is gyorsan lekérhető legyen,
valamint egy adott érték vagy értéksáv előfordulási pozíciói is
(keresés).
A perrmutáció esetén ez annyit jelent, hogy legyen invertálható.

A \textbf{kétfázisú értékkiosztásnak} ezzel az ötletével
egy egyszerűbb adatbázis szimulációjának kívánalmait már tulajdonképpen le is fedtük,
de néhány további hasznos értékkiosztási módszert is be fogok majd mutatni.

Ha oszlopokat elő tudunk állítani,
akkor a táblák és egyéb objektumok köréépítése nem jelent kihívást.
Ha pedig a relációs adatmodell ezen objektumait sikeresen implementáltuk,
a relációs műveletek és \textbf{az SQL lekérdezések megvalósítása már a hagyományos módon történhet}.
Éppen ezért a továbbiakban a lekérdezések problémakörével csak érintőlegesen foglalkozom.
Ma már elérhető néhány nyílt forrású jól felépített SQL query planner framework is,
melyek megkönnyítik egy teljes körű SQL-szerver kiépítését.
Ilyen például az általam használt Apache Calcite.
Ugyanakkor egy saját SQL-futtatót is implementáltam, mely sok esetben gyorsabban fut,
cserébe csak egy limitált SQL-t támogat.


\section{(XXX) (MÁS CÍM!!!!) A prototípus}

\begin{figure}[H]
\centering
\includesvg[height=0.8\textwidth]{diagram/simplearch}
\caption{A virtuális adatbázis vázlatos architektúrája}
\end{figure}

A virtuális adatbázist megvalósító szoftvert HoloDB néven tettem elérhetővé.
Az elnevezés a hologramokra utal:
a konfigurációból leképzett, „kivetített” virtuális adatbázis hasonlóan illuzórikus,
mint a hologramból előhívott térbeli látvány.

A HoloDB sok tekintetben már kinőtte a prototípus státuszt,
hiszen egy nagyívű rétegelt architektúráról van szó,
mely kellően megszilárdult alapvonalakkal rendelkezik a hosszútávú fejlesztéshez.
Ugyanakkor stabil verzióról még nem beszélhetünk;
az implementációs részleteket, valós használati eseteket,
más eszközökkel való integrációt illetően még számos felmérendő, kikísérletezendő dolog adódik.

A szerver Docker konténerként is futtatható, amihez a Docker Hubon biztosítok egy testreszabott képet.
A felhasználói Dockerfile mellé csak a konfigurációs fájlt kell elhelyezni az indításhoz.
A testreszabott kép a kis erőforrásigény miatt serverless végpontként is jól használható.

A HoloDB a MiniBase projektre épül.
Utóbbi egy relációsadatbázis-keretrendszer, mely definiálja a storage API-t,
és annak konkrét implementációjához agnosztikusan viszonyulva
egy komplett adatbázisszervert épít fölé;
beleértve a lekérdezések fordítását és futtatását,
a felhasználói munkamenetek, tranzakciók és változók kezelését,
a runtime sémamódosításokat illetve új táblákat és más felelősségi köröket.
A HoloDB ehhez a storage API virtuális implementációját,
a konfiguráció kezelését
és a megfelelő futtatási környezetet adja hozzá.

\todo[inline]{JDBC kritikája: ne jegyzetbe, lenyesni/tárgyilagosítani, akár elhagyni}

A MiniBase közvetlenül biztosít egy SQL-futtató felületet, a MiniConnect API megvalósításával.
A MiniConnect testvérprojekt célja egy egyszerű és egyértelmű adatbázis-elérési API definiálása,
melyet a JDBC nehézkessége inspirált.\footnote{
    Nem áll módomban ehelyt a JDBC részletes kritikájába bocsátkozni.
    De milyen problémát is old meg a \texttt{nativeSQL()} metódus?
    Egyáltalán miként egyszerűsítené a helyzetet ez a kellemetlenül rengeteg függvény?
    Miért származik a \texttt{PreparedStatement} a \texttt{Statement} típusból?
    Hosszan folytathatjuk a sort.
}
A MiniConnecthez különféle middleware változatok is elérhetők, például hálózaton keresztül is használható,
illetve letisztultsága miatt könnyen írhatók hozzá dekorátorok, proxyk.
Egy (deb csomagként is elérhető) REPL is tartozik hozzá,
mellyel parancssorból tudunk hálózaton keresztül csatlakozni a MiniConnectet támogató tetszőleges adatbázishoz.
Ez egyúttal a HoloDB kipróbálásának legkényelmesebb módja,
számos kényelmi funkciót támogat (szép kimenet, syntax highlight, parancstörténet stb.).

Az architektúra köré sokféle további eszközt fejlesztettem,
melyek a robusztusság eltérő fokain állnak.
Ilyenek például: automatikus REST API, GraphQL adapter, authentikációs proxy, Apache Calcite SQL driver stb.


\section{A relációs storage API}

Ha definiáltuk a relációs adathozzáférés fő entitásait,
akkor ezek megfelelő implementációi már megvalósítanak egy relációs adattárat,
mely funkcionálisan megfelel egy tényleges adatbázisnak.
De melyek legyenek ezek az elemek?
A kérdést elsőként a \textit{hozzáférés} szempontjából érdemes megközelíteni.
Az írhatóság problémakörével később foglalkozom.

Nyilvánvalóan gondoskodni kell az alapvető hierarchiáról,
ezek lesznek a \texttt{Schema}, \texttt{Table} és \texttt{Column} típusok.
Az oszlopok elvont tulajdonságait a \texttt{ColumnDefinition} típus fogja össze.
Bár az egyes mezők értékeit általában oszloporientáltan állítjuk elő
(jellemzően az értéklistákat megadó \texttt{Source} objektumok használatával),
a lekérésnél több szempontból is intuitívabbnak és előnyösebbnek mutatkozik
a rekordorientált megközelítés.
A rekordot a \textt{Row} típus reprezentálja,
ezen keresztül kérhetőek le a konkrét mezőértékek
az oszlopnév vagy oszloppozíció megadásával.

Az adatok hatékony keresését virtuális indexek fogják segíteni,
itt három ilyen indexfajtával fogok foglalkozni:
a normál \texttt{TableIndex}, a \texttt{FulltextIndex} és a \texttt{SpatialIndex} típusokkal.
Ezek keresőmetódusai egy \texttt{TableSelection} példánnyal térnek vissza,
melyen keresztül a találati listához férünk hozzá.

A \texttt{TableIndex} a szokásos rendezett indextípus,
mely egyúttal teljes kontrollal rendelkezik a találati lista rendezése
és a $NULL$ értékek kezelése fölött.
Ha többoszlopos, akkor az oszloplista bármely prefixére is indexként működik (leftmost prefix lookup).

A másik két típus találatainak rendezése definiálatlan.
Egyik sem ad vissza $NULL$ mezőket.

A következő osztálydiagramon a fenti interfészeket foglaltam össze,
a tömörség kedvéért néhol kicsit vázlatosan:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{diagram/storage-api-uml.png}
\caption{A storage API fő interfészei (egy relációs adattár alapontológiája)}
\end{figure}



\section{Az írhatósági réteg (EGYBEVENNI!)}

\todo[inline]{Az írhatósági réteg: nem kellenek alfejezetek}

\subsection{A \texttt{DiffTable} dekorátor}

A virtuális adattár egy-egy példánya várhatóan viszonylag rövid élettartamú lesz.
Feltehetjük, hogy az írási műveletek összességében kevés módosítást eredményeznek.
A tipikus szcenáriókban főként egyes konkrét rekordok lesznek módosítva, hozzáadva, törölve.
A tömeges módosítások kérdésével itt nem foglalkozom,
de nem látszik akadálya az ezek optimalizálását célzó továbbfejlesztésnek.

Az írhatósági réteget arra is használhatjuk,
hogy egy-egy tesztesethez egyéni rekordokat adjunk hozzá,
ami megkötéseken keresztül nehézkesebb lenne.

A módosítások támogatásához előszöris a \texttt{Table} interfészt magát
ki kell egészíteni a megfelelő műveletekkel
(pl. \texttt{isWritable()}, \texttt{applyPatch(TablePatch)}, \texttt{sequence()}).
A csak-írható implementáció persze az írási műveleteket nem fogja támogatni.

A \texttt{DiffTable} egy tetszőleges \texttt{Table} implementáció fölé elhelyezhető dekorátor.
Teljesen áttetszően működik: amíg nincsenek módosítások,
minden műveletet egyszerűen továbbít a dekorált tábla felé.
Módosítás esetén a különbségeket (módosított értékek, beszúrt és törölt sorok) a memóriában tárolja.
A további lekérdezésekkor figyelembe veszi a különbségréteget a továbbhíváskor,
illetve a szükséges mértékben összefésüli az eredményt a különbségekkel.

Olyan táblák is dekorálhatók így, amelyek valós adatbázistáblákat használnak backendként.
Ezzel a módszerrel írhatóan használhatunk egy létező adatbázist,
az eredeti adatok módosítása nélkül.

\subsection{Generikus tranzakciókezelés}

A fő elv, amit a tranzakciókkal kapcsolatban követni fogunk,
hogy megmaradjunk a réteges felépítésnél,
és semmiképp ne a táblákra toljuk a tranzakciókezelés terhét.\footnote{
    Mint több adatbáziskezelőnél láthatjuk (a \textit{H2} például tipikusan ilyen).
    A munkamenetek kezelésének a táblainterfész alá tolásával
    különféle architekturális nehézségek jelentkeznek.
}

Egy teljesen általános tranzakciókezelési módszer,
amikor a szerver két állapotát különböztetjük meg,
a \texttt{READ} és a \texttt{WRITE} státuszokat.
Feltételezzük, hogy jóval több olvasási kérés érkezik be, mint írási.

Az alapértelmezett \texttt{READ} státuszban feltétel nélkül,
konkurensen szolgáljuk ki a beérkező olvasási lekérdezéseket.
Ha azonban egy módosító lekérdezés illetve tranzakció érkezik,
elhelyezzük a módosító műveleteknek fenntartott queue-ba,
és átváltunk \texttt{WRITE} módba.

\texttt{WRITE} módban a beérkező olvasási illetve írási műveletek
a számukra fenntartott két dedikált queue-ba kerülnek.
Először kiszolgáljuk a még folyamatban lévő olvasási műveleteket,
ezután sorra szekvenciálisan kivesszük és végrehajtjuk a módosítási queue elemeit, amíg van ilyen.
Ha már nincs elem a módosítási queue-ban,
akkor az olvasási queue összes elemét felszabadítjuk, és továbbítjuk párhuzamos végrehajtásra,
miközben visszaállítjuk a \texttt{READ} státuszt.

Új módosító művelet érkezésekor a ciklus újrakezdődik.

\subsection{Snapshot-alapú tranzakciókezelés}

A \texttt{DiffTable} dekorátor minden további nélkül elhelyezhető
más \texttt{DiffTable} objektumok fölé is, ami lehetővé teszi, hogy egymásra épülő snapshotokat tároljunk.

Így könnyen megvalósítható a \textit{multiversion concurrency control} (MVCC),
amikor a kiinduló állapot snapshotja a módosító műveletek közben is rendelkezésre áll,
azaz az olvasási műveletek ekkor is zavartalanul futtathatók.
Az írási tranzakció végeztével az érintett \texttt{DiffTable} példányok
beküldik a saját módosítási rétegüket a dekorált táblába,
így az belefésülésre kerül a megosztott adattérbe.

A snapshotok alkalmazásából a tranzakciós savepointok támogatása is természetesen következik.
Egy savepointra való visszaugrás egyszerűen a megfelelő szintre való visszatérést jelenti
a \texttt{DiffTable} dekorátorok egymásra épülő hierarchiájában.

Mivel a snapshotkezelés táblánként függetlenül működtethető,
párhuzamosan is futtathatunk írási tranzakciókat,
amíg azok különböző táblákon futnak.





\section{(XXX) Konfiguráció}

\textcolor{red}{A virtuális adatbázis felépítése a felhasználó számára legegyszerűbben
egy konfigurációs fájl segítségével történik,
melyben megadja az adatbázis-séma egyes elemeit, megkötéseit.
Az oszlopok tartalmának keverési módja és értékkészlete kényelmesen állítható,
például előre adott értéklistákkal, reguláris kifejezéssel,
vagy akár a felhasználó által implementált Java-osztállyal.
A virtuális adatbázis Dockerben való indításához vagy beágyazott használatához
lényegileg elég egy ilyen konfigurációs fájl.}

A HoloDB egy YAML formátumú konfigurációs fájl segítségével paraméterezhető fel,
melyben az adatbázis minden lényeges eleme beállítható.
A konfigurációhoz formális specifikáció is tartozik,
így a fájl strukturális és szemantikai helyessége statikusan ellenőrizhető.
A konfigurációt hordozó Java-osztályok egy programkódban is használhatók,
ez csak egy dedikált, minimalisztikus csomag függőségként való beemelését igényli.

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.47\textwidth}
    \begin{minted}[fontsize=\small,framesep=4pt,frame=single,framerule=1.5pt,rulecolor=\color{gray!30}]{yaml}
seed: 425364
schemas:
  - name: shop
    tables:
      - name: customers
        size: 5
        columns:
          - name: id
            mode: COUNTER
          - name: firstname
            valuesBundle: forenames
          - name: lastname
            valuesBundle: surnames
          - name: birth
            valuesRange: [1950, 2000]
      - name: orders
        size: 12
        columns:
          - name: id
            mode: COUNTER
          - name: cid
            valuesForeignColumn:
              [customers, id]
          - name: product
            valuesBundle: fruits
          - name: quantity
            valuesRange: [1, 10]
    \end{minted}
  \end{minipage}
  \hspace*{\fill}
  \begin{minipage}[t]{0.35cm}
    \vspace{7cm}
    {\Large $\Rightarrow$}
  \end{minipage}
  \hspace*{\fill}
  \begin{minipage}[t]{0.45\textwidth}\begin{center}

    \vspace{1.2cm}

    \texttt{customers}
    \vspace{0.1cm}

    \begin{tabular}{ |r|l|l|r| }
      \hline
        \multicolumn{1}{|>{\centering\arraybackslash}m{6mm}|}{\textbf{\texttt{id}}} &
        \multicolumn{1}{>{\centering\arraybackslash}m{18mm}|}{\textbf{\texttt{firstname}}} &
        \multicolumn{1}{>{\centering\arraybackslash}m{18mm}|}{\textbf{\texttt{lastname}}} &
        \multicolumn{1}{>{\centering\arraybackslash}m{12mm}|}{\textbf{\texttt{birth}}} \\
      \hline
        1 & Howard & Anderson & 1968 \\
        2 & Rebecca & Ferguson & 1959 \\
        3 & Jeremy & Moore & 2000 \\
        4 & Julie & Ellis & 1951 \\
        5 & Kathleen & Cook & 1971 \\
      \hline
    \end{tabular}

    \vspace{1cm}

    \texttt{orders}
    \vspace{0.1cm}

    \begin{tabular}{ |r|r|l|r| }
      \hline
        \multicolumn{1}{|>{\centering\arraybackslash}m{6mm}|}{\textbf{\texttt{id}}} &
        \multicolumn{1}{>{\centering\arraybackslash}m{9mm}|}{\textbf{\texttt{cid}}} &
        \multicolumn{1}{>{\centering\arraybackslash}m{18mm}|}{\textbf{\texttt{product}}} &
        \multicolumn{1}{>{\centering\arraybackslash}m{16mm}|}{\textbf{\texttt{quantity}}} \\
      \hline
        1 & 5 & date & 10 \\
        2 & 2 & orange & 1 \\
        3 & 2 & sloe & 2 \\
        4 & 3 & melon & 7 \\
        5 & 5 & guava & 9 \\
        6 & 3 & orange & 6 \\
        7 & 2 & plantain & 3 \\
        8 & 4 & pear & 7 \\
        9 & 4 & papaya & 9 \\
        10 & 5 & lime & 9 \\
        11 & 3 & sloe & 4 \\
        12 & 1 & strawberry & 1 \\
      \hline
    \end{tabular}

  \end{center}\end{minipage}
  \par
  \caption{Egy minimalisztikus konfiguráció és eredménye}
\end{figure}

Java projektekben JPA-entitásokra helyezett annotációkkal is beállítható a teljes konfiguráció,
így beágyazott adatbázissal, külső eszközök nélkül is futtathatunk teszteket.
Ehhez elegendő a megfelelő JDBC connection sztringet megadni,
a többit (keretrendszertől függően\footnote{
    A \textit{Spring}, \textit{Micronaut} és \textit{Quarkus} beépítetten támogatott,
    de elméletileg bármilyen rendszerhez igazítható a megoldás.
}) automatikusan elvégzi az inicializáló service.

Az egyéni működéssel való kiegészítés több szinten támogatott.
A beépített értékkiosztási módok esetén is lehetőség van egyéni adatfájl betöltésére,
a permutációk és rányújtások egyéni implementációjának betöltésére.
De az oszlop értékkiosztásához teljesen egyedi implementáció is használható,
ehhez meg kell adnunk egy olyan osztály nevét, mely implementálja a \texttt{SourceFactory} interfészt
(illetve biztosítani kell, hogy az egyedi osztályok láthatóak legyenek a classpath alatt).

A YAML formátum a humán kezelhetőségen kívül azzal az előnnyel is jár, hogy könnyen előfeldolgozható.
Például egy megfelelő sablonkezelő használatával ugyanazt a vázat használva
különféle tesztesetekhez finomhangolhatjuk az adatbázist.\footnote{
    Ilyen megközelítést használ például a \textit{helm} nevű deployment eszköz.
}

A konfiguráció gépileg is generálható.
A prototípus tartalmaz néhány ilyen segédeszközt,
ezek közül a legfontosabb a konfigurációt a meglévő adatbázis letapogatásával,
különféle heurisztikák bevetésével legeneráló, Python nyelven írt szkriptfájl.
Az eredmény (és persze maga a szkriptfájl is) könnyen testreszabható.

Ahogy már említettem, a konfiguráció szofisztikáltabb generálásának egy lehetséges módja,
ha a konfigurációt optimalizáló mesterséges intelligenciára épülő módszert vetünk be.
Ennek kutatása egy izgalmas jövőbeli projektnek ígérkezik.
Érdekes probléma például, hogyan érdemes számítani a bemeneti adatbázis
és az előállított konfiguráció közötti megfelelőségi mutatót.%
\cite{Erritali2016DocumentSimilarity}

A konfigurációs fájl az adatbázis nagyjábóli definíciójának is felfogható.
Egy másik ígéretesnek tűnő továbbfejlesztési lehetőség,
hogy kiegészítsük a struktúrát olyan további opcionális elemekkel,
melyek egyrészt teljesen leírják az éles adatbázis szerkezetét,
másrészt pedig lehetővé teszik az adatmigrációknak
kizárólag a konfigurációs fájlok különbségén alapuló automatizálását.
Természetesen ez egyáltalán nem triviális,
hiszen néha a meglévő adatok átstruktúrálására van szükség,
illetve lehetnek a két konfigurációs állapot közötti történetben elbújó lényeges módosítások.
Úgy vélem, mindkét probléma kezelhető,
közeljövőbeli tervem egy ilyen migrációs keretrendszer kialakítása.









\chapter{A virtuális adattár építőkövei}

\section{(???) STORAGE API IMPL: BEVEZETŐ}

Bármilyen implementáció legyen is mögötte, a storage API egy relációs adatbázist ír le.
Tehát ahhoz, hogy a virtuális adatok funkcionálisan egy valódi (csak-olvasható) relációs adathalmaz képét adják,
nem kell más, mint hogy megfelelő viselkedéssel elérhetők legyenek a storage API-n keresztül.
A megfelelőség ez esetben két dolgot jelent:

\begin{enumerate}
  \item \textbf{Konzisztencia:}
        felépíthető egy olyan tényleges immutábilis relációs adatbázis ($M$),
        hogy minden lehetséges relációs lekérdezés esetében, amely a közös sémára ($S$) értelmes,
        a virtuális és a tényleges adatbázis esetében visszaadott eredménytábla megegyezik.
  \item \textbf{Megkötések kielégítése:}
        az $S$ séma teljesíti a virtuális adatbázis felhasználói konfigurációját ($C$),
        valamint az $M$-ben szereplő adatok tulajdonságai illeszkednek a $C$-ben leírt megkötésekre.
\end{enumerate}

A virtuális adatokhoz a legalsó szinten a storage API megfelelő hívásaival férünk hozzá.
Ezek a speciális hívások szűk keresztmetszetet képeznek,
hiszen ezek szimulálják például a közvetlen adatelérést is.
Ha ezen hívások performanciája nagyságrendileg (de legalább aszimptotikusan) összemérhető a tényleges adatbázisokéval,
akkor az erre épülő lekérdezésfuttató és egyéb rétegek már
a tényleges adatbázisoknál megszokott módon és nagyságrendi teljesítménnyel tudnak működni.

A lekérdező műveletek esetében két különösen fontos hozzáférési módot kell kiemelni:

\begin{enumerate}
  \item rekordok véletlen elérése (random access)
  \item adott érték előfordulásainak keresése egy oszlopban (reverse index)
\end{enumerate}

Ha e két hozzáférési mód hatékony, akkor már a lekérdezések jelentős részénél
elérhető a tényleges adatbázisokéval összemérhető performancia.

A storage API-ban úgy definiáltuk a \texttt{TableIndex} interfészt,
hogy a keresésen kívül még néhány további funkciót is támogatnia kelljen,
például a rendezést és a $NULL$ értékek kezelését.
Külön kitérek majd az ezzel kapcsolatos problémákra, ahol szükséges.

\section{Segédtípusok}

\subsection{Hatékony számítások (??? BEVEZETŐ)}

Mivel a szoftver létező prototípusa, a \textit{HoloDB}
(és a hozzá kapcsolódó segédeszközök, könyvtárak nagy része) Java programnyelven íródott,
a továbbiakban felteszem, hogy Java környezetben vagyunk.

A hagyományos, tényleges adatbázisokban az egyik szűk keresztmetszetet
az adatok fizikai elérése jelenti.
Esetünkben ezt a virtuális értékkiosztások fenti függvényei helyettesítik.
A blokkbetöltés, keresési index-struktúrák, materializált nézetek frissítése
és hasonló problémák helyett a virtuális adattárban
a visszaadandó adatok hatékony kiszámítását kell megoldani,
ami általában az oszlopok értékeinek on-the-fly produkálását és keresését jelenti.

Az on-the-fly értékszámítás során gyakran fordulnak elő átmeneti óriás számok,
ám nehezen jósolható előre, hogy pontosan mikor.
Például eleve egy értékkészlet is lehet óriási
(gondoljunk például az \texttt{{\textbackslash}w{30}}
reguláris kifejezés alapján kitöltött oszlop lehetséges értékeinek számára),
miközben szükségünk van bármely pozíció közvetlen hivatkozására.
Az adatbázis méretére vonatkozóan nem akartam megkötéseket tenni.
Nem lett volna célszerű explicit ellenőrzések tömegét beépíteni
a problémás számítások, potenciális túlcsordulások földerítésére.
Továbbá az egész számokat egységesen szerettem volna kezelni, függetlenül azok méretétől.
Így a beépített \texttt{BigInteger} típust egy idő után elvetettem,
mivel az kisebb számokra a \texttt{long} típushoz képest rendkívül rossz performanciájú.

\todo[inline]{ide a hashert, és bármit, ami még jöhet}

\todo[inline]{skálázhatóságról (FastHasher, LinearMonotonic, noop permutation)}


\subsection{Segédtípusok áttekintése}

\todo[inline]{"relációs ontológia" stb.: ezeket a kifejezéseket mellőzni}

Mielőtt rátérnék a relációs ontológiát leíró storage API tárgyalására,
röviden áttekintem azokat a segédinterfészeket,
amelyek a későbbiekben nélkülözhetetlenek lesznek majd.
Természetesen ezek az esszenciális típusok apró szeletét adják csak a teljes architektúrának.

A következő osztálydiagram összefoglalja a kiemelt és lentebb tárgyalt interfészeket:

\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{diagram/basic-ifaces-uml.png}
\caption[Alapvető segédinterfészek]{
    Néhány segédinterfész, melyek alapvető szerepet töltenek be az architektúrában
}
\end{figure}

\subsection{A \texttt{LargeInteger} típus}

Az adatok on-the-fly számításakor a szűk keresztmetszet
a tárolás és elérés problémáiról a kalkulációkra helyeződik át.
Világos tehát, hogy az egész koncepció sikere azon áll vagy bukik,
hogy milyen gyorsan tudjuk számítani az eredményt.

A legtöbb oszloptípusnál rendezett értékkészletbeli pozíciókat mappelünk össze sorindexekkel.
Az értékkészlet óriási is lehet,
illetve számos további részeredmény esetében merül föl az óriás számok problémája.
Ezért szükségesnek mutatkozott egy olyan egész típus megalkotása,
mely kis és óriás számokra egyaránt működik,
és kis számok esetében nagyon hatékony (ellentétben a beépített \texttt{BigInteger} típussal).

Az ecélból készített (egyébként teljesen általános) \texttt{LargeInteger} típus
valójában egy absztrakt típus két privát implementációval:
az \texttt{ImplSmall} a \texttt{Long.MIN\_VALUE}-tól a \texttt{Long.MAX\_VALUE}-ig
terjedő intervallumra (kis számok),
az \texttt{ImplBig} pedig az ezen kívül eső értékekre (nagy számok) optimalizálva.

Az \texttt{ImplSmall} egy primitív \texttt{long} értéket tárol.
Az egyes műveletei esetében egy olcsó ellenőrzéssel mindig megállapítja,
hogy biztos-e, hogy nem lesz túlcsordulás.
Ha ez biztos, akkor a műveletet primitív műveletként végzi el.
Ha nem biztos
(vagy azért, mert az ellenőrzés nem zárta ki a túlcsordulást,
vagy mert nagy számot kapott operandusként),
akkor a \texttt{BigInteger} segítségével hajtja végre a műveletet;
ha az eredmény mégis kis szám, akkor \texttt{ImplSmall} példányként adja vissza.

Az \texttt{ImplBig} mindig a \texttt{BigInteger}-en keresztül végzi a műveleteket.
Hasonlóan a másik implementációhoz, szintén ellenőrzi, hogy az eredmény kis szám-e.
Általában, egy \texttt{LargeInteger} szám akkor és csak akkor csomagolódik \texttt{ImplBig}-ként,
ha kívül esik a \texttt{long} értékkészletén.

A \texttt{LargeInteger} a \texttt{BigInteger} összes metódusát replikálja,
illetve számos további kényelmi metódust ad hozzá.
A zérusközeli értékek gyorsítótárazva vannak a hatékonyság érdekében.

A Scala standard könyvtárának \texttt{BigInt} típusa szintén a \texttt{long} és \texttt{BigInteger}
aritmetika váltogatásával oldja meg,
hogy az óriás számok támogatása mellett kis számokon viszonylag gyorsan fusson.\cite{Rosset2019ScalaBigInt}
A két megoldás performanciában hasonló,
a számunkra releváns esetek egy részében (kisebb számok, sokféle művelet)
a \texttt{LargeInteger} valamivel gyorsabb.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                symbolic x coords={primitive long,,LargeInteger,,BigInt (Scala),,BigInteger},
                xticklabel style={rotate=45,anchor=north east},
                xtick={primitive long,LargeInteger,BigInt (Scala),BigInteger},
                ylabel={Futásidő (ns)},
                ymajorgrids,
                ymin=0,
                ymax=150,
                bar width=25pt,
                enlarge x limits=0.2,
                nodes near coords,
                nodes near coords align={vertical},
            ]
            \addplot[ybar,fill=cyan] coordinates { (primitive long,27) };
            \addplot[ybar,fill=orange] coordinates { (LargeInteger,32) };
            \addplot[ybar,fill=cyan] coordinates { (BigInt (Scala),42) };
            \addplot[ybar,fill=cyan] coordinates { (BigInteger,126) };
        \end{axis}
    \end{tikzpicture}
    \caption[A \texttt{LargeInteger} teljesítményének összevetése]{
        A \texttt{LargeInteger} teljesítményének összehasonlítása
        egy kis számokon végzett összetett számítás átlagos futási ideje alapján
    }
\end{figure}



\subsection{HASH-FÜGGVÉNYEK}

\todo[inline]{Utalni a Feistel-hálózatban (és egyebütt) történt használatra}



\subsection{Hierarchikus véletlengenerátorok}

Az adatbázist egy hierarchikus képződményként is fölfoghatjuk,
ahol a sémákon belül táblák, azokon belül oszlopok,
azokon belül pedig sorindexekkel megjelölt mezők foglalnak helyet
(vagy egy holisztikus értékkészlet).
Végül a mezők értéke is lehet strukturált adat, melyben a rétegződés folytatódik.
Azt szeretnénk, hogy az értékek a hierarchiában bárhol látszólag véletlenszerűen álljanak elő,
ugyanakkor determinisztikusan, egy globális kiinduló seed alapján.
Ezért szükségünk van egy olyan véletlengenerátor típusra,
mely teljesíti a következő követelményeket:

\begin{itemize}
    \item tetszőleges számú bitet képes generálni, mindig ugyanazt a (végtelen) bitsorozatot adva vissza
   \item egy kulcs bájtsorozat megadásával visszaad egy alpéldányt (eggyel alacsonyabb szint)
   \item ugyanazzal a kulcs bájtsorozattal egyenlő példányt ad vissza
\end{itemize}

Valamint lehetőleg az alábbi gyenge követelményeket:

\begin{itemize}
   \item véletlenszerű kimenetet ad
   \item eltérő példányoknál jellemzően független, különböző kimenetet ad
\end{itemize}

Az ezeknek való megfelelés a \texttt{TreeRandom} interfészen keresztül történik.

Igen gyakran számokat akarunk előállítani,
ezért kényelmi okokból az interfészhez adtuk a \texttt{getNumber()} default metódust,
mely a paraméterül kapott számnál kisebb nem negatív egészet ad vissza.
Ehhez annyi generált bitet használ,
amennyi a szám elfogulatlan kiválasztásához szükséges (rejection sampling).\footnote{
  Az általános eset miatt valójában egy beépített limittel, melynek elérésekor zérót ad vissza.
  Ha a bitgenerátor-stratégia például csupa 1-est generál
  (ami nem teljesíti ugyan a gyenge követelményeket, de fapados implementációként teljesen valid),
  könnyen végtelen ciklus alakulhatna ki.
}

A fő implementáció hashelésen alapul, és magának a hashelésnek a minőségével skálázható.
Minden ilyen példány belsőleg tartalmaz egy kulcs-bájtsorozatot.
A root példánynál ez a root seed alapján áll elő.
Egy alpéldány előállításakor az eredeti objektum kulcs-bájtsorozatához egy szeparátorbájt,
majd a megadott kulcs alapján előállított bájtsorozat kerül hozzáfűzésre,
így áll elő az alpéldány kulcs-bájtsorozata.

Amikor az adott példányból bitek kerülnek lekérdezésre,
az első néhány bájt a kulcs-bájtsorozat hashelésével áll elő.
A további bitek ennek derivátumai, aminek legegyszerűbb algoritmusa,
ha a hash alapján előállított seeddel inicializált \texttt{Random} példánnyal
rendre visszaadott bájtokat használjuk.

\subsection{(---) A \texttt{Monotonic} és implementációi}

Gyakran egy adott értékkészlet elemeiből akarunk képezni egy $n$ hosszúságú rendezett listát.
Az eloszlás (beleértve, hogy valamely érték egyáltalán előfordul-e)
teljesen implementációfüggő lehet.
Mivel az értékkészlet maga is egy $k$ hosszúságú lista,
a további leképezési kompozíciók szempontjából rugalmasabb,
ha az értékindexeket, azaz a $[0..k)$ egész-intervallum számait tesszük a listába.

Követelmény, hogy le tudjuk kérdezni az alábbiakat:

\begin{itemize}
    \item egy adott pozícióra mely értékindex került
    \item adott értékindex mely sávba került
\end{itemize}

A rendezettség miatt a második kérdésre mindig egy összefüggő sáv a válasz. Ha az értékindex nem szerepel, akkor ez egy zéróhosszú sáv azon a helyen, ahová a rendezés szerint beszúrható lenne.

Értelemszerűen értéksávokra is tudunk keresni, hiszen az értéksáv szélső értékeinek értékindexeire keresve megkapjuk a képzett sáv széleinek sorindexeit is.

Ezeket az elvárásokat formalizálja a \texttt{Monotonic} interfész.

A legegyszerűbb implementáció a diszkrét lineáris vetítés.
A $k$ hosszúságú értékkészletet ekkor úgy vetítjük az $n$ listahosszra,
hogy az $i$-edik helyre a $\lfloor \frac{k \cdot i}{n} \rfloor$ pozíciójú érték kerül.

A költségesebb, jobb minőségű implementációk pszeudovéletlen mintavételezésen alapulnak.
Ehhez rekurzívan osztjuk föl az értékkészletet, miközben a felosztás fája determinisztikus, mindig ugyanaz.
A fa minden elágazásához tartozik egy mintavételezés, mely eldönti,
hogy az értékkészlet szeletei a céllista mely szeleteire vetüljenek.
A mintavételezéshez jelenleg az Apache Commons Maths könyvtár
\texttt{BinomialDistribution} osztályát használom,
melyet a \texttt{TreeRandom} példányból generált seeddel hívva
determinisztikus eredményt tudok előállítani.

A mintavételezéses eljárás többféleképpen is továbbszofisztikálható.
Egy ilyen lehetőség a szürjektív eredmény biztosítása, amikor szeretnénk, ha minden érték előfordulna
(a zajosan monoton értékkiosztásoknál ennek lesz egy további alkalmazása).
Illetve az is lehetséges, hogy konkrét táblázatunk van,
mely minden értékkészletbeli értékre tartalmazza annak várható előfordulási gyakoriságát.
Ezeket az előre definiált gyakoriságokat használhatjuk a mintavételezésben,
de akár közvetlenül is alkalmazhatjuk egy diszkrét lineáris vetítéssel.


\subsection{(+++) Monoton méretigazítás}

A \texttt{Monotonic} interfész példánya egy függvényt reprezentál,
amely az első $n$ nemnegatív egész számot értékkészletként nem monoton növekvő módon hozzárendeli az első $N$ nemnegatív egész számhoz.
Ez arra használható, hogy egy $n$ elemű, rendezetten tárolt értékkészletet egy $N$ méretű adatbázistábla oszlopához igazítsunk méretben,
Tehát a példány $M(i) = m$ függvényt valósítja meg, ahol $0 \leq i < N$ illetve $0 \leq m < n$, továbbá $i_1 < i_2 \implies M(i_1) < M(i_2)$.
Ugyanakkor indexelt elérést is biztosít, azaz lekérdezhető, hogy egy adott érték mely indexsávhoz rendelődik,
azaz megvalósítja az $M^{-1}(m) = [i, j)$ függvényt is, ahol $0 \leq m < n$ illetve $0 \leq i \leq j \leq N$, és teljesülnek az alábbiak:

\begin{itemize}
\item $x < i \implies M(x) < M(i)$
\item $i \leq x < j \implies M(i) \leq M(x) < M(j)$.
\item $i < x \implies M(i) < M(x)$
\end{itemize}

Az indexelt elérés az \texttt{indicesOf(value)} metóduson keresztül biztosított, amely egy \texttt{Range} objektumot ad vissza.
A metódus egy variánsa maga is egy \texttt{Range} példányt vár, és értelemszerűen általánosítja az előbbit.

\subsubsection{Triviális implementációk}

\todo[inline]{Monotonic: triviális implementációk}

\todo[inline]{Monotonic: egyszerű diszkrét lineáris interpoláció: általában ez is elég, a permutáció majd úgyis elkeveri}

\subsubsection{Szimulált mintavétel}

\todo[inline]{Monotonic: szimulált mintavétel (jó ez a cím?)}

\todo[inline]{Monotonic: szimulált mintavétel: a lényeg, hogy bizonyos mértékű véletlenszerűséget ad}

\subsubsection{Gyakoriságtáblázat használata}

A fenti implementációk mindegyike általánosítható úgy, hogy figyelembe vegyen egy gyakoriságtáblázatot,
mely az értékkészlet elemeihez azok relatív gyakoriságát rendeli.

\todo[inline]{Monotonic: gyakoriságtáblázat}


\subsection{(---) A \texttt{Permutation} és implementációi}

Gyorsan számítható invertálható permutációk például modulo szorzásból kiindulva valósíthatók meg.

\todo[inline]{Permutációk: egyszerűen a Feiste-háló rugalmasságával, teljesen kontrollálható paramétereivel, sokrétű skálázhatóságával érvelni}

Költségesebb, de erősen véletlenszerű összevisszaságot biztosító invertálható permutációk
különféle kriptográfiai módszerekkel állíthatók elő.
Többféle algoritmuscsalád kipróbálása után
végül egy megfelelően kialakított Feistel-hálózat bizonyult a legkezelhetőbbnek.
A Feistel-háló fő skálázási paramétere (akárcsak a hash-alapú \texttt{TreeRandom}-implementációnál)
a hashelő algoritmus minősége,
de a háló mérete illetve a permutáció $n$ hossza is lényeges.

A következő ábrán vizuálisan összehasonlítottam néhány permutáció-típus értékeloszlását:

\begin{figure}[H]
  \centering
  \hspace*{\fill}
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{image/permutation-fpe1.png}
    \par Rank-then-encipher FPE\protect\cite{Bellare2009FormatPreservingE}
  \end{minipage}
  \hspace*{\fill}
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{image/permutation-mp1.png}
    \par Modulo permutáció
  \end{minipage}
  \hspace*{\fill}

  \vspace*{0.7cm}

  \hspace*{\fill}
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{image/permutation-feif1.png}
    \par Feistel-háló (1-round, fast hash)
  \end{minipage}
  \hspace*{\fill}
  \begin{minipage}[b]{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{image/permutation-feis2.png}
    \par Feistel-háló (2-round, SHA256)
  \end{minipage}
  \hspace*{\fill}

  \caption[Permutációk kimeneteinek összehasonlítása]{
    A \texttt{Permutation} különféle implementációinak kimenetei \\
    (a közeli értékek színe hasonló; minél nagyobb a káosz, annál jobb)
  }
\end{figure}

A kriptográfiában alkalmazott legtöbb megfordítható permutáció mérete eleve kötött.
A Feistel-háló ebből a szempontból elég rugalmas,
de a permutáció hossza még ekkor is csak 2-hatvány lehet.
A tetszőleges hossz biztosítására átméretező dekorátorok használhatók.
A hosszcsökkentő dekorátor továbbhív a dekorált permutációra,
és ha a kapott érték a célhosszon belül van, akkor visszaadja, ha nem,
akkor a mindenkori értékkel addig hívja újra,
amíg megfelelő értéket nem kap.
A hossznövelő dekorátor ennek felhasználásával implementálható,
de a jelentősége kisebb, jellemzően hosszcsökkentés történik.
Egyes speciális permutációknál az előbbi módszer nagyon sok újrahívással járhat
(például egy olyannál, ahol az értékek eggyel előre tolása történik).
Az átméretező metódus bekerült magába az interfészbe,
hogy az ilyen implementációk maguk kezeljék a helyzetet.
A jó szétszórást biztosító permutációknál valószínűtlen a sok újrahívás.

\subsection{(+++) Permutációk}

A \texttt{Permutation} interfész megvalósításához olyan eljárást keresünk, amellyel változó (és akár óriás) tartományon értelmezett,
seed szerint variálható permutációfüggvény és annak inverze előáll, ahol mindkét függvény hatékony.

Az inverz függvényhívás az \texttt{indexOf(value)} metódus biztosítja.

\subsubsection{Triviális implementációk}

Megjegyzem, hogy triviális implementációként használhatjuk egyszerűen az azonosságfüggvényt is.
Ekkor nem keverednek az adatok, cserébe viszont performanciában verhetetlen.

Gyorsan számolható, de ránézésre már jó keveredést adó permutációt nyerünk, ha kihasználjuk, hogy az $ai \equiv p \pmod{n}$ lineáris kongruencia
$i$-re és $p$-re nézve egy permutációt ad, ha $a$ és $n$ relatív prímek:

$$
\sigma_L^{n(a,b)}(i) = (ai + b) \bmod n
$$

Az $a$ és $b$ számot előre, a seed alapján határozzuk meg.
A $b$ eltolás értéke tetszőleges lehet.
Az $a$ szám relatív prím kell legyen $n$-re nézve.
A megkereséséhez először választunk egy jelöltet,
majd megvizsgáljuk (például az euklideszi algoritmussal), hogy relatív prím-e $n$-re.
Ha igen, kész vagyunk.
Ha nem, a számot tároló \texttt{LargeInteger} példány
\texttt{nextProbablePrime()} metódusát hívva előállítjuk a következő jelöltet.
Ez a háttérben a \texttt{BigInteger} osztály ugyanilyen nevű metódusát hívja, amely a Java beépített heurisztikus megoldása a következő valószínű prím megkeresésére.

\subsubsection{Feistel-hálózatok alkalmazása}

Az invertálható, változtatható méretű, seed alapján újrakeverhető permutációk könnyen analógiába állíthatók kriptográfiai titkosítófüggvényekkel.
Ezek általában változtatható blokkméretűek, kulcs alapján újrakeverhetőek és természetesen dekódolhatóak.
Felmerül a gondolat, hogy közvetlenül egy ilyen kriptográfiai módszert alkalmazzunk a permutáció megvalósításához.
Mindenképpen olyan megoldásra van szükség, amelynél a blokkméret tetszőlegesen megadható, és kellően flexibilis, skálázható.

A Feistel-hálózatok pontosan ilyenek, és emiatt gyakran használják is őket összetett titkosító eljárások keretrendszereként.
A Feistel-hálózat egy többkörös eljárás, ahol minden körben az adat egyik felének bitjeit módosítjuk.
Ez alapesetben megköveteli a páros blokkméretet, de mindjárt kitérek arra, hogyan lehet ezen enyhíteni.

Az egyes körökben felváltva a bitsorozat bal illetve jobb oldali felét változtatjuk; nevezzük ezt célhelynek, a másik fél tartalmát pedig forrásbiteknek.
Egy-egy kör abból áll, hogy a forrásbitekre meghívunk egy az adott körhöz dedikált hossztartó hash-függvényt,
és az így kapott eredményt XOR-ral ráfésüljük a célhelyre.
Vegyük észre, hogy az XOR művelet tulajdonságai miatt ez reverzibilis művelet (bármi legyen is a hash-függvény),
ha újra alkalmazzuk, az eredeti bitsorozatot kapjuk vissza,
és a forrásbitek még rendelkezésre állnak, mivel azokat ezen a ponton még nem írtuk felül.
Tehát a Feistel-hálózattal kapott kód visszafejtése úgy történik, hogy a lépéseket visszafelé újra végrehajtjuk.
Általában nem szükséges, hogy az egyes körökhöz különböző hash-függvényt használjunk,
számunkra bőven elég, ha a páros és páratlan körökhöz eltérő függvényt alkalmazunk (szigorúan még ez sem lenne szükséges).

A Feistel-hálózatokkal való kísérletezés során észrevettem,
hogy a páros blokkméret követelménye enyhíthető a következőképpen.
Páratlan blokkméret esetén vegyünk még egy zéró bitet az adat végéhez,
amivel biztosítjuk, hogy a jobb oldali bitmező is a megfelelő méretű legyen.
Minden olyan kör végén, ahol a jobb oldali biteket írtuk felül, az utolsó bitet ezután nullázzuk.
A legvégén pedig csak az utolsó bit nélküli részt adjuk vissza.
Az utolsó bit ekkor minden művelet előtt és után is fixen zéró,
ami természetesen visszafelé haladva ugyanúgy teljesül, tehát a dekódolás is determinisztikus marad.
Ha egyúttal páros számú kört követelünk meg,
akkor az utolsó kör végére garantáltan ugyanabba a fázisba kerülünk, mint az első kör kezdete előtt;
vagyis a dekódolás folyamata teljesen egyezni fog az elkódolással, ami egyszerűsíti az eljárást.

Lorem ipsum. XXX~\ref{appendix:algorithm_xyz}

\todo[inline]{ALGORITMUS: Feistel: last-zero algoritmus-változat}

Persze, még ha a páros blokkméret követelményét így sikerült is eliminálni,
továbbra is adott a probléma, hogy az $n$ blokkmérethez mindig $2^n$ méretű permutáció tartozik.
Vagyis, ha nem csak 2-hatvány méretű permutációkat akarunk támogatni, méretigazítás szükséges.
Ezt a rejection sampling technika alkalmazásával érjük el.
Az adott $n$ mérethez vesszük azt a legkisebb $N$ 2-hatványt, amely $n$-nél nem kisebb.
Amikor egy adott $i$ értékre a Feistel-hálótól az intervallumon kívül eső $p$-t kapunk,
azaz amelyre $p \geq n$ ($p < N$), akkor újra meghívjuk a permutáló függvényt $p$-re.
Addig ismételjük a hívást, míg végre $n$-nél kisebb szám nem születik.
A páratlan blokkméret megengedettsége a hatékonyság szempontjából is fontos,
mivel jelentősen csökkentheti az intervallumon kívülre esés esélyét,
és így a Feistel-hálózatra történő hívások számát.

\todo[inline]{Feistel leírásához igazítani a hasher-implementációk tárgyalását}

\subsubsection{Permutációk méretigazítása}

\todo[inline]{Rejection sampling}

A rejection sampling nem minden implementációnál hatékony,
de pontosan ezért vezettük be a $resized(newSize)$ metódust magán a $Permutation$ interfészen.
Így minden implementáció maga döntheti el, hogyan kell átméretezni.
Egy extrém eset például, amikor a $P_{+1}(i) = (i + 1) \bmod N$ permutációt $n$-re kell átméreteznünk.
Rejection sampling módszerrel $i = n$ esetén $N - n$ számú extra iterációt kellene végrehajtani,
ami nagy méret esetén rendkívül költséges.
Helyette viszont egyszerűen áttérhetünk modulo $n$-re: $P_{+k}(k)(i) = (i + k) \bmod n$.

\subsubsection{XXXX}

\todo[inline]{egy trükkös saját permutáció, ami elég jó szórást ad (ha sikerül megoldani)}

\todo[inline]{bitkeverés, kétfelől átfedő 2-hatványos permutálás stb.}



\section{1OSZL. IND. ÉRTÉKKIOSZTÁSOK}

\subsection{Általános megfontolások}

\todo[inline]{Általános megfontolások: Itt írni a nem indexelt értékkiosztásokról (máshol már akkor nem kell, pl. régi regex)}

Olyan értékkiosztási módszereket veszek most végig,
amelyek lehetőleg biztosítják az alábbiakat:

\begin{enumerate}
  \item hatékony elérés (random access)
  \item pozíciótól függő érték (\texttt{TreeRandom} seed)
  \item hatékony kereshetőség valamilyen formában
  \item hatékony rendezés
  \item konzisztencia
\end{enumerate}

Az egyes esetekben megvizsgálom majd, hogyan teljesíthetők ezek az elvárások.


\subsection{(???) Keretrendszer az értékkiosztásokhoz}

Virtuális adatok alatt elsősorban az egy-egy oszlop alá besorakozó,
közös típussal rendelkező mezőértékeket értem.
Azaz alapesetben, ahogy korábban fogalmaztam, az adatokat oszlop-orientáltan fogjuk előállítani.
Minden oszlophoz tartozik majd egy virtuális adatlista,
melynek hossza egyenlő az adott oszlopot tartalmazó tábla hosszával
(opcionálisan szerepelhetnek benne $NULL$ értékek),
a többi érték típusának pedig kompatibilisnak kell lennie az oszlophoz megadott típussal.

Bár ez a megközelítés oszloponként független adatlistákra van szabva,
valójában más jellegű értékkiosztási módszer is lehet mögötte,
amennyiben az egy-egy oszlophoz tartozó listanézetek biztosítottak.
Majd a több oszlopot érintő megkötések esetében ez lesz a helyzet.
De lássuk először az egyoszlopos értékkiosztások lehetséges módszereit.

\subsection{(???) Általános kétfázisú értékkiosztás}

\subsubsection{XXXXXXXXX}

Talán a leggyakoribb eset,
hogy előre ismert véges értékkészlet elemeit szeretnénk viszontlátni az oszlopban,
mégpedig összevissza.

Fentebb már utaltam rá, hogy ez a legtöbb esetben megoldható két független lépéssel.
Először egy \texttt{Monotonic} példány segítségével a kívánt célhosszra
értelmezett függvényt állítunk elő, mely az értékkészlet pozícióira mutat
(ezt a továbbiakban \textit{rányújtás}nak nevezem majd).
Majd az így előállt (esetlegesen $NULL$ értékekkel kiegészített) listát
egy \texttt{Permutation} példány segítségével elkeverjük.

\begin{figure}[H]
\centering
\includesvg[width=0.55\textwidth]{diagram/distribution}
\caption[A kétfázisú értékkiosztás alapelve]{
    A kétfázisú értékkiosztás alapelve:
    egymás után végrehajtott visszafejthető disztribúció és permutáció
}
\end{figure}

Láttuk, hogy mindkét függvénytípus interfésze úgy van definiálva, hogy relációként megfordítható.
A \texttt{Monotonic} \texttt{indicesOf()} metódusaival egy értékindexnek illetve -sávnak
a céllistabeli sávjára tudunk rákérdezni.
A \texttt{Permutation} pedig az \textt{indexOf()} metódust biztosítja hasonló célra
(illetve inverze maga is permutáció).
Ezek használatával egy reverse index könnyen implementálható.

A még keveretlen rányújtott adathalmazbeli találati sáv határai
egyúttal azonnal megadják a találatok számosságát,
ami egyszerűbb \texttt{COUNT} lekérdezéseknél igen előnyös.

A következő két ábra szemlélteti az adatlekérés illetve értékkeresés folyamatát:

\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.48\textwidth}
    \includesvg[width=\textwidth]{diagram/getvalue}
    \caption{Adatlekérés a kétfázisú értékkiosztásból}
  \end{minipage}
  \hspace*{\fill}
  \begin{minipage}[t]{0.48\textwidth}
    \includesvg[width=\textwidth]{diagram/findvalue}
    \caption[Keresés a kétfázisú értékkiosztásban]{
      Keresés a kétfázisú értékkiosztásban:
      a rendezett értékkészlet és a leképezések megfordíthatósága
      biztosítja a hatékony keresést a virtuális listában
    }
  \end{minipage}
\end{figure}

\todo[inline]{Függetlenül paraméterezhető és skálázható (a gyakoriságtáblázatot is ehhez)}

\subsubsection{Kétfázisú értékkiosztás gyakoriságtáblázattal}

Egyes esetekben előre ismert az értékek eloszlása,
amit szeretnénk nagyjából vagy pontosan viszontlátni a céloszlopban.
Ekkor egyszerűen a \texttt{Monotonic} fentebb tárgyalt konkrét eloszlásokat használó
implementációnak egyikét kell használnunk.

A felhasználó számára biztosítani kell a gyakoriságok könnyű konfigurálhatóságát.
Ennek legegyszerűbb módja, ha szám-érték párokat kérünk be.
Az értékekhez tartozó számok egymáshoz képesti relatív gyakoriságok.

\subsubsection{Zajosan monoton értékkiosztások}

A rányújtásnak az előzőleg monoton függvény előállításához használt elve
másféle értékkiosztáshoz is felhasználható,
nevezetesen olyan (szigorúan vagy nem szigorúan) monoton adatsor előállításához,
melynek egyes értékei összességében egy adott sűrűség szerint növekednek
(vagy csökkennek; az egyszerűség kedvéért most csak a növekedő esetről lesz szó),
de lokálisan nagy az egyenetlenség.
Ezt is két lépésben fogjuk megvalósítani.\footnote{
  A két lépés általánosítható egy általános sűrűségfüggvény és egy zajfüggvény összegévé,
  ahol a sűrűségfüggvény meredeksége és a zajfüggvény kilengése közötti megfelelő viszonyt kell biztosítani,
  hogy az értékek ne ugorják át egymást.
  Itt most nem foglalkozom ezzel az általánosabb kerettel.
}

Az első lépés alapelve tehát hasonló az előbbi megoldás első fázisához.
Ám itt nem lehetséges értékeket vetítünk ki a tábla hosszára,
hanem a táblahossznyi alaphalmazt vetítjük majd ki egy diszkrét lehetséges értékkészletre.
Minden sorindexhez hozzárendelődik egy (a szigorú monotonitás elvárása esetén nemüres)
dedikált sáv az értékkészletből.

A második lépés választ egy értéket a sávból.
Nem szigorúan monoton értéksor esetén megengedjük az üres sávot is,
és ilyen esetekben mindig a rákövetkező elemet választjuk.\footnote{
  Ebben az esetben explicite ki kell zárni, hogy az utolsó sáv üres legyen.
  Ez legtermészetesebben egy logikai paraméter beiktatásával érhető el,
  amelyet a rekurzió során mindig csak a felsőbb sávra küldünk tovább \texttt{true} értékkel
  (a többire \texttt{false} értékkel),
  alapértelmezett értéke \texttt{true}.
  Ha tehát \texttt{true} értéket kaptunk, biztosítani kell, hogy az aktuális felsőbb sáv ne legyen üres.
}

Az érték elérése ekkor úgy történik, hogy először lekérjük az értéksávot a rányújtó függvénytől,
majd meghívjuk az értékválasztó függvényt,
melynek paraméterei az értéksáv, a sorindex és a \texttt{TreeRandom}-ból vett seed lesznek.
Egy kézenfekvő megvalósítás,
hogy a seed alapján inicializált véletlengenerátorral
generáltatunk egy véletlen értéket, ami a sávba esik.

Értéksávra való kereséskor vesszük a minimális és a maximális keresett értéket,
és az inverz rányújtást használva megkeressük a megfelelő sorindexeket, amelyek sávjához az érté tartozik.
Ezen sorindexek feszítik majd ki a találati sorindexsávot.
Hogy az alsó érték beletartozik-e, annak eldöntéséhez le kell generálni
az alsó sorindexhez a konkrét értéket, és ellenőrizni, hogy nagyobbegyenlő-e,
mint a keresett alsó érték.
A felső érték esetében hasonlóan kell eljárni.

A rendezés triviális, mivel az értékek eleve rendezettek.

Ezzel az eljárással nem csak zajossá tudtuk tenni az eloszlást,
de a monotonitás garantálása mellett megengedtük,
hogy az értékek esetlegesen csomósodhassanak,
illetve elméletben tetszőlegesen eltávolodhassanak attól a helytől,
amit egy egyszerű, szigorúan egyenletes kiosztás esetén vettek volna föl.

Ez az értékkiosztás különösen alkalmas időbélyegek szimulálására,
amikor az események általános sűrűsége adott,
de véletlenszerű, zajos kimenetet szeretnénk látni.



\subsection{(+++) KÉTLÉPÉSES ÉRTÉKKIOSZTÁS}

\todo[inline]{KÉTLÉPÉSES ÉRTÉKKIOSZTÁS}

\subsection{$NULL$ értékek kezelése}

\todo[inline]{NULL értékek kezelése: ez mehet az általános megfontolásokba}

Általánosan, bármely értékkiosztási módszerhez könnyen hozzáilleszthető a $NULL$ értékek támogatása.
Mindössze az szükséges, hogy az értékkiosztást a tábla méreténél kisebb intervallumra végezzük,
a maradék helyeket pedig $NULL$-nak tekintjük.
Ezen felül opcionálisan egy permutáció beiktatásával az értékeket elkeverhetjük,
ami által a $NULL$ értékek is szétszórtan szerepelnek majd.

Nem szükséges ez a plusz kompozíció,
ha a fentiek magába az eredeti értékkiosztásba is könnyen beépíthetők.
Például a kétfázisú értékkiosztásokba egyszerűen felvehető a $NULL$ mint érték.

Akár beépítetten, akár plusz kompozícióval van megvalósítva a $NULL$ értékek kezelése,
természetesen figyelni kell a speciális kezelésre a keresés-rendezés során.

A fentiek fényében az egyes értékkiosztások tárgyalásánál a $NULL$ értékek kezelésével nem foglalkozom.

\subsection{Egyszerű értékkiosztások}

\todo[inline]{Egyszerű értékkiosztások: ez mehet az általános megfontolásokba}

Tényleges adatbázisokban némely esetben meglehetősen következetes módon
szerepelnek az értékek az oszlop értéklistájában.
Természetesen az ilyen esetek szimulálása a legegyszerűbb.

Triviális eset, ha egy oszlop mezői egy közös konstans értéket tartalmaznak.

Továbbá, ha olyan adathalmazt szimulálunk, melyre megengedhető a feltevés,
hogy nem történt még releváns módosítás
(ami persze az írási réteggel utána korrigálható),
úgy egyes oszlopok szekvenciális alapon generálhatók.
Az ilyen esetekben az $n$-edik érték lekérése
egy egyszerű lineáris függvénnyel számolható.
Az érték keresése hasonló, lényegileg az inverz függvényt kell alkalmazni.

A szekvenciális oszlopoknál az érték megegyezik a rekord 1-től indított sorszámával.

Időbélyegeket is generálhatunk hasonló módon,
amennyiben megengedhető, hogy a szimulált időadatok között egyenletes időközök legyenek.

\subsection{Unique értékkiosztás}

Elsődleges vagy más egyedi kulcsok szimulálásakor fontos,
hogy ugyanaz az érték ne forduljon elő kétszer az értéklistában.
Ennek természetesen előfeltétele,
hogy a lehetséges értékkészlet nagyobb legyen, mint az oszlop hossza
($NULL$ értékek nélkül).

Egy egyszerűen megvalósítható módszer,
ha egyszerűen valamilyen \texttt{Permutation} példányt alkalmazunk az értékkészlet fölött,
és a permutált lista oszlophosszúságú prefixét vesszük értéklistaként.
Ekkor a random elérés triviális: csak a permutációt kell visszafejteni.
Az index a rendezett értékkészletre alapozható,
a találatokat a permutációval megforgatva kell listázni,
kihagyva a prefixen kívül eső értékeket.

Ha azonban az értékkészlet mérete lényegesen nagyobb, mint az oszlophossz,
jellemzően nagyon sok értéket kell átugranunk, amely a prefixen kívül esik.
Sokkal hatékonyabb, ha a következő részben leírt általános kétfázisú értékkiosztást használjuk,
ami nem csak unique oszlopokhoz alkalmazható.
A unique oszlop esetén csak annyi a megkötés, hogy a használt \texttt{Monotonic}
a kimenetet szigorúan monoton módon állítsa elő.


\subsection{Értékkészletek reguláris kifejezésekből}

\subsubsection{Reguláris kifejezésből képzett szó-fák}

Karakterláncokból álló értékkészletnek egy reguláris kifejezés alapján történő generálása
talán a legáltalánosabb és legrugalmasabb módja az értékek definiálásának.
Emiatt ezzel a résztémával fogok a legrészletesebben foglalkozni.

Véletlenszerű karakterláncok reguláris kifejezés alapján történő generálására
számos könyvtár elérhető a Java környezethez
(például a \textit{xeger} és a \textit{generex}).
Ezek olyan automaták, melyek az állapotgráfot véletlenszerű választásokkal járják be.
Ez a megközelítés önmagában nem alkalmas arra,
hogy az összes illeszkedő karakterláncból álló virtuális értékkészletet
a számunkra megfelelő módon implementálni lehessen vele.
Nem tudjuk ugyanis lekérni a rendezett értékkészlet $n$-edik elemét,
és nem tudjuk kikeresni, hogy adott érték mely pozíción található az értéklistában.

Lássuk, hogyan tudnánk valami módon mégis egy rendezett értékkészletet implementálni.
Első megközelítésben a reguláris kifejezéssel kapcsolatban az alábbi megszorításokat tesszük:

\begin{itemize}
    \item a reguláris kifejezés csak konstans karaktereket, karakterosztályokat, csoportokat és alternációt tartalmaz
    \item a rendezés karakterenkénti, azaz nincs másodlagos, harmadlagos stb. összehasonlítási szempont
\end{itemize}

Ilyen megkötések mellett a reguláris kifejezéshez problémamentesen rendelhető egy
irányított körmentes szógráf (DAWG),
melyben egy érték nélküli kiinduló elemből érjük el a karaktereket reprezentáló csomópontokat,
és kizárólag a szóvég csomópontokon (EOW) terminálhatunk,
melyekből már nem érhető el további csomópont.

\textcolor{red}{TODO TODO TODO TODO TODO} \parencite{AppelScrabble1988}
% TODO: https://www.cs.cmu.edu/afs/cs/academic/class/15451-s06/www/lectures/scrabble.pdf

A gráf lényegileg egy prefix fa, ahol egyes azonos részfákat újrahasznosítunk.
Ezért a továbbiakban az egyszerűség kedvéért néha faként fogok a gráfra hivatkozni.
A gyökértől az adott részfáig vezető útvonalra
néha az adott részfa \textit{prefixeként} fogok hivatkozni.
Az újrahasznosítás miatt a fa tárhelyigénye a reguláris kifejezéshez képest lineáris.
A szóvég csomópontok lesznek a fa levelei.
A prefix fa könnyen felépíthető a reguláris kifejezés szintaxisfájából (AST).
A gráf tömöríthető úgy, hogy konstans karakterek helyett karakterosztályokat engedünk meg.

\todo[inline]{tisztázni, hogy a "szó"-fa teljes sztringekre vonatkozik}

A felépítés két lépcsőben történik.
Először egy nyers fát állítunk elő, ami közvetlenül reprezentálja a reguláris kifejezést.

\todo[inline]{nyers szófa előállításának rövid leírása}

\todo[inline]{ALGORITMUS: nyers szófa előállítása}

Például a \texttt{([a-c]z(tt|uu)r|a[s-z])} reguláris kifejezés nyers szófája így néz ki:

\begin{figure}[H]
\centering
\includesvg[width=0.9\textwidth]{diagram/trie-simple-raw}
\caption{Egyszerű reguláris kifejezés nyers szófája}
\end{figure}

A nyers fa nagyon hasonlít egy szintaxisfához,
a fő különbség, hogy az alternációk vége tovább van csatlakoztatva a külső szekvencia
következő eleméhez tartozó csomóponthoz
(a példában a \texttt{[t]} és az \texttt{[u]} csatlakozik így az \texttt{[r]}-hez).
Ebből világos, hogy a nyers fa mérete és előállításának költsége
a reguláris kifejezés méretével egyenesen arányos.

Látható, hogy a szekvenciákból láncolás lett,
az alternációkból pedig egy több elágazással (gyermekelemmel) rendelkező üres csomópont.
Az is látható, hogy néhány rákövetkező karakter duplikáltan szerepel egyes elágazásokban
(például az '\texttt{a}' betű rögtön az elején).

Ezen problémák kiküszöbölésére második lépésben egy normalizálási eljárást hajtunk végre,
mely után a gráfban adott csomópontból elérhető csomópontok listájára az alábbiak érvényesek:

\begin{itemize}
    \item \textbf{explicit terminálás:} a lista opcionális eleme a szóvég csomópont, ami kötelező, ha egyébként nincs más a listában; kivételt képez maga a szóvég csomópont, melynek soha nincs gyermeke
    \item \textbf{egyértelműség:} a lista elemei ezen kívül diszjunkt karakterosztályok
    \item \textbf{rendezettség:} bármely karakterosztály listaelem minden tartalmazott karaktere későbbi, mint bármely megelőző karakterosztály listaelem bármely tartalmazott karaktere; a szóvég csomópont mindig első elem
\end{itemize}

Ezek a szabályok biztosítják, hogy a gráf bejárása rendezett sztringeket ad,
valamint egy trie-szerű adatstruktúrát kényszerítenek ki, amelyben hatékonyan tudunk keresni.

Az átalakítás rekurzív módon történik.
Ahol egymásnak alternatíváját jelentő két karakterosztály között átfedés vagy átugrás van,
ott mindkét karakterosztályt szét kell bontani úgy,
hogy a kapott karakterosztályokból képzett halmazra a fentiek érvényesek legyenek.
Például ha az \texttt{[a-c]}, \texttt{[cf]} és \texttt{[c-e]} jelennek meg alternatívaként,
akkor ezek szétbomlanak az \texttt{[ab]}, \texttt{[c]}, \texttt{[de]} és \texttt{[f]}
karakterosztályokra, és átveszik a korábbiakból elérhető csomópontokat.
A \texttt{[c]} karakterosztály egy alternációt vesz át a három eredeti csomópontból.
A fát egyúttal célszerű úgy továbbalakítani, hogy megszüntetjük az alternációk miatti üres értékű,
pusztán az elágazás kedvéért felvett csomópontokat,
ez természetesen az egyenlő csomópontok részfáinak összefésülését vonja maga után.
Nevezzük az így nyert gráfot a reguláris kifejezés kompakt szófájának.

\todo[inline]{ALGORITMUS: kompakt szófa előállítása}

A \texttt{([a-c]z(tt|uu)r|a[s-z])} reguláris kifejezés fenti nyers szófájából
az alábbi kompakt szófát nyerjük:

\begin{figure}[H]
\centering
\includesvg[width=0.9\textwidth]{diagram/trie-simple-compact}
\caption{Egyszerű reguláris kifejezés kompakt szófája}
\end{figure}

Minden csomóponthoz hozzárendeltük a hozzá tartozó részfa leveleinek effektív számát,
ami rekurzióval könnyen számítható.
Ez az információ már elegendő ahhoz, hogy kikeressük az $n$-edik elemet,
illetve hogy megkeressük egy adott karakterlánc (megtalált vagy beillesztési) pozícióját.

\todo[inline]{ALGORITMUS: iteráció stb.}

A kompakt szófában megtalálható különböző részfák száma
a nyers szófa különböző részfáinak számához képest exponenciálisan megnőhet.
Induljunk ki ugyanis egy $n$ elemű $C$ karakterhalmazból,
és képezzük rendre a $C_1, C_2, \cdots, C_{n-1}, C_n$ karakterosztályokat úgy,
hogy $C_i$-t $C$-ből az $i$-edik karakter kihagyásával nyerjük.
Képezzük azt az $n$ ágú alternációt,
ahol az $i$-edik ág $C_i$\texttt{\{}$n$\texttt{\}} alakú.
Világos, hogy a reguláris kifejezés által megengedett összes $n-1$ hosszú prefixekben
szereplő karakterek lehetséges halmazai kiadják
$C$ összes legfeljebb $n-1$ elemű halmazait,
ami exponenciálisan növekszik $n$ növelésével.
Minden ilyen halmaz az alternáló ágak egyedi kombinációját azonosítja
(ebben a konkrét esetben ez azt jelenti, hogy különböző lesz a lehetséges utolsó betűk halmaza),
tehát egyúttal legalább ennyi különböző részfa is van.

Az előbbi reguláris kifejezés lényegileg azt fejezi ki,
hogy a karakterláncban nem szerepelhet $C$ összes eleme, ezen kívül bármit megengedünk.
Az ehhez hasonló konfigurációk szerencsére meglehetősen életszerűtlennek tűnnek.
Az ilyen szempontból problémás alternációkat akár előre is detektálhatjuk,
és túl nagy becsült növekedés esetén expliciten hibát dobhatunk.

Most pedig nézzük hogyan tudunk engedni a fenti megszorításokon.

A repetíciós operátorok explicite kifejtés által kerülnek visszavezetésre a fentiekre.
A kifejtés természetesen egyes esetekben lényegesen megnövelheti a fa méretét a bemenethez képest.
Ennek kioptimalizálásával most nem foglalkozom.

Először a korlátlan repetíciót (\texttt{*}, \texttt{+} stb.) elimináljuk rendre az alábbi szabályokkal:

\begin{enumerate}
    \item \texttt{$A$+} ~ $\longrightarrow$ ~ \texttt{$A$\{1,\}}
    \item \texttt{$A$\{$n$,\}} ~ $\longrightarrow$ ~ \textt{$A$\{$n$\}$A$*}
    \item \texttt{$A$*} ~ $\longrightarrow$ ~ \texttt{$A$\{$c$\}} ~~~~~ (ahol $c$ előre definiált konstans)
\end{enumerate}

A fix repetíció egyszerű konkatenációvá alakul,
az opcionális repetíció kifejtése pedig a következő rekurzív módon történik:

\begin{enumerate}
    \item $R_1~=$ \texttt{$A$\{,1\}} ~ $\longrightarrow$ ~ \texttt{($A$|)}
    \item $R_n~=$ \texttt{$A$\{,$n$\}} ~ $\longrightarrow$ ~ \texttt{($AR_{n-1}$|)} ~~~~~ (ahol $n>1$)
\end{enumerate}

Az opcionális előfordulás (\texttt{?}) az $R_1$ esettel egyenlő.

A támogatott konstrukciókat könnyen tovább bővíthetjük az egyszerűbb horgonyokkal,
mint például szövegvég (\texttt{\$}) vagy szóhatár (\texttt{{\textbackslash}b}).
Ezeket a kezdeti feldolgozáskor úgy tekintjük,
mint az átmeneti üres csomópontokat, tehát elimináljuk és megjegyezzük őket,
de utána sorban kikényszerítjük a teljesülésüket.
A horgonynak ellentmondó gyermekelemeket eltávolítjuk,
ha ezután nem marad gyermekelem, a teljesíthetetlen ágat jelző kivételt dobunk,
a kivételt elkapó szint el fogja dobni az ágat mint lehetetlent,
ami újabb kivételdobást eredményezhet, ha az ottani gyermekelemek emiatt elfogytak,
és így tovább.

\subsubsection{(+++) Unicode karakterláncok és többszintű rendezés reguláris kifejezésekkel}

Az igazi nehézségek akkor kezdődnek,
amikor elengedjük a karakterenkénti rendezés megszorítását
(ami természetesen további többletköltségeket fog eredményezni,
ezért érdemes opcionálissá, konfigurálhatóvá tenni).
Erre például ékezetes karakterek vagy kis- és nagybetűk keveredése esetén lehet szükségünk.
Ekkor ugyanis esetleg csak egy későbbi eltérés dönti el,
hogy egy korábbi ékezet vagy nagybetűsség számít-e.
Vagyis az egyszerű, karakterenkénti prefix-fa sem elég a rendezettség biztosításához.

Ha egy ilyen szofisztikáltabb rendezési elvet akarunk megvalósítani,
az \textit{Unicode Collation Algorithm} (UCA) szabványból érdemes kiindulni.
A UCA azonban rendkívül összetett,
így egyelőre csak az alapvető szempontok implementálását fogom bemutatni.
Ki-kitérek majd arra, hogyan lehet ezeket a későbbiekben bővíteni, továbbfejleszteni,
hogy még részletesebben lefedje a szabvány által tárgyalt lehetőségeket.

A UCA mindenekelőtt definiál egy általános keretrendszert,
mely egy sémát ad arra, hogyan kell többszintű rendezéseket megvalósítani Unicode karakterláncokon.
A sémát implementáló konkrét algoritmusokat \textit{kollációknak} nevezzük.
Egy Unicode karakterlánc lényegileg Unicode codepoint-ok tömbjét jelenti
(Java környezetben a \texttt{char} adattípus ilyen codepointot tárol).
Egy kolláció egy-egy ilyen tömbhöz rendezési szintenként hozzárendel egy-egy
kollációselem-listát (\textit{Collation Element Array}).
A kollációs elemek lényegileg codepointok, de a kolláció által definiált súlyozás szerint átszámozva.
Tehát úgy is felfoghatjuk, hogy a kolláció a Unicode karakterlánchoz
szintenként egy másik Unicode karakterlánc nézetet rendel,
ahol a codepointok között egyúttal egyedi sorrendezést definiál.
A UCA öt alapértelmezett rendezési szintet különböztet meg:

\begin{itemize}
    \item \textbf{L1}: alapkarakterek (Base characters)
    \item \textbf{L2}: diakritikus jelek (Accents)
    \item \textbf{L3}: kis-/nagybetűsség (Case/Variants)
    \item \textbf{L4}: írásjelek (Punctuation)
    \item \textbf{Ln}: kódolási különbségek (Identical)
\end{itemize}

\todo[inline]{példa a szintekbeli nézetre, esetleg a lista helyett táblázat (legalább lenne táblázat)}

Például a normál magyar kollációban a '\texttt{Kör}' karakterlánc L1-es alakja '\texttt{kor}'
ahol a diakritikus jel és nagybetűsség stb. ignorálásra (eltávolításra) került.
Ellenben például a svéd nyelvben az '\texttt{ö}' betű az ABC végén
teljesen különálló betűként szerepel,
így a normál svéd kollációban a fenti karakterlánc L1-es alakja '\texttt{kör}',
és a rendezésben is egészen máshogy fog viselkedni.

A UCA szerint két karakterlánc összehasonlításakor sorban vizsgálandók az egyes szintek,
míg az egyik szerint különbséget nem találunk.
Ekkor egyszerűen a kollációselem-listák elemenkénti összehasonlítása szerint
kerül megállapításra a két karakterlánc egymáshoz képesti sorrendje.

Az \textit{International Components for Unicode} a Unicode Consortium hivatalos
referenciaimplementációja általában a Unicode szabványhoz,
beleértve számos konkrét kollációt a világ nyelveihez.
Konkrét szabályok kialakításához ebből érdemes kiindulni.
Az ICU általában (beleértve a standard, európai és magyar kollációkat is)
teljesen az L1 szinten kezeli az írásjeleket is
(az egymás utáni több írásjel is külön-külön elsődleges karakternek számít).
Ez logikusabb, mint az írásjeleket egyátalán nem figyelembe venni a betűk vizsgálatakor,
hiszen akkor a szavak tagolása sem történik meg
(például az '\texttt{ab:cd}' előrébb kerül, mint az '\textttt{a:cd}').
Ezt is figyelembe véve, mi most csak az első három szinttel foglalkozunk,
legalábbis kizárólag pozíciótartó összehasonlításokkal.

A kompakt szófa fentebbi implementációját a következő ötlettel tudjuk kibővíteni
a többszintű rendezés támogatásához.
Minden rendezési szinthez külön szófát készítünk, melyeket az eredeti szófa egyszerűsítésével nyerünk
(az életszerűség csorbítása nélkül feltehetjük,
hogy a reguláris kifejezés olyan, hogy ez problémamentesen megtehető).
Ekkor az egyszerűsített fákban egy-egy maximális útvonalhoz 1-nél több eredeti karakterlánc tartozhat
(az egyszerűsítés éppen ezt az összevonást jelenti
az adott szinten egyenlőnek számító karakterláncok között).
Ezeket a szófákat aztán (virtuális értelemben) egymás alá fűzzük,
azaz effektíve minden előző szint EOW csomópontja után konkatenáljuk a következő szint fáját.
Az összevont (például egybeolvadó karakterekhez tartozó) részfák
többszörös számossággal számítanak, amit jelezni kell.

\todo[inline]{ÁBRA: többszintű kompakt fa}

\todo[inline]{ALGORITMUS: többszintű kompakt fa}

Az n-edik karakterlánc lekérése úgy történik,
hogy az egymás alá fűzött fák rendszerében (ami maga is egy nagy faként értelmezhető) navigálunk.
Az első fában normál módon keresünk, azzal a kiegészítéssel,
hogy a szorzó csomópontok egyszerűen olyan részfák listájaként értelmezendők,
melyek a szorzó csomópont gyermekelemeihez tartozó részfákkal egyenlők,
azonban minden méretükben a szorzóval növelve.
A levélhez érve a szorzók miatt egy 1-nél nagyobb számosságot kaphatunk.
Ezt a számosságot a további szintek részfái fogják tovább-bontani.
Az utolsó szint kivételével mindegyik fában megengedett (sőt, a dolog lényegéhez tartozik)
a többszörös számosság.

Világos, hogy (a szorzókat figyelembe véve) egy-egy szint fája
a teljes eredeti értékkészlet számosságával bír.
Amikor azonban már a többedik fában kezdünk továbbmenni,
valójában a fa olyan szűkített nézetét kell vennünk,
amely kompatibilis az összes már előbbi szint fájában bejárt útvonallal.
Például (magyarral kompatibilis kollációs szabályokat feltéve),
ha az első szinten a '\texttt{tarol}' karakterláncot kaptuk,
a második szint fája megengedheti a '\texttt{tarol}', `\texttt{tárol}', '\texttt{táról}' stb.
karakterláncokhoz tartozó útvonalakat,
de mondjuk a (tegyük föl, eredetileg megengedett) "hasonlít" útvonalát már nem.

\todo[inline]{ÁBRA: lekérés a többszintű fából (szűkített fával)}

Ez a szűkítés a bemenettől függ.
Vagyis nem tudjuk előre generálni, a futási idejű performanciát fogja befolyásolni.
A előbb bemutatott adatszerkezet alkalmas arra, hogy ezt hatékonnyá tegye,
de ehhez szükség van egy lényeges megkötésre,
nevezetesen, hogy az egyszerűsítés pozíciótartó legyen,
azaz egy adott karakterlánc összes szintbeli nézetének hossza egyezzen az eredeti hosszal,
és bármely karaktere a vele azonos pozíción található eredeti karakter derivátuma legyen.
Mivel csak pozciótartó összehasonlításokra szorítkoztunk,
ez a megkötés eleve biztosított.

\todo[inline]{ALGORITMUS: lekérés a többszintű fából (fa szűkítésével)}

\todo[inline]{megjegyezni a negyedik szint nem is feltétlen hasznos}
% TODO: különösen ha a szóköz is írásjelnek számít (vs. szavankénti rendezés),
%       illetve ha a speciális karakterek dominálnak illetve fontosak}

Karakterlánc keresésekor is a fenti szempontokkal egészítjük ki
a többszintű rendezést nem támogató kompakt fánál látott algoritmus-sémát.
A fő eltérés, hogy a kereséskor nem az addigi útvonalakkal,
hanem a keresett eredeti karakterlánccal való kompatibilitás számít,
nyilván már az első szinttől kezdve.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%









\subsubsection{(+++) (???) OLD XXX}

A UCA egy nyelvfüggő többszintű összehasonlítási sémát definiál,
de ezek között van olyan általános séma is, mely tekinthető egy nyelvfüggetlen összehasonlításnak.
Ennek egy egyszerűsített háromszintű változatát fogjuk alapul venni.

Minden karaktert három komponens,
nevezetesen az \textit{alapkarakter}, a \textit{diakritikus jel}
és a \textit{kis-/nagybetűsség} együttesének tekintünk.
Alapkarakternek a karakter diakritikus jelektől megfosztott kisbetűs alakját nevezzük.
Tehát például az '\texttt{Á}' karakter az '\texttt{a}' alapkarakterre,
a '\texttt{´}' ékezetre és a nagybetűsségre bontható.
Komponensenként egyértelmű és független lesz a rendezés.

Maguk a karakterláncok is három rétegre esnek szét, és komponensenként hasonlítandók össze.
Összehasonlításkor a későbbi komponens csak akkor vizsgálandó,
ha az azt megelőző komponensek a két karakterláncban egyeznek.
A pozicionált diakritikus jeleket és nagybetűsség-jelzéseket speciális karaktereknek is tekinthetjük,
ekkor lényegileg karakterenkénti összehasonlításra is visszavezethetjük az eljárást.
Például:

\begin{verbatim}
AL      =   al   __   __     =   al<U:0><U:1>
alom    =   alom ____ ____   =   alom
Alom    =   alom ____ U___   =   alom<U:0>
álom    =   alom ´___ ____   =   alom<D:´:0>
BAL     =   bal  ___  UUU    =   bal<U:0><U:1><U:2>
báL     =   bal  _´_  __U    =   bal<D:´:1><U:2>
\end{verbatim}

% FIXME ez a megközelítés jobban adaptálható később a svéd collation-höz
Vagy:

\begin{verbatim}
AL      =   al.al.AL         =   al<U:0><U:1>
alom    =   alom.alom.alom   =   alom
Alom    =   alom.alom.Alom   =   alom<U:0>
álom    =   alom.álom.álom   =   alom<D:´:0>
BAL     =   bal.bal.BAL      =   bal<U:0><U:1><U:2>
báL     =   bal.bál.báL      =   bal<D:´:1><U:2>
\end{verbatim}

A jobb oldalon a karakterláncoknak egy normálalakja látható,
mely az alapkarakterek után csak a módosításokat tartalmazza,
és a kisbetűsséget alapértelmezettnek tekinti.
Az ilyen normálalak karakterenkénti rendezése problémamentes:

\begin{itemize}
    \item egy előbbi komponenshez tartozó karakter mindig megelőzi a későbbit.
    \item a komponensen belüli rendezés %TODO
\end{itemize}

Természetesen másfajta normálalakot is választhatunk (például ahol a nagybetűsség az alapértelmezett),
akár a reguláris kifejezés tulajdonságaitól függően,
de mindenképpen rendelkeznünk kell bizonyos, egymással konzisztens algoritmusokkal,
amelyek együtt implementálják az értékkészlethez tartozó rendezési kollációt:

\begin{enumerate}
    \item karakterláncot és normálalakot egymásba alakító függvényekkel
    \item normálalakban lévő karakterláncokat összehasonlító komparátorral
    \item reguláris kifejezést a normálalak szerinti prefix-gráfba alakító algoritmussal
    \item a prefix-gráfot bejáró, kereső stb. algoritmusokkal
\end{enumerate}

\todo[inline]{a két- vagy háromrétegű prefix-fa leírása}

\todo[inline]{FUTURE PLAN: az UCA még részletesebb és konfigurálható megvalósítása}







% TODO: svédek: å, ä, ö: base characters
% https://en.wikipedia.org/wiki/Swedish_alphabet

% Az ötlet az, hogy egyetlen prefix-fa helyett többet hozunk létre.
% A harmadik a normál prefix-fa, mely az eredeti karaktereket tartalmazza, karakterenként rendezve.
% A második ennek topológiai egyszerűsítése úgy, hogy a kis-nagybetű különbségeket összevonjuk.
% Ahol az összevonásnál azonos gyermekelemeket találunk, azok közül csak az egyiket tartjuk meg,
% de beiktatunk egy szorzó csomópontot (TODO: megmagyarázni).
% Az első fa szintén a normál prefix-fa egyszerűsítése,
% de itt minden azonos alapkarakterű karaktert összevonunk.
% A csomópontok számításánál figyelembe kell venni a szorzókat,
% amelyek felszorozzák a részfák levél-számát.

% az algoritmus:
    % - az első fát normál módon érjük el, azonban a leveleknél 1-nél nagyobb számosságok fognak maradni a szorzók miatt
    % - a második fát le kell szűkíteni az alapkarakterek szerint, ez alternatív számosságokat fog adni, ez alapján tudjuk számolni az alapkarater-fán belüli pozíciókat, itt is 1-nél nagyobb számosságokra lyukadhatunk ki
    % - a harmadik fát szintén szűkítjük a kisbetűs karakterek szerint
% a számosságok csak az érintett karakterekre számítandók, és cache-elhetők
% így elég hatékony algoritmust kapunk







\subsection{EGYÉB DOLGOK, FÁJL STB.}

\todo[inline]{további éértékkészlet-betöltési módok, fájlból stb., monotonic, eloszlás stb.}

\subsection{Full-text indexelt értékkiosztás}

Ez az értékkiosztási mód folyószövegeket állít elő, melyekhez full-text indexet tesz elérhetővé.
Itt most csak a full-text index legegyszerűbb fajtájával foglalkozunk,
melynek bemenete egy néhány szóból álló halmaz,
eredménye pedig az ezen szavak mindegyikét tartalmazó mezők sorindexeinek halmaza.
Rendezéssel ez esetben nem foglalkozunk, a full-text indexek ezt általában nem is biztosítják.
Látni fogjuk, hogy a bemutatott megoldás könnyen továbbfejleszthető.

Első körben induljunk ki abból, hogy adott egy szólista, a mellékelt $f_i$ előfordulási gyakoriságokkal.
Az $f_i$ előfordulási gyakoriság szimulálható is Zipf törvényére alapozva\cite{Zipf1942UnityOfNature}.
A szavakat egyszerűen betűkarakterek sorozataiként fogjuk föl,
előzetesen függetlenítve a központozás, szövegfelépítés problematikájától.

Lesz emellett egy a szövegekre vonatkozó várható szószám is, jelöljük ezt $c$-vel.
A gyakoriság és a hossz együtt meghatározza az adott szó
$p_i$ szereplési valószínűségét valamely szövegben a következő egyszerű képlet szerint:

$$
p_i = 1 - (1 - f_i)^c
$$

Legyen $s$ az egy szövegben várhatóan előforduló szavak halmazának számossága ($s \leq c$).
Az $n$ táblahossz számú mezőre akarjuk képezni a szövegeket,
amit egy $n \times s$ méretű virtuális mátrix segítségével fogunk elvégezni.
A szavakat valamilyen sorrendben véve (a sorrend megválasztásának problémáit most mellőzöm),
az $n s$ hosszú listát feltöltjük úgy, hogy minden szó egy összefüggő sávban szerepel,
a sávok sorrendje az említett szósorrend,
a sávok egymáshoz képesti relatív hossza pedig a $p_i$ szereplési valószínűségen alapul.
Ezt a listát képezzük bele oszlopfolytonosan a mátrixba.

Egy adott sorindexhez tartozó mezőben szereplő szavakat úgy kapjuk,
hogy egyszerűen vesszük a mátrix megfelelő sorát.
Szavakra keresni pedig úgy tudunk, hogy a keresett szavakhoz tartozó
(esetleg oszlopváltással megszakított) sávok metszetsávját vesszük,
ha nincs metszet, nincs találat.
A mátrixsorokat az életszerűség kedvéért érdemes permutálni a fentebb már ismertetett módszerrel.

A mátrixos módszer egyik hátránya,
hogy az egymás mellé került szavak eltartanak egymástól ugyanabban az oszlopban,
így jellemzően nem tudnak majd bekerülni ugyanabba a szövegbe,
illetve általában is viszonylag korlátolt,
hogy melyek a lehetséges konstellációk.
Ez viszonylag jól orvosolható azzal, ha egy-egy szó sávja több független sávba törik szét
(jól ügyelve arra, hogy ezek a részsávok távoli sorokra essenek,
de legalábbis ne legyen metszetük modulo $n$).

Egy másik probléma (hacsaknem épp ez a cél), hogy a szövegek túl uniformak, ugyanannyiféle szóból állnak.
Ezen könnyen segíthetünk, ha a szósávok közé sok apró üres sávot helyezünk el.

A következő ábrák szemléltetik az eddig leírt szóösszeállítási illetve keresési módszert:

\begin{figure}[H]
  \centering
  \hspace*{\fill}
  \begin{minipage}[t]{0.42\textwidth}
    \includesvg[width=\textwidth]{diagram/fulltext-getvalue}
    \caption{Full-text szóösszeállítás}
  \end{minipage}
  \hspace*{\fill}
  \begin{minipage}[t]{0.42\textwidth}
    \includesvg[width=\textwidth]{diagram/fulltext-findvalue}
    \caption{Full-text keresés}
  \end{minipage}
  \hspace*{\fill}
\end{figure}

Egy harmadik egyszerű javítás adódik,
ha előre ismert, mely néhány (ideálisan kb. 6-8) szó lesz gyakran keresve,
ezeket teljesen külön kezelhetjük.
Vesszük az összes lehetséges előfordulási kombinációt azok pontos gyakoriságával,
az így kapott értékkészletet pedig lineárisan rányújtjuk a táblahosszra.
Célszerű ezt a többi szótól teljesen függetlenül permutálni.
Egy mező szóhalmazaként a kétféle kiosztás unióját kell venni,
kereséskor pedig a kettő által adott találatok metszetét.

Megjegyzem, ha vannak kiemelt szavak, akkor a többi szó halmaza részben vagy egészben
akár mesterségesen előállított szavakból is állhat.
Ekkor egy minta alapján generálunk szavakat
(on-the-fly, ld. a reguláris kifejezések kapcsán írottakat),
majd például Zipf törvénye alapján hozzájuk rendeljük a gyakoriságokat.
Ha így járunk el, megspóroljuk a konkrét szavak tárolásának tárhelyigényét.

Egy-egy mezőértékhez még csak a szavak halmazát állítottuk elő.
Amikor lekérjük a mező konkrét értékét, akkor egy tényleges szöveget szeretnénk kapni,
mely pontosan ennek a halmaznak a szavait tartalmazza (sorrendtől és multiplicitástól függetlenül),
és lehetőleg egy viszonylag valóságosnak tűnő szöveg, még ha nem is értelmes.
Vegyük észre, hogy ebben teljes szabadságunk van, hiszen az érték és index konzisztenciája már biztosított.

Az aktuális \texttt{TreeRandom}, a sorindex és a kapott szóhalmaz függvényében
a szöveg generálására tetszőleges módszert használhatunk,
itt tehát egy széles skálázási lehetőség adódik.
A fapados megoldás a halmaz szavainak egyszerű felsorolása szóközzel választva
(abban a sorrendben, ahogy a kiemelt szavak fel voltak sorolva, illetve ahogyan a mátrixból nyertük őket).
Egy nagyon szofisztikált módszer lehet, ha nyelvi modellt használunk a szöveg generálására.

Egy olcsó, ugyanakkor viszonylag jó eredményt adó módszer a következő:

\begin{enumerate}
    \item Határozzuk meg a $c_i$ konkrét szószámot.
    \item Vegyük a szóhalmazt valamilyen konkrét sorrendben.
    \item Helyezzünk ezek közé további szavakat, míg a $c_i$ szószámot el nem érjük.
    \item Határozzuk meg a mondat- illetve részmondathatárok pozícióit.
    \item Fűzzük össze a szöveget szóközökkel, adjuk hozzá a központozást.
\end{enumerate}

Mindegyik lépés finomítható különféle heurisztikákkal,
ezekkel most nem foglalkozom.

A fent tárgyalt full-text index természetes módon bővíthető.
Könnyen integrálható például a kizárandó szavak kezelése.
A szóhalmaz kiválasztásának és a szöveg generálásának összehangolásával
a kifejezésekre való keresés bizonyos formái is megvalósíthatóvá válnak.
Ezekre a bővítési lehetőségekre ezen keretek között szintén nem tudunk részletesen kitérni.



\subsection{Földrajzi koordináták értékkiosztása}

A legegyszerűbben közelítjük meg a problémát,
ami nagyobb dimenziószámra és lényegileg tetszőleges geometriára is átvihető.
Csak a megoldás vázát ismertetem.

Az értékek sűrűségének, elhelyezkedésének valamint opcionálisan a várható lekérdezések
figyelembevételével a felületet térszeletekre osztjuk.
Ennek a felosztásnak nem feltétlenül kell egyenletesnek vagy egyáltalán szabályosnak lennie,
de az egyszerűség kedvéért most maradjunk a szokványos hálónál:
adott $w$ földrajzi hosszúságonként illetve adott $h$ földrajzi szélességenként vágjuk föl a gömbfelszínt.
A kapott háló (a sarkoknál háromszöggé fajuló) kis kvázi-négyszögeket határoz meg.
Ezeket soronként véve egy lineáris sorrendezést kapunk.

A szélesség-hosszúság párokat egy $2d$ bites ábrázolással kódoljuk.
Ennek lehetséges értékkészletére nyújtjuk rá a sorindexeket
a zajosan monoton értékkiosztásoknál ismertetett módszer valamely variánsával,
de itt a nagyléptékű egyenletességet más szempontokra cserélhetjük.
Az értékkészlet összefüggő sávjai a kvázi-négyszögeknek feleltethetők meg,
tehát a kvázi-négyszögekbe tartozó értékekre ugyanúgy tudunk keresni,
mint fentebb az időbélyegekre.

Kereséskor a kvázi-négyszögek két halmazát kell elkülöníteni:
amelyek teljesen az alakzaton belülre esnek,
illetve amelyeket az alakzat határvonala átmetsz.
Az előbbiek értékei feltétel nélkül találatok lesznek.
Utóbbiaknál a kvázi-négyszögön belülre kell hatolnunk,
esetleg egyesével kell ellenőriznünk az értékek alakzaton belüliségét.
Mivel ezek a határvonalhoz köthetők,
számuk egyszerű alakzatok esetén a kerülettel egyenesen arányos,
nem kell tehát területarányos mennyiségű kvázi-négyszöget megbontani,
sőt, az egy sorban egymás után lévők összevontan is kezelhetők.

\begin{figure}[H]
\centering
\includesvg[height=0.4\textwidth]{diagram/geospatial-search}
\caption[Geolokációk keresése alakzat alapján]{
    Geolokációk keresése alakzat alapján, rácsozott gömbfelületen. \\
    Csak a határvonal által metszett mezőkbe eső pontokkal kell részletesebben foglalkozni.
}
\end{figure}


\section{Egyéb értékkiosztási módok}

\subsection{Nem-indexelt egyoszlopos értékkiosztások}

A nem-indexelt oszlopok szimulálásakor a keresési-rendezési szempontokat egyáltalán nem kell figyelembe venni.
Az indexelt oszlopokra vonatkozó elvárások közül így csak az első kettő releváns:

\begin{enumerate}
    \item hatékony elérés (random access)
    \item pozíciótól függő érték (\texttt{TreeRandom} seed)
\end{enumerate}

Vagyis elég, ha a \texttt{TreeRandom} által a sorindexhez generált seed alapján
előállítunk egy normál \texttt{Random} példányt,
és ennek felhasználásával a tartalmat tetszőleges determinisztikus módszerrel állíthatjuk elő.
Erre az előállításra kizárólag a konfigurációban megadott beállítások jelentenek megszorítást.

A következő listában csak fölvillantanám az ilyen adattartalmak néhány jellegzetes,
egyúttal könnyen szimulálható típusát:

\begin{itemize}
  \item \textbf{Egyszerű szöveg:}
    a legegyszerűbb megoldásban egy szótár szavait véletlenszerűen helyezzük egymás után,
    közben feljavítva a szövegképet nagybetűs szavakkal és központozással
    (az eredmény tovább javítható kifejezésminták használatával,
    amikor időnként egy többszavas részletre előre adott sablont használunk;
    de ha a minőség az elsődleges, bevethetünk akár
    Markov-láncokra épülő egyszerűbb nyelvi modelleket\cite{Amrrs2021MarkovHeadlines})
  \item \textbf{Strukturált szöveg:}
    először előállítunk egy általános struktúrát (címek, bekezdések stb.),
    majd ennek elemeit feltöltjük szöveggel (lásd az előző pontot),
    végül a struktúrát a kívánt jelölőnyelven szolgáltatjuk (HTML, MarkDown, plain text stb.)
  \item \textbf{Strukturált adat:}
    generált vagy konfigurációban megadott adatséma alapján rekurzívan generáljuk le az adatstruktúrát,
    ekkor a sémanyelv (pl. JSON Schema) és a kimenet formátuma (JSON, YAML, TOML stb.)
    függetlenül konfigurálható
  \item \textbf{Egyszerűbb képek:}
    véletlen színű háttérre véletlen paraméterekkel
    rárajzolunk néhány egyszerű alakzatot,
    majd a kívánt vektoros vagy raszteres formátumban szolgáltatjuk
  \item \textbf{Jelszó-hash:}
    a tényleges a hash-algoritmust hívjuk meg valamely rendelkezésre álló adatra,
    pusztán formai elvárás esetén akár magára a \texttt{TreeRandom} seedre,
    demózáshoz pedig hasznos lehet a felhasználónevet használni jelszóként
  \item \textbf{Általános BLOB/CLOB:}
    lekéréskor a konfigurációban megadott mérethatárok közötti bájtsort/karaktersort kell visszaadni,
    ami könnyen ellátható random bájtok/karakterek generálásával
    (legegyszerűbb közvetlenül a \texttt{TreeRandom} által szolgáltatott biteket használni)
\end{itemize}

További indexeletlen eset, amikor az oszlop teljes tartalma explicite felsorolva van megadva,
például egy listafájlból.
Nagy táblahossznál érdemes lehet ezt indexeltté tenni egy tényleges adatstruktúrával (például B-Tree),
mivel ilyenkor valószínűleg úgysem a memóriahasználat minimalizálása a cél.

\subsection{RÉGIN REGEXP (előzőbe fésülni)}

\todo[inline]{régi regexp: előzőbe fésülni}

Ha elengedjük a gyors keresés kritériumát,
akkor a reguláris kifejezés alapján történő értékkiosztást nagyon könnyen megvalósíthatjuk
egy véges automatával történő véletlenszerű inverz mintaillesztéssel\footnote{
  A jelenlegi implementáció a \textit{Generex} könyvtárat használja erre.
},
a sorindex és a \texttt{TreeRandom}-ból vett seed figyelembevételével.

Ha azonban fenn szeretnénk tartani a gyors keresés lehetőségét,
szükséges lesz, hogy képezni tudjuk az adott reguláris kifejezésre illeszkedő összes sztring virtuális listáját.
Azaz bármely $n$ sorindexre elő kell tudnunk állítani az $n$-edik illeszkedő sztringet
(méghozzá ABC-rendben, nem pedig a reguláris kifejezés szerkezete alapján).
És fordítva, tetszőleges sztringre meg kell tudni mondani,
hányadik illeszkedő sztringgel azonos vagy melyikhez van közel, ha nem illeszkedik.
Ha már van egy ilyen virtuális listánk, azt a kétfázisú értékkiosztással könnyen a kívánt oszloppá alakíthatjuk.

A megoldás a reguláris kifejezések lehetőségeinek csak valamilyen limitált részhalmazát fogja támogatni.
De még így is bőven lefedi az olyan egyszerű eseteket, mint például a telefonszámok, email-címek stb.

Az ilyen típusú szöveggenerálás részletes bemutatása kimutat a jelen dolgozat keretei közül,
így ennek ismertetését itt mellőzöm\footnote{
  Egy egyszerű erre készült prototípus a \texttt{Strex} könyvtár.
}.

\subsection{Értékkiosztás oszlopok közötti összefüggéssel}

Az oszlopok közötti összefüggésnek számtalan módja lehetséges.
A legáltalánosabb eset képletszerű összefüggések felsorolása lenne,
ami indexeletlen esetben meg is megvalósítható:
a \texttt{TreeRandom}-ból nyert seeddel véletlenszerűsítve futtathatunk valamilyen constraint solvert,
mely a követelményeket teljesítő értékeket determinisztikusan előállítja.
Ha lehetséges, hogy egyes seed értékek mellett a követelményrendszer nem kielégíthető,
akkor meg kell engednünk a $NULL$ értékeket az érintett oszlopokban,
és a kielégíthetetlen esetekben mindegyikre $NULL$-t kell visszaadni
(esetleg valamilyen egyéb alapértelmezett értéket).

Szintén elég általános eset, amikor mesteroszlopok
alapján akarunk indexeletlen oszlopokat egymástól függetlenül legenerálni.
Ekkor teljes szabadságunk van,
értelemszerűen a mesteroszlopok értékei és a \texttt{TreeRandom}-ból nyert seed lesz a bemenet.

Életszerű azonban, hogy indexelt mesteroszlopokon túl
a függő oszlopokra is szeretnénk indexelést.
Ha egy nagy közös index elég,
akkor elégséges a függő értékeket becsoportosítani a mesterértékek alá.

Bonyolult, de még mindig teljesen életszerű eset,
amikor a függő értékek külön önálló indexeket is kapnak
(ilyen eset például, ha az életkor adatot a végzettséghez igazítva generáljuk).
Ha a mesteroszlop(ok) értékkészlete kicsi,
akkor minden ilyen értéken belül elvégezhetjük a keresést illetve rendezést a függő értékekre,
és ezek összefésült eredményét adjuk vissza.

\subsection{Táblák közötti kapcsolatok}

Amíg megmaradunk a read-only szempontoknál,
a táblák közötti idegenkulcs jellegű kapcsolatokra vonatkozó egyetlen kemény követelmény,
hogy a hivatkozó tábla érintett oszlopa (vagy oszlop-$n$-ese)
csak a hivatkozott tábla érintett oszlopához (vagy oszlop-$n$-eséhez)
tartozó értékkészlet elemeit vehesse föl;
vagyis ($NULL$ értékektől eltekintve),
a hivatkozó oszlop értékkiosztásakor az alaplista a hivatkozott oszlop tartalma lesz.
Másként fogalmazva, az értéklistát indextartó módon kell átképezni a másik táblára.
Ez a fentiek tükrében már egyszerűen biztosítható.

Egyes esetekben (bizonyos típusú 1:1 kapcsolatok) valójában egy tábla kerül szétbontásra több táblára.
Ekkor a több táblára bontottság csak külsődleges,
és az előző részben oszlopok közötti összefüggésekről írottak alkalmazhatók,
még akkor is, ha a két tábla számossága nem egyezik teljesen
(a hiányzó sorok a $NULL$ értékek kezelésének fentebb ismertetett
általános módszerével kerülhetnek kiválasztásra).

A legbonyolultabb esetben 1:n kapcsolat mellett különféle statisztikai
vagy egyéb követelményeket biztosító indexelt oszlopokat szeretnénk előállítani.
Ennek tárgyalása messzire vezet,
de egy rövid megjegyzéssel érzékeltetném, hogy a dolog nem lehetetlen:
egy lehetséges kiinduló megközelítés, hogy a két táblát egyként kezeljük,
az egyesített táblában a kisebb tábla adatait pedig redundánsan szerepeltetjük
(ez egy oszlopközi összefüggés a fenti értelemben);
így az összefüggéseket már táblán belül tudjuk értelmezni.



\chapter{Gyakorlati eredmények}

\section{Néhány bővítési példaszcenárió}

Eddig általános igényekkel foglalkoztunk,
melyeket a megoldásnak out-of-the-box kell támogatnia.
Alább bemutatok három speciálisabb szcenáriót,
demonstrálva az architektúra bővíthetőségét és rugalmasságát.

\subsection{Valós címadatok használata}

Előfordulhat, hogy bizonyos adatok óriás értékkészletét külső adattárakból szeretnénk betölteni,
amivel ugyan elvesztjük a teljes virtualitás kis memóriaigényét és további kedvező jellemzőit,
de még mindig igényt támaszthatunk az egyéb előnyökre.
Most egy ilyen szcenáriót vizsgálok meg:
valamely tábla rekordjaiban olyan magyarországi közigazgatási címeket szeretnénk viszontlátni,
melyek valósak.
Amennyire lehetséges, indexeket is szeretnénk biztosítani.

A cím elemei különálló mezőkre bonthatók.
Első körben kiválasztjuk azokat, melyek egy egyértelmű hierarchiát definiálnak:

\begin{center}
    településnév ~ $\rightarrow$ ~
    kerület ~ $\rightarrow$ ~
    közterület neve ~ $\rightarrow$ ~
    közterület jellege ~ $\rightarrow$ ~
    házszám ~ $\rightarrow$ ~
    egyéb
\end{center}

Az „egyéb” részt nem szofisztikáljuk tovább, egy szabad szöveges mezőnek tekintjük.

Néhány további mező nem tartozik a fő hierarchiába:

\begin{center}
    település típusa ~ $\bullet$ ~
    vármegye ~ $\bullet$ ~
    irányítószám
\end{center}

Utóbbi mezőkre külön-külön indexeket fogunk biztosítani.
A fő hierarchiára pedig egy közös többoszlopos (leftmost prefix) indexet,
pontosabban kettőt:
lesz egy rövidített változat vidéki címekhez, melyben a kerületet nem kell megadni,
és implicite üres értékkel töltődik, amikor továbbhív a másik indexre.

A címadatbázist egy nagy fastruktúrába rendezzük,
minden szintnek a fenti hierarchia egy-egy eleme felel meg.
Az alsóbb szinteken spórolhatunk a tárolással,
például egymásutáni megbontatlan házszámok tartományként tárolhatók.
Egy prefix indexhívásnál a megfelelő szintig megyünk le a keresésben,
a találatok pedig az az alatti levelek lesznek.
A tényleges rekordokra való leképezést
a kétfázisú értékkiosztásnál megismerthez hasonló módszerrel végezzük el.

A fő hierarchián kívüli mezők értékei és a fa közötti indexelést külön adatstruktúrába generáljuk.
Változó lehet, hogy a tábla mely szintjén történik meg az irányítószámra való leképezés,
a legtöbb helyen egy egész várost tudunk hozzárendelni.

\subsection{Valószerű geográfiai értékek előállítása}

A probléma: földrajzi koordinátákhoz akarunk valamilyen számszerű adatot kapcsolni úgy,
hogy mindkettőhöz külön index tartozik,
és a koordinátával ellátott adatok valamilyen szempontból valószerű elrendeződést mutatnak.
Olyan megoldást kell találni, ahol tetszőleges számú ilyen kapcsolt adat akár függetlenül generálható
(különben elég lenne a szélesség-hosszúság-adat háromdimenziós terében megfelelően alkalmazni
a földrajzi koordináták fentebb ismertetett értékkiosztását).

Az alapötlet rendkívül egyszerű, és kellően rugalmas.
Szükségünk lesz egy $F$ függvényre, mely minden földrajzi koordinátához rendel egy kapcsolt értéket.
A kapcsolt adat értékkészletét kis sávokra bontjuk föl (nevezzük ezeket kontextussávoknak),
és minden sávhoz le kell tudnunk kérni azon alakzatok (véges) listáját,
melyekben az $F$ függvény értéke a sávba esik.
Bármilyen függvény megfelel, ha ezeket tudjuk biztosítani hozzá.
Használhatunk például teljesen virtuális háromszögelt felületet,
ahol egy-egy háromszög mindig egy konkrét értéksávba esik, nem nyúlik át.
Vagy akár betölthetünk tényleges domborzati adatokat.

Ha ezt sikerült megoldani, az értékkiosztás működése már triviális.
Adott rekordhoz a földrajzi koordinátát a korábban ismertetett módszerrel generáljuk,
a kapcsolt adatot ez alapján számítjuk az $F$ függvény szerint.
A kapcsolt adat valamely egyéni értéksávjára való kereséskor
lekérjük a sávon teljesen belülre eső illetve a kilógó kontextussávokat,
lekérjük az ezekhez tartozó alakzatokat,
és ezen alakzatokra keressük a földrajzi koordinátákat.
A keresett sávon teljesen belülre eső kontextussávokhoz tartozó találatok feltétel nélkül megtarthatók.
A kilógó kontextussávokban le kell kérni a konkrét kapcsolt értéket,
és eldobni a találatot, ha az mégsem esik a keresett sávba.

\subsection{On-demand schema}

Egyes használati esetek megengedik,
hogy további drasztikus lépésként teljesen nyitottá és automatikussá tegyük a virtuális adatbázis felépítését.
Ebben az esetben konfigurációs fájl létrehozására sincsen szükség,
mindent a beérkező kérések alapján fogunk generálni.
Itt feltételezem, hogy csak olvasási műveletek érkeznek,
de az elv, komoly komplikációk vállalásával, írható műveletekre is kiterjeszthető.

Néhány alapbeállítást (pl. \texttt{seed}) beégethetünk, vagy az első lekérés függvényévé tehetünk.

A táblás lekérések implicite megadnak valamilyen szerkezetet,
mivel konkrét sémákon és táblákon belüli konkrét oszlopokat próbálnak elérni.
Ezeket on-demand kell létrehozni, és a továbbiakban megtartani.
A táblák hosszának beállításakor esetleg figyelembe vehetjük az első lekérés limit értékeit, ha van.
Az oszlop paramétereit a lekérésben az adott oszlopra vonatkozó
implicit információk alapján kell meghatároznunk.
Az oszlopnév, az érték használati módja, a keresési feltételek stb.
mind hasznosak lehetnek.
Ha nem tudtuk eléggé leszűkíteni az oszlop típusát,
valamilyen kellően generikus alapértelmezett beállítást kell használnunk,
ekkor is vállalva persze, hogy egy jövőbeli lekérdezés ezzel inkompatibilis lesz.

A wildcardos lekérdezéseknél többféle stratégiát követhetünk,
legegyszerűbb, ha egyszerűen az addigi oszlopokat adjuk vissza
(új tábla esetén egy alapértelmezett oszlopot adhatunk hozzá).

A szisztéma megvalósításához mindössze egy proxy réteg szükséges,
mely minden beérkező kérés továbbítása előtt megfelelően kiegészíti a konfigurációt,
és újratölti a virtuális adatbázist, ha szükséges.
A \texttt{TreeRandom} hierarchia természetéből adódóan a korábbi oszlopok
továbbra is ugyanazokat a konkrét értékeket fogják szolgáltatni.


\section{(SZÉTBONTANI, SOK FEJEZET LÁTSZÓDJON) (+bővíteni, módszertan stb.) Empirikus eredmények}

A HoloDB teljesítményét mind egyedi lekérdezések szintjén,
mind realisztikus integrált teszt keretében mértem.\footnote{
    A használt benchmark projekt forráskódjának linkjét megadtam a függelékben.
}
A Docker-képek összehasonlítását leszámítva minden esetben
a HoloDB beágyazott verzióját használtam.
Az összehasonlítást MySQL-lel (mint az egyik legelterjedtebb szerveradatbázissal)
és H2-vel (mint a Java környezetben legelterjedtebb beágyazott adatbázissal) valósítottam meg.

Az integrált tesztelést egy Micronaut keretrendszerben implementált
alkalmazás REST API-n keresztüli hívásával futtattam.
A \ref{fig:benchmarkPage}~ábra felső részén
az integrált teszt három verziójának eredményei láthatók.
A \texttt{simple-readonly} teszt főleg egyszerűbb lekérdezéseket tartalmazott,
a \texttt{complex-readonly} teszt ezt néhány összetettebb lekérdezéssel egészítette ki.
A \texttt{complex-writeable} tesztben pedig adatírások,
majd arra épülő lekérdezések is történtek.

A lekérdezések közvetlen mérésénél az SQL parancsnak a JDBC API-n keresztüli beküldésétől
az eredménytábla végigiterálásnak végéig eltelt időt mértem.
Különböző bonyolultságú és válaszméretű lekéréseket vizsgáltam.
A HoloDB teljesítménye igen jó, amíg kevés adat lekéréséről van szó,
és drasztikusan romlik, amikor sok adatot kell visszaadni.
Vagyis a korábban tárgyalt indexelési technikák valóban jól teljesítenek,
és kulcsfontosságúak a megfelelő teljesítményhez.
Ugyanakkor az adatelérés kevésbé optimális;
feltehetően mind az adatok számítása, mind az eredménytábla továbbítása jelentősen javítható még.

Az adatok tárhelyigényében a HoloDB természetesen drasztikus csökkenést mutat,
hiszen eleve így lett tervezve.
A szerveres kontextust nézve, a HoloDB Docker-képének mérete jóval kisebb, mint a MySQL-é;
de a base image változtatásával,
az operációs rendszer és a JRE testreszabásával ez valószínűleg tovább javítható.
A minden függőséget tartalmazó shadow JAR lényegesen nagyobb, mint a H2-nél,
ám ez minden bizonnyal jelentősen csökkenthető (ez eddig nem is volt szempont).


\cleardoublepage

\begin{figure}[H]
    \centering

    \begin{tikzpicture}
        \begin{axis}[
                title={Integrált tesztek átlagos futási idejének összehasonlítása (Micronaut$,$ REST)},
                symbolic x coords={simple-readonly,complex-readonly,complex-writeable},
                ybar,
                axis on top,
                width={\textwidth},
                height=6cm,
                bar width=0.8cm,
                ymajorgrids,
                tick align=inside,
                enlarge y limits={value=.1,upper},
                enlarge x limits=0.2,
                ymin=0,
                ymax=17,
                axis x line*=bottom,
                axis y line*=left,
                tickwidth=1pt,
                ylabel={Futási idő (s)},
                xtick=data,
                nodes near coords={
                    \pgfmathprintnumber{\pgfplotspointmeta}
                }
            ]
            \addplot[fill=cyan] % MySQL
                coordinates {
                    (simple-readonly,7.870093628)
                    (complex-readonly,9.0179515675)
                    (complex-writeable,14.3489598653)
                };
            \addplot[fill=blue] % H2
                coordinates {
                    (simple-readonly,5.103988787)
                    (complex-readonly,5.1902669386)
                    (complex-writeable,8.7714582641)
                };
            \addplot[fill=yellow] % HoloDB (fapados)
                coordinates {
                    (simple-readonly,6.441775179)
                    (complex-readonly,8.0399578991)
                    (complex-writeable,13.4957423082)
                };
            \addplot[fill=orange] % HoloDB (default)
                coordinates {
                    (simple-readonly,8.182373242)
                    (complex-readonly,9.4207860077)
                    (complex-writeable,14.4941282014)
                };
            \addplot[fill=red] % HoloDB (extra konf.)
                coordinates {
                    (simple-readonly,8.819935654)
                    (complex-readonly,10.5782986771)
                    (complex-writeable,16.7720866313)
                };
            \legend{}
        \end{axis}
    \end{tikzpicture}

    \vspace{\baselineskip}

    \begin{tikzpicture}
        \begin{axis}[
                title={Néhány lekérdezéstípus átlagos futási idejének összehasonlítása (log.)},
                title style={
                    at={(-0.1,1.15)},
                    anchor=north west
                },
                symbolic x coords={Mező,Rekord,Összetett,Teljes tábla,Többtáblás},
                ybar,
                axis on top,
                width={\textwidth},
                height=6cm,
                bar width=0.4cm,
                ymajorgrids,
                tick align=inside,
                enlarge y limits={value=.1,upper},
                enlarge x limits=0.15,
                ymin=0,
                ymax=2000000000,
                ymode=log,
                point meta=rawy,
                every node near coord/.append style={
                    anchor=west,
                    rotate=90,
                    font=\small
                },
                axis x line*=bottom,
                axis y line*=left,
                tickwidth=1pt,
                legend style={
                    at={(0.5,-0.2)},
                    anchor=north,
                    legend columns=-1,
                    /tikz/every even column/.append style={column sep=0.3cm}
                },
                ylabel={Futási idő (ns)},
                xtick=data,
                nodes near coords={
                    \pgfmathprintnumber{\pgfplotspointmeta}
                }
            ]
            \addplot[fill=cyan] % MySQL
                coordinates {
                    (Mező,106276)
                    (Rekord,109819)
                    (Összetett,146710)
                    (Teljes tábla,146710)
                    (Többtáblás,1637889)
                };
            \addplot[fill=blue] % H2
                coordinates {
                    (Mező,12648)
                    (Rekord,13015)
                    (Összetett,21927)
                    (Teljes tábla,1971234)
                    (Többtáblás,189958)
                };
            \addplot[fill=yellow] % HoloDB (fapados)
                coordinates {
                    (Mező,21709)
                    (Rekord,24207)
                    (Összetett,50474)
                    (Teljes tábla,32271101)
                    (Többtáblás,481306)
                };
            \addplot[fill=orange] % HoloDB (default)
                coordinates {
                    (Mező,25941)
                    (Rekord,140232)
                    (Összetett,293885)
                    (Teljes tábla,1228183178)
                    (Többtáblás,4030303)
                };
            \addplot[fill=red] % HoloDB (extra konf.
                coordinates {
                    (Mező,49860)
                    (Rekord,207383)
                    (Összetett,2543441)
                    (Teljes tábla,1676796378)
                    (Többtáblás,8121736)
                };
            \legend{MySQL,H2,HoloDB (fapados),HoloDB (default),HoloDB (extra konf.)}
        \end{axis}
    \end{tikzpicture}

    \vspace{\baselineskip}

    \begin{minipage}[b]{0.25\textwidth}
        \begin{tikzpicture}
            \begin{axis}[
                    symbolic x coords={Docker image},
                    ybar,
                    axis on top,
                    width={\textwidth},
                    height=7.5cm,
                    bar width=0.8cm,
                    ymajorgrids,
                    tick align=inside,
                    enlarge y limits={value=.1,upper},
                    enlarge x limits=0.15,
                    ymin=0,
                    ymax=700,
                    point meta=rawy,
                    every node near coord/.append style={
                        anchor=west,
                        rotate=90,
                        font=\small
                    },
                    axis x line*=bottom,
                    axis y line*=left,
                    tickwidth=1pt,
                    legend style={
                        at={(0.5,-0.2)},
                        anchor=north,
                        legend columns=-1,
                        /tikz/every even column/.append style={column sep=0.3cm}
                    },
                    ylabel={Méret (MB)},
                    xtick=data,
                    nodes near coords={
                        \pgfmathprintnumber{\pgfplotspointmeta}
                    }
                ]
                \addplot[fill=cyan] % MySQL
                    coordinates { (Docker image,605.3) };
                \addplot[fill=orange] % HoloDB
                    coordinates { (Docker image,231.7) };
                \legend{MySQL,HoloDB}
            \end{axis}
        \end{tikzpicture}
    \end{minipage}
    \hspace*{\fill}
    \begin{minipage}[b]{0.25\textwidth}
        \begin{tikzpicture}
            \begin{axis}[
                    symbolic x coords={Shadow JAR},
                    ybar,
                    axis on top,
                    width={\textwidth},
                    height=7.5cm,
                    bar width=0.8cm,
                    ymajorgrids,
                    tick align=inside,
                    enlarge y limits={value=.1,upper},
                    enlarge x limits=0.15,
                    ymin=0,
                    ymax=8,
                    point meta=rawy,
                    every node near coord/.append style={
                        anchor=west,
                        rotate=90,
                        font=\small
                    },
                    axis x line*=bottom,
                    axis y line*=left,
                    tickwidth=1pt,
                    legend style={
                        at={(0.5,-0.2)},
                        anchor=north,
                        legend columns=-1,
                        /tikz/every even column/.append style={column sep=0.3cm}
                    },
                    ylabel={Méret (MB)},
                    xtick=data,
                    nodes near coords={
                        \pgfmathprintnumber{\pgfplotspointmeta}
                    }
                ]
                \addplot[fill=blue] % H2
                    coordinates { (Shadow JAR,2.49) };
                \addplot[fill=orange] % HoloDB
                    coordinates { (Shadow JAR,7.65) };
                \legend{H2,HoloDB}
            \end{axis}
        \end{tikzpicture}
    \end{minipage}
    \hspace*{\fill}
    \begin{minipage}[b]{0.4\textwidth}
        \begin{tikzpicture}
            \begin{axis}[
                    symbolic x coords={Adatméret},
                    ybar,
                    axis on top,
                    width={\textwidth},
                    height=7.5cm,
                    bar width=0.8cm,
                    ymajorgrids,
                    tick align=inside,
                    enlarge y limits={value=.1,upper},
                    enlarge x limits=0.15,
                    ymin=0,
                    ymax=2.4,
                    point meta=rawy,
                    every node near coord/.append style={
                        anchor=west,
                        rotate=90,
                        font=\small
                    },
                    axis x line*=bottom,
                    axis y line*=left,
                    tickwidth=1pt,
                    legend style={
                        at={(0.5,-0.2)},
                        anchor=north,
                        legend columns=-1,
                        /tikz/every even column/.append style={column sep=0.3cm}
                    },
                    ylabel={Méret (MB)},
                    xtick=data,
                    nodes near coords={
                        \pgfmathprintnumber{\pgfplotspointmeta}
                    }
                ]
                \addplot[fill=cyan] % MySQL
                    coordinates { (Adatméret,2.27) };
                \addplot[fill=blue] % H2
                    coordinates { (Adatméret,0.55) };
                \addplot[fill=orange] % HoloDB
                    coordinates { (Adatméret,0.001) };
                \legend{MySQL,H2,HoloDB}
            \end{axis}
        \end{tikzpicture}
    \end{minipage}

    \vspace{1.5\baselineskip}

    \caption[Teljesítmény-összehasonlítás]{
        A különböző megközelítések teljesítményének és tárigényének összehasonlítása
    }
    \label{fig:benchmarkPage}
\end{figure}


\chapter{Összegzés}

\section{Az eredmények értékelése}

A dolgozatban az adatbázismockolás egy újszerű megközelítését mutattam be.
Láthattuk, hogy néhány alapvetően egyszerű ötlet kombinálásával lehetséges
egy funkcionálisan teljes értékű relációs adatbázist működtetni,
anélkül, hogy azt ténylegesen tárolni kellene.

A prototípus a teszteredmények alapján valóban drasztikusan csökkenti az erőforráshasználatot,
miközben tisztán deklaratív, snapshot jellegű entitássá teszi a nem-produkciós adatbázist.
Megmutattam, hogy memóriahasználatban a HoloDB lényegesen kedvezőbb,
mint a vizsgált alternatívák;
valamint hogy runtime teljesítménye (nem óriás méretű eredménytáblák esetén)
összemérhető a valódi adatbázisokéval;
ez pedig a generálási, anonimizálási és egyéb inicializáló lépések kikerülése miatt
összességében gyorsabb tesztfuttatást tesz lehetővé.

A deklaratív konfiguráció átláthatóan, integráltan írja le a sémát és az adatokat,
kényelmesen verziókezelhető, generálható, előfeldolgozható.

Azzal, hogy nincs jelentős generálási és indítási költség,
olyan új lehetőségek nyílnak meg, mint a teljes adatbázistartalommal történő füstteszt,
vagy a teljesen izolált automatikus integrációs tesztek.

\section{A tézisek teljesülése}

\todo[inline]{Tézisek teljesülését tételesen vizsgálni}

\section{Tervek a közeljövőre}

A HoloDB egy alapvető, a szoftverfejlesztők többségét érintő probléma megoldását kínálja.
Ha a szoftver, leküzdve gyermekbetegségeit, eljut egy igazán stabil verzióig,
valamint sikerül a koncepció ideáját a fejlesztők szélesebb nyilvánosságával megismertetni,
úgy természetes következményként nagy népszerűségre lehet számítani.

A további munkát több frontvonalra tudom bontani:

\begin{itemize}
    \item az alapvető működés optimalizálása
    \item további kiegészítő funkciók, eszközök fejlesztése
    \item dokumentáció és kommunikáció
\end{itemize}

Az értékkiosztásokat és lekérdezéseket sok apró alkatrész teljesítményének finomhangolásával,
elsősorban algoritmuselméleti kutatással lehet feljavítani.
Ugyanilyen fontos viszont a programkörnyezet hatékonyabbá tétele is,
különösen a Docker-képé (lehetőleg GraalVM alapon).
Optimalizáláson nem csak a performancia tuningolását,
de az architekturális célszerűség növelését és a felhasználói élmény javítását is értem.
Ebbe a logikus, könnyen érthető, lényegre törő konfiguráció éppúgy beletartozik,
mint a kiegészítő eszközök ergonómiája.

Az új eszközök fejlesztése elsősorban a különféle meglévő technológiákkal történő integrációt jelenti
(tervezőeszközök, adatbázis-böngészők, programozási környezetek stb.).
Másodsorban pedig a használat megkönnyítését szeretném megcélozni
(generálás/materializálás, további REPL-funkciók, telepítőcsomagok, webes API-k stb.).

Az API-dokumentáción túl egy-egy alapvető kalauz megtalálható
az egyes projektek \texttt{README.md} fájljaiban,
valamint különféle instant elindítható példaprojektek is segítik az ismerkedést.
Ezek bővítésén túl tutorial stílusú leírásokra is szükség lesz.
Felmérendő, hogy milyen eszközökkel lenne hatékony az aktív kapcsolattartás a potenciális ügyfelekkel.
A bevett online felületeken túl (\textit{GitHub}, \textit{Stack Overflow}, \textit{BetaPage} stb.)
mind az akadémiai, mind a versenyszféra-közegben tervezem,
hogy workshopokon és kérdőíves formában is gyűjtsem a visszajelzéseket.

És ha már a kommunikációnál tartunk, maradt itt még egy elsőre talán mellékesnek tűnő,
valójában azonban égető probléma.
Nevezetesen, hogy az új megoldás esszenciája a bevett terminológiákkal
csak tekervényesen fogalmazható meg (ld. a dolgozat címét).
Egy megfelelő, jellegzetes és egyszerű megnevezés
szükséges feltétele lesz a szélesebb körű népszerűsítésnek.
Felmerült jelöltként például
a \textit{holografikus adatbázis},
a \textit{fantomadatbázis},
és a \textit{deklaratív adatbázis} kifejezés.
Mindegyik mellett szólnak pro és kontra érvek.

\pagebreak


\phantomsection
\addcontentsline{toc}{chapter}{\biblabel}
\printbibliography[title=\biblabel]
\cleardoublepage


\appendix

\chapter{Algoritmusok}

\section{Algoritmus 1}
\label{appendix:algorithm_1}

HELLO

\cleardoublepage

\section{Algoritmus XYZ}
\label{appendix:algorithm_xyz}

\begin{algorithm}[H]
    \KwData{this text}
    \KwResult{how to write algorithm with \LaTeX2e }
    initialization\;
    \While{not at end of this document}{
        read current\;
        \eIf{understand}{
            go to next section\;
            current section becomes this one\;
            }{
            go back to the beginning of current section\;
        }
    }
    \caption{How to write algorithms}
\end{algorithm}

\cleardoublepage


\chapter{\lstfigurelabel}
\listoffigures
\cleardoublepage


\chapter{Projektlinkek}

\begin{itemize}
    \item GitHub organization: \par \url{https://github.com/miniconnect/}
    \item Használati példák: \par \url{https://github.com/miniconnect/general-docs/tree/main/examples}
    \item HoloDB projekt: \par \url{https://github.com/miniconnect/holodb}
    \item Docker Hub repository: \par \url{https://hub.docker.com/r/miniconnect/holodb/tags}
    \item \texttt{LargeInteger} benchmark: \par \url{https://github.com/miniconnect/miniconnect-api/tree/master/projects/lang/src/jmh/java/hu/webarticum/miniconnect/lang}
    \item HoloDB benchmark: \par \url{https://github.com/davidsusu/holodb-tdk/tree/main/benchmark}
\end{itemize}

\end{document}
