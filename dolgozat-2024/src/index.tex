\documentclass[
    parspace,
    noindent,
    nohyp,
]{elteiktdk}[2023/04/10]

\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{svg}
\usepackage{float}
\usepackage{pgfplots}
\usepackage{arydshln}
\usepackage{fontawesome}
\usepackage{picture,xcolor}
\usepackage{pdflscape}
\usepackage{fancyvrb}
\usepackage[backend=bibtex,style=numeric]{biblatex}

\usepackage{todonotes}

\newcommand{\rhpad}{\vspace{0.6\baselineskip}}

\newcommand{\thesispar}[1]{
\vspace{1em}
\hspace{0.7cm}\parbox[left][][c]{15.8cm}{\linespread{1.2}\selectfont #1}
\vspace{1em}
}

\usepackage[newfloat]{minted}

\title{HoloDB: szintetikus adatok on-the-fly szolgáltatása relációs adatbázisként}
\date{2024}
\author{Horváth Dávid \\ {\small\href{mailto:horvathdown@student.elte.hu}{horvathdown@student.elte.hu}} }
\degree{Programtervező Informatikus BSc}

\supervisor{Dr. Vincellér Zoltán}
\affiliation{Mesteroktató}

\university{Eötvös Loránd Tudományegyetem}
\faculty{Informatikai Kar}
\department{Információs Rendszerek Tanszék}
\city{Budapest}
\logo{elte_cimer_szines}


\addbibresource{references.bib}

\begin{document}

\documentlang{hungarian}

\listoftodos
\cleardoublepage

\makecover
\cleardoublepage

\maketitle

\tableofcontents
\cleardoublepage


\begin{abstract}
ABSTRACT
\end{abstract}



\chapter{STORAGE API}

\section{ARCHITEKTÚRA}

\section{REFERENCIA-IMPLEMENTÁCIÓK (így???)}

\subsection{KÉTLÉPÉSES ÉRTÉKKIOSZTÁS}

\subsection{A monoton méretigazítás megvalósítása}

A \texttt{Monotonic} interfész példánya egy függvényt reprezentál,
amely az első $n$ nemnegatív egész számot értékkészletként nem monoton növekvő módon hozzárendeli az első $N$ nemnegatív egész számhoz.
Ez arra használható, hogy egy $n$ elemű, rendezetten tárolt értékkészetet egy $N$ méretű adatbázistábla oszlopához igazítsunk méretben,
Tehát a példány $M(i) = m$ függvényt valósítja meg, ahol $0 \leq i < N$ illetve $0 \leq m < n$, továbbá $i_1 < i_2 \implies M(i_1) < M(i_2)$.
Ugyanakkor indexelt elérést is biztosít, azaz lekérdezhető, hogy egy adott érték mely indexsávhoz rendelődik,
azaz megvalósítja az $M^{-1}(m) = [i, j)$ függvényt is, ahol $0 \leq m < n$ illetve $0 \leq i \leq j \leq N$, és teljesülnek az alábbiak:

\begin{itemize}
\item $x < i \implies M(x) < M(i)$
\item $i \leq x < j \implies M(i) \leq M(x) < M(j)$.
\item $i < x \implies M(i) < M(x)$
\end{itemize}

Az indexelt elérés az \texttt{indicesOf(value)} metóduson keresztül biztosított, amely egy \texttt{Range} objektumot ad vissza.
A metódus egy variánsa maga is egy \texttt{Range} példányt vár, és értelemszerűen általánosítja az előbbit.

\subsubsection{Triviális implementációk}

\todo[inline]{Monotonic: triviális implementációk}

\todo[inline]{Monotonic: egyszerű diszkrét lineáris interpoláció: általában ez is elég, a permutáció majd úgyis elkeveri}

\subsubsection{Szimulált mintavétel}

\todo[inline]{Monotonic: szimulált mintavétel (jó ez a cím?)}

\todo[inline]{Monotonic: szimulált mintavétel: a lényeg, hogy bizonyos mértékű véletlenszerűséget ad}

\subsubsection{Gyakoriságtáblázat használata}

A fenti implementációk mindegyike általánosítható úgy, hogy figyelembe vegyen egy gyakoriságtáblázatot,
mely az értékkészlet elemeihez azok relatív gyakoriságát rendeli.

\todo[inline]{Monotonic: gyakoriságtáblázat}

\subsection{Permutációk megvalósítása}

A \texttt{Permutation} interfész megvalósításához olyan eljárást keresünk, amellyel változó (és akár óriás) tartományon értelmezett,
seed szerint variálható permutációfüggvény és annak inverze előáll, ahol mindkét függvény hatékony.

Az inverz függvényhívás az \texttt{indexOf(value)} metódus biztosítja.

\subsubsection{Triviális implementációk}

Triviális implementációként használhatjuk egyszerűen az azonosságfüggvényt:

$$
P_{id}(i) = i (0 \leq i < n)
$$

Gyorsan számolható, de ránézésre már jó keveredést adó permutációt nyerünk, ha kihasználjuk, hogy az $ai \equiv p \pmod{n}$ lineáris kongruencia
$i$-re és $p$-re nézve egy permutációt ad, ha $a$ és $n$ relatív prímek:

$$
P_n(a,b)(i) = ai + b \mod n
$$

Az $a$ és $b$ számot előre, a seed alapján határozzuk meg.
Az $a$ szám relatív prím kell legyen $n$-re nézve, amit úgy érünk el, hogy ciklusban az euklideszi algoritmussal ellenőrizzük, hogy a legnagyobb közös osztójuk 1-e,
és amíg nem, addig meghívjuk a számot tároló \texttt{LargeInteger} példány \texttt{nextProbablePrime()} metódusát az új jelölt lekéréséhez.
Ez a háttérben a \texttt{BigInteger} osztály ugyanilyen nevű metódusát hívja, amely a Java beépített heurisztikus megoldása a következő valószínű prím megkeresésére.
Nekünk csak egy relatív prím kell, tehát nincs szükség valódi prímtesztre, elég a gyors heurisztika és aztán a legnagyobb közös osztó ellenőrzése.

\subsubsection{Feistel-hálózatok alkalmazása}

Az invertálható, változtatható méretű, seed alapján újrakeverhető permutációk könnyen analógiába állíthatók kriptográfiai titkosítófüggvényekkel.
Ezek általában dekódolhatóak, változtatható blokkméretűek és kulcs alapján újrakeverhetőek.
Felmerül a gondolat, hogy közvetlenül egy ilyen kriptográfiai módszert alkalmazzunk a permutáció megvalósításához.
Mindenképpen olyan megoldásra van szükség, amelynél a blokkméret tetszőlegesen megadható, és kellően flexibilis, skálázható.

A Feistel-hálózatok pontosan ilyenek, és emiatt gyakran használják is őket összetett titkosító eljárások keretrendszereként.
A Fesitel-hálózat egy többkörös eljárás, ahol minden körben az adat egyik felének bitjeit módosítjuk.
Ez alapesetben megköveteli a páros blokkméretet, de mindjárt kitérek arra, hogyan lehet ezen enyhíteni.

Az egyes körökben felváltva a bitsorozat bal illetve jobb oldali felét változtatjuk; nevezzük ezt célhelynek, a másik fél tartalmát pedig forrásbiteknek.
Egy-egy kör abból áll, hogy a forrásbitekre meghívunk egy az adott körhöz dedikált hossztartó hash-függvényt,
és az így kapott eredményt XOR-ral ráfésüljük a célhelyre.
Vegyük észre, hogy az XOR művelet tulajdonságai miatt ez reverzibilis művelet (bármi legyen is a hash-függvény),
ha újra alkalmazzuk, az eredeti bitsorozatot kapjuk vissza,
és a forrásbitek még rendelkezésre állnak, mivel azokat ezen a ponton még nem írtuk felül.
Tehát a Feistel-hálózattal kapott kód visszafejtése úgy történik, hogy a lépéseket visszafelé újra végrehajtjuk.
Általában nem szükséges, hogy az egyes körökhöz különböző hash-függvényt használjunk,
számunkra bőven elég, ha a páros és páratlan körökhöz eltérő függvényt alkalmazunk (szigorúan még ez sem lenne szükséges).

A Feistel-hálózatokkal való kísérletezés során rájöttem,hogy a páros blokkméret követelménye enyhíthető a következőképpen.
Páratlan blokkméret esetén vegyünk még egy zéró bitet az adat végéhez,
amivel biztosítjuk, hogy a jobb oldali bitmező is a megfelelő méretű legyen.
Minden olyan kör végén, ahol a jobb oldali biteket írtuk felül, az utolsó bitet ezután nullázzuk.
A legvégén pedig csak az utolsó bit nélküli részt adjuk vissza.
Az utolsó bit ekkor minden művelet előtt és után is fixen zéró,
ami természetesen visszafelé haladva ugyanúgy teljesül, tehát a dekódolás is determinisztikus marad.
Ha egyúttal páros számú kört követelünk meg,
akkor az utolsó kör végére garantáltan ugyanabba a fázisba kerülünk, mint az első kör kezdete előtt;
vagyis a dekódolás folyamata teljesen egyezni fog az elkódolással, ami egyszerűsíti az eljárást.

\todo[inline]{Feistel: kell egyáltalán a páros számú kör követelménye?}

\todo[inline]{Feistel: volt már tárgyalva ez a speciális padding technika?}

\todo[inline]{Feistel: algoritmust csatolni}

Persze, még ha a páros blokkméret követelményét így sikerült is eliminálni,
továbbra is adott a probléma, hogy az $n$ blokkmérethez mindig $2^n$ méretű permutáció tartozik.
Vagyis, ha nem csak 2-hatvány méretű permutációkat akarunk támogatni, méretigazítás szükséges.
Ezt a rejection sampling technika alkalmazásával érjük el.
Az adott $n$ mérethez vesszük azt a legkisebb $N$ 2-hatványt, amely $n$-nél nem kisebb.
Amikor egy adott $i$ értékre a Feistel-hálótól az intervallumon kívül eső $p$-t kapunk,
azaz amelyre $p \geq n$ ($p < N$), akkor újra meghívjuk a permutáló függvényt $p$-re.
Addig ismételjük a hívást, míg végre $n$-nél kisebb szám nem születik.
A páratlan blokkméret megengedettsége a hatékonyság szempontjából is fontos,
mivel jelentősen csökkentheti az intervallumon kívülre esés esélyét,
és így a Feistel-hálózatra történő hívások számát.

\subsubsection{Permutációk méretigazítása}

Itt érdemes megjegyezni, hogy más implementációknál a rejection sampling potenciálisan nem hatékony,
de pontosan ezért vezettük be a $resized(newSize)$ metódust magán a $Permutation$ interfészen.
Így minden implementáció maga döntheti el, hogyan kell átméretezni.
Extrém példa: ha a $P_{+1}(i) = i + 1 \mod N$ permutációt $n$-re kell átméreteznünk,
akkor rejection sampling módszerrel $i = n$ esetén $N - n$ számú extra iterációt kellene végrehajtani,
ami nagy méret esetén rendkívül költséges.
Helyette viszont egyszerűen áttérhetünk modulo $n$-re: $P_{+k}(k)(i) = i + k \mod n$.

\subsubsection{XXXX}

\todo[inline]{egy trükkös saját permutáció, ami elég jó szórást ad (ha sikerül megoldani)}

\todo[inline]{bitkeverés, kétfelől átfedő 2-hatványos permutálás stb.}

\subsection{HASH-FÜGGVÉNYEK}

\todo[inline]{Utalni a Fesitel-hálózatban (és egyebütt) történt használatra}

\subsection{VIRTUÁLIS ÉRTÉKKÉSZLETEK}

\subsubsection{EGYSZERŰEK...}

\todo[inline]{Számsávok, felsorolások stb.}

\subsubsection{Reguláris kifejezések}

Véletlenszerű karakterláncok reguláris kifejezés alapján történő generálására
számos könyvtár elérhető a Java környezethez
(például a \textit{xeger} és a \textit{generex}).
Ezek olyan automaták, melyek az állapotgráfot véletlenszerű választásokkal járják be.
Ez a megközelítés önmagában nem alkalmas arra,
hogy az összes illeszkedő karakterláncból álló virtuális értékkészletet
a számunkra megfelelő módon implementálni lehessen vele.
Nem tudjuk ugyanis lekérni a rendezett értékkészlet $n$-edik elemét,
és nem tudjuk kikeresni, hogy adott érték mely pozíción található.

Nézzük most meg, hogyan tudjuk mégis implementáltni rendezett értékkészletet.
Első megközelítésben a reguláris kifejezéssel kapcsolatban az alábbi megszorításokat tesszük:

\begin{itemize}
    \item a reguláris kifejezés csak konstans karaktereket, karakterosztályokat, csoportokat és alternációt tartalmaz
    \item a rendezés karakterenkénti, azaz nincs másodlagos, harmadlagos stb. összehasonlítási szempont
\end{itemize}

Ilyen megkötések mellett a reguláris kifejezéshez egyértelműen rendelhető egy
irányított körmentes szógráf (DAWG),
melyben egy érték nélküli kiinduló elemből érjük el a karaktereket reprezentáló csomópontokat,
és kizárólag a szóvég csomópontokon (EOW) terminálhatunk,
melyekből már nem érhető el további csomópont.
A gráf tömöríthető úgy, hogy konstans karakterek helyett karakterosztályokat engedünk meg.
Ekkor egy adott csomópontból elérhető csomópontok listájára az alábbiak érvényesek:

\begin{itemize}
    \item a lista elemei diszjunkt karakterosztályok
    \item bármely listaelem minden tartalmazott karaktere későbbi, mint bármely megelőző elem bármely tartalmazott karaktere
\end{itemize}

\todo[inline]{egyszerű DAWG példa}

A gráf lényegileg egy prefix fa, ahol egyes azonos részfákat újrahasznosítunk.
Az újrahasznosítás miatt a fa tárhelyigénye a reguláris kifejezéshez képest lineáris.
A szóvég csomópontok lesznek a fa levelei.
A prefix fa könnyen felépíthető a reguláris kifejezés szintaxisfájából (AST)
a következő módon:

\todo[inline]{prefix fa algoritmusa (szövegesen és/vagy kóddal), kivételkezelés is}

Rekurzív módon a fa minden csomópontjához könnyen hozzárendelhetjük
a hozzá tartozó részfa leveleinek számát.
Ez az információ már elegendő ahhoz, hogy kikeressük az $n$-edik elemet,
illetve hogy megkeressük egy adott karakterlánc (megtalált vagy beillesztési) pozícióját,
valamint hogy tetszőleges pozíciótól kezdve iteráljunk az illeszkedő karakterláncokon.

\todo[inline]{iteráció stb. algoritmusa}

Most pedig nézzük hogyan tudunk engedni a fenti megszorításokon.

A repetíciós operátorok explicite kifejtés által kerülnek visszavezetésre a fentiekre.
A kifejtés természetesen egyes esetekben lényegesen növeli a fa méretét a bemenethez képest.
Ennek kioptimalizálásával most nem foglalkozom.

Először a korlátlan repetíciót elmináljuk az rendre alábbi szabályokkal:

\begin{enumerate}
    \item \texttt{$A$+} ~ arr ~ \textt{$A$\{1,\}}
    \item \texttt{$A$\{$n$,\}} ~ arr ~ \textt{$A$\{$n$\}$A$*}
    \item \texttt{$A$*} ~ arr ~ \textt{$A$\{$c$\}} ~~~ (ahol $c$ előre definiált konstans)
\end{enumerate}

Az opcionális előfordulás az üres minta és az opcionális minta alternációjává alakul,
a fix repetíció pedig egyszerű konkatenációvá.
Az opcionális repetíció kifejtése pedig a következő rekurzív módon történik:

\begin{enumerate}
    \item $R_1~=$ \texttt{$A${,1}} ~ arr ~ \textt{($A$|)} ~~~ (mint az opcionális előfordulás)
    \item $R_n~=$ \texttt{$A${,$n$}} ~ arr ~ \textt{($A$~$R_{n-1}$|)} ~~~ (ahol $n>1$)
\end{enumerate}

Horgonyokat is implementálhatunk (például kezdet vagy szóhatár).
Ezeket a kezdeti feldolgozáskor úgy tekintjük,
mint az átementi üres csomópontokat, tehát elimináljuk őket,
de közben megjegyezzük megjegyezzük és sorban kikényszerítjük a teljesülésüket.
A horgonynak ellentmondó gyermekelemeket eltávolítjuk,
ha ezután nem marad gyermekelem, a teljesíthetetlen ágat jelző kivételt dobunk.

\todo[inline]{példa}

Az igazi nehézségek akkor kezdődnek,
amikor elengedjük a karakterenkénti rendezés megszorítását.
Erre például ékezetes karakterek vagy kis- és nagybetűk keveredése esetén lehet szükségünk.
Ekkor ugyanis esetleg csak egy későbbi eltérés dönti el,
hogy egy korábbi ékezet vagy nagybetűsség számít-e.
Vagyis nem elegendő a karakterenkénti prefix-fa a rendezettség biztosításához.

Megjegyzem, hogy továbbra is csak egy egységesített (nyelvfüggetlen) rendezési sémát tekintünk,
és a többkarakteres betűk felismerésétől is eltekintünk.
A Unicode Collation Algorithm (UCA) szabvány egy nyelvfüggő többszintű összehasonlítási sémát definiál,
de ezek között van olyan általános séma is, mely tekinthető egy nyelvfüggetlen összehasonlításnak.
Ennek egy egyszerűsített háromszintű változatát fogjuk alapul venni.

Minden karaktert három komponens,
nevezetesen az alapkarakter, a diakritikus jel és a kis-/nagybetűsség együttesének tekintünk.
Alapkarakternek a karakter diakritikus jelektől megfosztott kisbetűs alakját nevezzük.
Tehát például az '\texttt{Á}' karakter az '\texttt{a}' alapkarakterre,
a '\texttt{´}' ékezetre és a nagybetűsségre bontható.
Komponensenként egyértelmű és független lesz a rendezés,
ennek konkrét szabályait most nem részletezem.

Maguk a karakterláncok is három rétegre esnek szét, és komponensenként hasonlítandók össze.
Egy későbbi komponens csak akkor vizsgálandó,
ha az azt megelőző komponensek a két karakterláncban egyeznek.
A pozícionált diakritikus jeleket és nagybetűsség-jelzéseket speciális karaktereknek is tekinthetjük,
ekkor lényegileg karakterenkénti összehasonlításra is visszavezethetjük az eljárást.
Például:

\begin{verbatim}
AL      =   al   __   __     =   al<U:0><U:1>
alom    =   alom ____ ____   =   alom
Alom    =   alom ____ U___   =   alom<U:0>
álom    =   alom ´___ ____   =   alom<D:´:0>
BAL     =   bal  ___  UUU    =   bal<U:0><U:1><U:2>
báL     =   bal  _´_  __U    =   bal<D:´:1><U:2>
\end{verbatim}

A jobb oldalon a karakterláncoknak egy normálalakja látható,
mely csak az alapkarakterek után csak a módosításokat tartalmazza,
és a kisbetűsséget alapértelmezettnek tekinti.
Természetesen másfajta normálalakot is választhatunk (például ahol a nagybetűsség az alapértelmezett),
akár a reguláris kifejezés tulajdonságaitól függően,
de mindenképpen rendelkeznünk kell bizonyos, egymással konzisztens algoritmusokkal,
amelyek együtt implementálják az értékkészlethez tartozó rendezési kollációt:

\begin{enumerate}
    \item karakterláncot és normálalakot egymásba alakító függvényekkel
    \item normálalakban lévő karakterláncokat összehasonlító komparátorral
    \item reguláris kifejezést a normálalak szerinti prefix-gráfba alakító algoritmussal
    \item a prefix-gráfot bejáró, kereső stb. algoritmusokkal
\end{enumerate}

\todo[inline]{a háromrétegű prefix-fa leírása}
% Az ötlet az, hogy egyetlen prefix-fa helyett hármat hozunk létre.
% A harmadik a normál prefix-fa, mely az eredeti karaktereket tartalmazza, karakterenként rendezve.
% A második ennek topológiai egyszerűsítése úgy, hogy a kis-nagybetű különbségeket összevonjuk.
% Ahol az összevonásnál azonos gyermekelemeket találunk, azok közül csak az egyiket tartjuk meg,
% de beiktatunk egy szorzó csomópontot (TODO: megmagyarázni).
% Az első fa szintén a normál prefix-fa egyszerűsítése,
% de itt minden azonos alapkarakterű karaktert összevonunk.
% A csomópontok számításánál figyelembe kell venni a szorzókat,
% amelyek felszorozzák a részfák levél-számát.

% az algoritmus:
    % - az első fát normál módon érjük el, azonban a leveleknél 1-nél nagyobb számosságok fognak maradni a szorzók miatt
    % - a második fát le kell szűkíteni az alapkarakterek szerint, ez alternatív számosságokat fog adni, ez alapján tudjuk számolni az alapkarater-fán belüli pozíciókat, itt is 1-nél nagyobb számosságokra lyukadhatunk ki
    % - a harmadik fát szintén szűkítjük a kisbetűs karakterek szerint
% a számosságok csak az érintett karakterekre számítandók, és cache-elhetők
% így elég hatékony algoritmust kapunk

\pagebreak

\appendix

\phantomsection
\addcontentsline{toc}{chapter}{\biblabel}
\printbibliography[title=\biblabel]
\cleardoublepage

\phantomsection
\addcontentsline{toc}{chapter}{\lstfigurelabel}
\listoffigures
\cleardoublepage

\phantomsection
\addcontentsline{toc}{chapter}{\lsttablelabel}
\listoftables
\cleardoublepage

\chapter{Projektlinkek}

\begin{itemize}
    \item GitHub organization: \par \url{https://github.com/miniconnect/}
    \item Használati példák: \par \url{https://github.com/miniconnect/general-docs/tree/main/examples}
    \item HoloDB projekt: \par \url{https://github.com/miniconnect/holodb}
    \item Docker Hub repository: \par \url{https://hub.docker.com/r/miniconnect/holodb/tags}
    \item \texttt{LargeInteger} benchmark: \par \url{https://github.com/miniconnect/miniconnect-api/tree/master/projects/lang/src/jmh/java/hu/webarticum/miniconnect/lang}
    \item HoloDB benchmark: \par \url{https://github.com/davidsusu/holodb-tdk/tree/main/benchmark}
\end{itemize}

\end{document}
